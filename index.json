[{"id":3,"pagetitle":"Introduction","title":"DynamicalSystems","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#DynamicalSystems","content":" DynamicalSystems  —  Module DynamicalSystems.jl  is an  award-winning  Julia software library for nonlinear dynamics and nonlinear timeseries analysis. To install  DynamicalSystems.jl , run  import Pkg; Pkg.add(\"DynamicalSystems\") . To learn how to use it and see its contents visit the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. DynamicalSystems.jl  is part of  JuliaDynamics , an organization dedicated to creating high quality scientific software. Highlights Aspects of  DynamicalSystems.jl  that make it stand out among other codebases for nonlinear dynamics or nonlinear timeseries analysis are: Exceptional documentation . All implemented algorithms provide a high-level scientific description of their functionality in their documentation string as well as references to scientific papers. The documentation features hundreds of tutorials and examples ranging from introductory to expert usage. Accessible source code . One of the main priorities of the library is that the source code of (almost) all implementations is small, simple, easy to understand and modify. This increases confidence, reduces bugs, and allows users to become developers without unnecessary effort. Open source community project . Built from the ground up entirely on GitHub,  DynamicalSystems.jl  is 100% open source and built by community contributions. Anyone can be a developer of the library. Everyone is welcomed. Extensive content . It aims to cover the entire field of nonlinear dynamics. It has functionality for complexity measures, delay embeddings, stability and bifurcation analysis, chaos, surrogate testing, recurrence quantification analysis, and much more. Furthermore, all algorithms are \"general\" and work for any dynamical system applicable. Missing functionality that falls under nonlinear dynamics is welcomed to be part of the library! Well tested . All implemented functionality is extensively tested. Each time any change in the code base is done, the extensive test suite is run and checked before merging the change in. Extendable .  DynamicalSystems.jl  is a living, evolving project. New contributions can become part of the library and be accessed by all users in the next release. Most importantly, all parts of the library follow professional standards in software design and implement extendable interfaces so that it is easy to contribute new functionality. Active development . Since the start of the project (May 2017) there has been  activity every month: new features, bugfixes, and the developer team answers users questions on Discourse/Slack. Performant . Written entirely in Julia, heavily optimized and parallelized, and taking advantage of some of the best packages within the language,  DynamicalSystems.jl  is  really fast . Goals The primary goal of  DynamicalSystems.jl  is to be a library in the literal sense: where people go to learn something (here in particular for nonlinear dynamics). That is why the main priority is that the documentation is detailed and references articles and why the source code is written as clearly as possible, so that it is examinable by any user. The second goal is to fill the missing gap of high quality general purpose software for nonlinear dynamics which can be easily extended with new functionality. The purpose of this is to make the field of nonlinear dynamics accessible and reproducible. The third goal is to fundamentally change the perception of the role of code in both scientific education as well as research. It is rarely the case that real,  runnable  code is shown in the classroom, because it is often long and messy. This is especially hurtful for nonlinear dynamics, a field where computer-assisted exploration is critical. And published scientific work in this field fares even worse, with the overwhelming majority of published research not sharing the code used to create the paper. This makes reproducing these papers difficult, while some times straight-out impossible.  DynamicalSystems.jl  can change this situation, because it is high level (requires writing little code to get lots of results) while offering extensive and well-tested functionality. source Star us on GitHub! If you have found this library useful, please consider starring it on  GitHub . This gives us an accurate lower bound of the (satisfied) user count."},{"id":4,"pagetitle":"Introduction","title":"Introduction","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Introduction","content":" Introduction Welcome to the documentation of  DynamicalSystems.jl ! If you have not used the library before, and would like to get started, then please read the  overarching tutorial  for the library. The  contents  page gives a summary of all packages that are part of the library. See the  learning resources  below to find out more resources about learning the library and using it in scientific research and/or education. Besides the formal algorithmic/scientific content of  DynamicalSystems.jl  (those in the  contents ) page, the library also provides basic functionality for interactive or offline animations and visualizations. These are found in the  visualizations  page. The remaining of this introduction page discusses our goals with the library, how to participate as a user or developer, how to cite, and other relevant information (see the sections of the sidebar on the left)."},{"id":5,"pagetitle":"Introduction","title":"Latest news","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Latest-news","content":" Latest news DynamicalSystems.jl now integrates with ModelingToolkit.jl and allows using symbolic variables to access/observe state and parameter. At a low level, this happens via the functions  observe_state ,  set_state! ,  current_parameter  and  set_parameter! . Additionally,  interactive_trajectory_timeseries  allows symbolic indexing for state space plot, timeseries plots, or parameter sliders. Everything is also automatically named and limits are also automatically deduced for everything! Super convenient!"},{"id":6,"pagetitle":"Introduction","title":"Learning resources","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#learning","content":" Learning resources"},{"id":7,"pagetitle":"Introduction","title":"Textbook with DynamicalSystems.jl","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Textbook-with-DynamicalSystems.jl","content":" Textbook with DynamicalSystems.jl We have written an undergraduate level textbook as an introduction to nonlinear dynamics. The text is written in an applied, hands-on manner, while still covering all fundamentals. The book pages are interlaced with real Julia code that uses DynamicalSystems.jl and is published in the Undergraduate Lecture Notes in Physics by Springer Nature: Nonlinear Dynamics: A concise introduction interlaced with code  by G. Datseris & U. Parlitz. Additional textbooks on nonlinear dynamics with practical focus are: Chaos in Dynamical Systems - E. Ott Nonlinear Time series Analysis - H. Kantz & T. Schreiber Nonlinear Dynamics and Chaos - S. Strogatz"},{"id":8,"pagetitle":"Introduction","title":"Course on applied nonlinear dynamics and complex systems","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Course-on-applied-nonlinear-dynamics-and-complex-systems","content":" Course on applied nonlinear dynamics and complex systems We are developing a full course (targeting a graduate or undergraduate semester long course) on applied nonlinear dynamics, nonlinear timeseries analysis, and complex systems, using the packages of  JuliaDynamics .  DynamicalSystems.jl  is part of this course. The materials of the course are on GitHub:  https://github.com/JuliaDynamics/NonlinearDynamicsComplexSystemsCourses"},{"id":9,"pagetitle":"Introduction","title":"Citing","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Citing","content":" Citing There is a (small) paper associated with  DynamicalSystems.jl . If we have helped you in research that led to a publication, please cite it using the DOI  10.21105/joss.00598  or the following BiBTeX entry: @article{Datseris2018,\n  doi = {10.21105/joss.00598},\n  url = {https://doi.org/10.21105/joss.00598},\n  year  = {2018},\n  month = {mar},\n  volume = {3},\n  number = {23},\n  pages = {598},\n  author = {George Datseris},\n  title = {DynamicalSystems.jl: A Julia software library for chaos and nonlinear dynamics},\n  journal = {Journal of Open Source Software}\n} Irrespectively of  DynamicalSystems.jl ,  please also cite the specific algorithm that you used from the library . The documentation of the function used will point you to the correct reference. Besides the library, we would also appreciate it if you cited the textbook we wrote that  DynamicalSystems.jl  accompanies: @book{DatserisParlitz2022,\n  doi = {10.1007/978-3-030-91032-7},\n  url = {https://doi.org/10.1007/978-3-030-91032-7},\n  year = {2022},\n  publisher = {Springer Nature},\n  author = {George Datseris and Ulrich Parlitz},\n  title     = \"Nonlinear dynamics: A concise introduction interlaced with code\",\n  address   = \"Cham, Switzerland\",\n  language  = \"en\",\n}"},{"id":10,"pagetitle":"Introduction","title":"Asking questions","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#ask_questions","content":" Asking questions There are three options for asking questions: As a new post in the official  Julia discourse  and ask a question under the category Specific Domains > Modelling & Simulations, also using  dynamical-systems  as a tag. This option is preferred for any meaningfully involved question, as the answer there will be future-searchable. As a message in our channel  #dynamics-bridged  in the  Julia Slack  workplace. This option is preferred for a brief question with (expected) simple answer, or to get an opinion about something, or to chat about something. By opening an issue directly on the  GitHub page of DynamicalSystems.jl  while providing a Minimal Working Example. This option is preferred when you encounter unexpected behavior."},{"id":11,"pagetitle":"Introduction","title":"Contributing & Donating","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Contributing-and-Donating","content":" Contributing & Donating Be sure to visit the  Contributor Guide  page, because you can help make this package better without having to write a single line of code! Also, if you find this package helpful please consider staring it on  GitHub ! This gives us an accurate lower bound of users that this package has already helped! Finally, you can donate for the development of  DynamicalSystems.jl . You can do that by adding bounties to existing issues on the GitHub repositories (you can open new issues as well). Every issue has an automatic way to create a bounty using  Bountysource , see the first comment of each issue."},{"id":12,"pagetitle":"Introduction","title":"Issues with Bounties","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Issues-with-Bounties","content":" Issues with Bounties Money that  DynamicalSystems.jl  obtains from awards, sponsors, or donators are converted into bounties for GitHub issues. The full list of issues that have a bounty is  available here . By solving these issues you not only contribute to open source, but you also get some pocket money to boot :)"},{"id":13,"pagetitle":"Introduction","title":"Maintainers and Contributors","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Maintainers-and-Contributors","content":" Maintainers and Contributors The  DynamicalSystems.jl  library is maintained by  George Datseris , who is also curating and writing this documentation. The software code however is built from the contributions of several individuals. The list is too long to write and constantly update, so the best way to find out these contributions is to visit the GitHub page of each of the subpackages and checkout the \"contributors\" pages there."},{"id":14,"pagetitle":"Introduction","title":"Version numbers","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Version-numbers","content":" Version numbers The version of  DynamicalSystems  by itself is a bit meaningless, because the module does not have any source code, besides re-exporting other modules. For transparency, the packages and versions used to build the documentation you are reading now are: using Pkg\nPkg.status([\n    \"DynamicalSystems\",\n    \"StateSpaceSets\", \"DynamicalSystemsBase\", \"RecurrenceAnalysis\", \"FractalDimensions\", \"DelayEmbeddings\", \"ComplexityMeasures\", \"TimeseriesSurrogates\", \"PredefinedDynamicalSystems\", \"Attractors\", \"ChaosTools\", \"CairoMakie\",\n    ];\n    mode = PKGMODE_MANIFEST\n) Status `~/work/DynamicalSystems.jl/DynamicalSystems.jl/docs/Manifest.toml`\n  [f3fd9213] Attractors v1.17.0\n  [13f3f980] CairoMakie v0.12.2\n  [608a59af] ChaosTools v3.1.2\n  [ab4b797d] ComplexityMeasures v3.4.3\n  [5732040d] DelayEmbeddings v2.7.4\n  [61744808] DynamicalSystems v3.3.16 `~/work/DynamicalSystems.jl/DynamicalSystems.jl`\n  [6e36e845] DynamicalSystemsBase v3.8.3\n  [4665ce21] FractalDimensions v1.8.1\n  [31e2f376] PredefinedDynamicalSystems v1.2.0\n  [639c3291] RecurrenceAnalysis v2.0.6\n  [40b095a5] StateSpaceSets v1.4.6\n  [c804724b] TimeseriesSurrogates v2.6.4 Version numbers do not strictly follow SemVer2.0 Because of the nature of the  DynamicalSystems.jl  library, the exported API contains hundreds of algorithm implementations, most of which are independent of each other. Our development approach is that breaking changes to these individual algorithms (due to e.g., better API design or better performance implementations or better default keyword arguments) can be done  without incrementing any major version numbers . We increment major version numbers only for breaking changes that have wide impact over most of the  DynamicalSystems.jl  library."},{"id":15,"pagetitle":"Introduction","title":"Other NLD-relevant packages","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/#Other-NLD-relevant-packages","content":" Other NLD-relevant packages Besides DynamicalSystems.jl, the Julia programming language has a thriving ecosystem with plenty of functionality that is relevant for nonlinear dynamics. We list some useful references below: DifferentialEquations.jl  - Besides providing solvers for standard ODE systems (infastructure already used in DynamicalSystems.jl), it also has much more features like SDE solvers or uncertainty quantification. DiffEqSensitivity.jl  - Discrete and continuous local sensitivity analysis, i.e., derivatives of the solutions of ODEs, or functions of the solutions, versus parameters, hosting  various forward and adjoint methods as well as methods tailored to chaotic systems . GlobalSensitivity.jl  - Global sensitivity analysis assessing the effect of any input variables over a larger domain on the output. BifurcationKit.jl  - Featureful toolkit for automated bifurcation analysis. NetworkDynamics.jl  - Simulating dynamics on networks and transforming network systems into  ODEProblem  (that can be made directly into a  ContinuousDynamicalSystem ). Agents.jl  - Agent based modelling. EasyModelAnalysis.jl  - Analysis tools for conveniently analysing solutions of DiffEq systems. SignalDecomposition.jl  - Decompose a signal/timeseries into structure and noise or seasonal and residual components. ARFIMA.jl  - generate ARFIMA process timeseries. ConcurrentSim.jl  - discrete event process oriented simulation framework. CausalityTools.jl  - hundreds of algorithms for relational/causal timeseries analysis and causal graphs."},{"id":18,"pagetitle":"Contents","title":"Contents","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#contents","content":" Contents When you do  using DynamicalSystems  in your Julia session, the module re-exports and brings into scope all submodules (Julia packages) that compose  DynamicalSystems.jl . These are listed in this page. Of course, you could be using these packages directly instead of adding  DynamicalSystems . However, doing  using DynamicalSystems  provides the environment all these packages were designed to work together in, and so we recommend to simply install  DynamicalSystems  and use that."},{"id":19,"pagetitle":"Contents","title":"Exported submodules","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#Exported-submodules","content":" Exported submodules The submodules that compose  DynamicalSystems.jl  are the following packages, which are re-exported by  DynamicalSystems : Core StateSpaceSets DynamicalSystemsBase For observed/measured data ComplexityMeasures RecurrenceAnalysis DelayEmbeddings FractalDimensions TimeseriesSurrogates For dynamical system instances PredefinedDynamicalSystems ChaosTools Attractors At the very end of this page, a full list of exported names is presented."},{"id":20,"pagetitle":"Contents","title":"Core","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#Core","content":" Core"},{"id":21,"pagetitle":"Contents","title":"StateSpaceSets.StateSpaceSets","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#StateSpaceSets.StateSpaceSets","content":" StateSpaceSets.StateSpaceSets  —  Module StateSpaceSets.jl A Julia package that provides functionality for state space sets. These are collections of points of fixed, and known by type, size (called dimension). It is used by many other packages in the JuliaDynamics organization. The main export of  StateSpaceSets  is the concrete type  StateSpaceSet . The package also provides functionality for distances, neighbor searches, sampling, and normalization. To install it you may run  import Pkg; Pkg.add(\"StateSpaceSets\") , however, there is no real reason to install this package directly as it is re-exported by all downstream packages that use it. source"},{"id":22,"pagetitle":"Contents","title":"DynamicalSystemsBase.DynamicalSystemsBase","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#DynamicalSystemsBase.DynamicalSystemsBase","content":" DynamicalSystemsBase.DynamicalSystemsBase  —  Module DynamicalSystemsBase.jl A Julia package that defines the  DynamicalSystem  interface and many concrete implementations used in the DynamicalSystems.jl ecosystem. To install it, run  import Pkg; Pkg.add(\"DynamicalSystemsBase\") . Typically, you do not want to use  DynamicalSystemsBase  directly, as downstream analysis packages re-export it. All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. source"},{"id":23,"pagetitle":"Contents","title":"For observed/measured data","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#For-observed/measured-data","content":" For observed/measured data"},{"id":24,"pagetitle":"Contents","title":"ComplexityMeasures.ComplexityMeasures","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#ComplexityMeasures.ComplexityMeasures","content":" ComplexityMeasures.ComplexityMeasures  —  Module ComplexityMeasures.jl A Julia package that provides: A rigorous framework for extracting probabilities from data, based on the mathematical formulation of  probability spaces . Several (12+) outcome spaces, i.e., ways to discretize data into probabilities. Several estimators for estimating probabilities given an outcome space, which correct theoretically known estimation biases. Several definitions of information measures, such as various flavours of entropies (Shannon, Tsallis, Curado...), extropies, and probability-based complexity measures, that are used in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems. Several discrete and continuous (differential) estimators for entropies, which correct theoretically known estimation biases. Estimators for other complexity measures that are not estimated based on probability functions. An extendable interface and well thought out API accompanied by dedicated developer documentation pages. These makes it trivial to define new outcome spaces, or new estimators for probabilities, information measures, or complexity measures and integrate them with everything else in the software. ComplexityMeasures.jl can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . To install it, run  import Pkg; Pkg.add(\"ComplexityMeasures\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was called Entropies.jl. source"},{"id":25,"pagetitle":"Contents","title":"RecurrenceAnalysis.RecurrenceAnalysis","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#RecurrenceAnalysis.RecurrenceAnalysis","content":" RecurrenceAnalysis.RecurrenceAnalysis  —  Module RecurrenceAnalysis.jl A Julia package that offers tools for computing Recurrence Plots and exploring them within the framework of Recurrence Quantification Analysis and Recurrence Network Analysis. It can be used as a standalone package, or as part of DynamicalSystems.jl. To install it, run  import Pkg; Pkg.add(\"RecurrenceAnalysis\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. source"},{"id":26,"pagetitle":"Contents","title":"DelayEmbeddings.DelayEmbeddings","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#DelayEmbeddings.DelayEmbeddings","content":" DelayEmbeddings.DelayEmbeddings  —  Module DelayEmbeddings.jl A Julia package that provides a generic interface for performing delay coordinate embeddings, as well as cutting edge algorithms for creating optimal embeddings given some data. It can be used as a standalone package, or as part of DynamicalSystems.jl. To install it, run  import Pkg; Pkg.add(\"DelayEmbeddings\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. source"},{"id":27,"pagetitle":"Contents","title":"FractalDimensions.FractalDimensions","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#FractalDimensions.FractalDimensions","content":" FractalDimensions.FractalDimensions  —  Module FractalDimensions.jl A Julia package that estimates various definitions of fractal dimension from data. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"FractalDimensions\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was part of ChaosTools.jl. Publication FractalDimensions.jl is used in a review article comparing various estimators for fractal dimensions. The paper is likely a relevant read if you are interested in the package. And if you use the package, please cite the paper. @article{FractalDimensions.jl,\n  doi = {10.1063/5.0160394},\n  url = {https://doi.org/10.1063/5.0160394},\n  year = {2023},\n  month = oct,\n  publisher = {{AIP} Publishing},\n  volume = {33},\n  number = {10},\n  author = {George Datseris and Inga Kottlarz and Anton P. Braun and Ulrich Parlitz},\n  title = {Estimating fractal dimensions: A comparative review and open source implementations},\n  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science}\n} source"},{"id":28,"pagetitle":"Contents","title":"TimeseriesSurrogates.TimeseriesSurrogates","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#TimeseriesSurrogates.TimeseriesSurrogates","content":" TimeseriesSurrogates.TimeseriesSurrogates  —  Module TimeseriesSurrogates.jl A Julia package for generating timeseries surrogates. TimeseriesSurrogates.jl is the fastest and most featureful open source code for generating timeseries surrogates. It can be used as a standalone package, or as part of other projects in JuliaDynamics such as DynamicalSystems.jl or CausalityTools.jl. To install it, run  import Pkg; Pkg.add(\"TimeseriesSurrogates\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. Citing Please use the following BiBTeX entry, or DOI, to cite TimeseriesSurrogates.jl: DOI: https://doi.org/10.21105/joss.04414 BiBTeX: @article{TimeseriesSurrogates.jl,\n    doi = {10.21105/joss.04414},\n    url = {https://doi.org/10.21105/joss.04414},\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {77},\n    pages = {4414},\n    author = {Kristian Agasøster Haaga and George Datseris},\n    title = {TimeseriesSurrogates.jl: a Julia package for generating surrogate data},\n    journal = {Journal of Open Source Software}\n} source"},{"id":29,"pagetitle":"Contents","title":"For dynamical system instances","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#For-dynamical-system-instances","content":" For dynamical system instances"},{"id":30,"pagetitle":"Contents","title":"PredefinedDynamicalSystems.PredefinedDynamicalSystems","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#PredefinedDynamicalSystems.PredefinedDynamicalSystems","content":" PredefinedDynamicalSystems.PredefinedDynamicalSystems  —  Module PredefinedDynamicalSystems.jl Module which contains pre-defined dynamical systems that can be used by the  DynamicalSystems.jl  library. To install it, run  import Pkg; Pkg.add(\"PredefinedDynamicalSystems\") . Predefined systems exist as functions that return a  DynamicalSystem  instance. They are accessed like: ds = PredefinedDynamicalSystems.lorenz(u0; ρ = 32.0) The alias  Systems  is also exported as a deprecation. This module is provided purely as a convenience. It does not have any actual tests, and it is not guaranteed to be stable in future versions. It is not recommended to use this module for anything else besides on-the-spot demonstrative examples. For some systems, a Jacobian function is also defined. The naming convention for the Jacobian function is  \\$(name)_jacob . So, for the above example we have  J = Systems.lorenz_jacob . All available systems are provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source"},{"id":31,"pagetitle":"Contents","title":"ChaosTools.ChaosTools","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#ChaosTools.ChaosTools","content":" ChaosTools.ChaosTools  —  Module ChaosTools.jl A Julia module that offers various tools for analysing nonlinear dynamics and chaotic behaviour. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"ChaosTools\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. ChaosTools.jl is the jack-of-all-trades package of the DynamicalSystems.jl library: methods that are not extensive enough to be a standalone package are added here. You should see the full DynamicalSystems.jl library for other packages that may contain functionality you are looking for but did not find in ChaosTools.jl. source"},{"id":32,"pagetitle":"Contents","title":"Attractors.Attractors","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#Attractors.Attractors","content":" Attractors.Attractors  —  Module Attractors.jl A Julia module for finding attractors of arbitrary dynamical systems finding their basins of attraction or the state space fractions of the basins analyzing global stability of attractors (also called non-local stability or  resilience) \"continuing\" the attractors and their basins over a parameter range finding the basin boundaries and analyzing their fractal properties tipping points related functionality for systems with known dynamic rule and more! It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"Attractors\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, Attractors.jl was part of ChaosTools.jl source"},{"id":33,"pagetitle":"Contents","title":"All exported names","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contents/#All-exported-names","content":" All exported names This section lists all exported names of the  DynamicalSystems.jl  library. We do not list their documentation in any way here. This list is only meant as a quantitative listing of features, as well as perhaps helping searching via the search bar. To actually learn how to use all these exported names you need to use above-linked documentation of the respective submodules! The total exported names are: using DynamicalSystems\nall_exported_names = names(DynamicalSystems)\nlength(all_exported_names) 478 And they are: using DisplayAs\nDisplayAs.unlimited(all_exported_names) 478-element Array{Symbol, 1}:\n :..\n Symbol(\"@windowed\")\n :AAFT\n :AR1\n :AbstractBinning\n :AbstractDataset\n :AbstractEmbedding\n :AbstractRecurrenceType\n :AbstractStateSpaceSet\n :AddConstant\n :AlizadehArghami\n :AllSlopesDistribution\n :AmplitudeAwareOrdinalPatterns\n :ApproximateEntropy\n :ArbitrarySteppable\n :AttractorMapper\n :Attractors\n :AttractorsBasinsContinuation\n :AttractorsViaFeaturizing\n :AttractorsViaProximity\n :AttractorsViaRecurrences\n :AutoRegressive\n :BayesianRegularization\n :BlockMaxima\n :BlockShuffle\n :BruteForce\n :BubbleEntropy\n :BubbleSortSwaps\n :BubbleSortSwapsEncoding\n :Centroid\n :ChaoShen\n :ChaosTools\n :Chebyshev\n :CircShift\n :Cityblock\n :ClusteringConfig\n :CombinationEncoding\n :ComplexityEstimator\n :ComplexityMeasure\n :ComplexityMeasures\n :Composite\n :ContinuousDynamicalSystem\n :ContinuousTimeDynamicalSystem\n :CoreDynamicalSystem\n :Correa\n :CosineSimilarityBinning\n :CountOccurrences\n :Counts\n :CoupledODEs\n :CramerVonMises\n :CrossRecurrenceMatrix\n :CrossingAccurateInterpolation\n :CrossingLinearIntersection\n :Curado\n :CycleShuffle\n :Dataset\n :DelayEmbedding\n :DelayEmbeddings\n :DeterministicIteratedMap\n :DifferentialEntropyEstimator\n :DifferentialInfoEstimator\n :DiscreteDynamicalSystem\n :DiscreteEntropyEstimator\n :DiscreteInfoEstimator\n :DiscreteTimeDynamicalSystem\n :Dispersion\n :Diversity\n :DynamicalSystem\n :DynamicalSystems\n :DynamicalSystemsBase\n :Ebrahimi\n :EdgeTrackingResults\n :ElectronicEntropy\n :Encoding\n :EntropyDefinition\n :Euclidean\n :Exceedances\n :FAN\n :FT\n :FeaturizeGroupAcrossParameter\n :FirstElement\n :FixedRectangularBinning\n :FractalDimensions\n :Gao\n :GaussianCDFEncoding\n :GeneralizedEmbedding\n :GeneralizedSchuermann\n :GlobalRecurrenceRate\n :Goria\n :GroupAcrossParameter\n :GroupAcrossParameterContinuation\n :GroupViaClustering\n :GroupViaHistogram\n :GroupViaNearestFeature\n :GroupViaPairwiseComparison\n :GroupingConfig\n :HRectangle\n :HSphere\n :HSphereSurface\n :Hausdorff\n :HorvitzThompson\n :IAAFT\n :Identification\n :InformationMeasure\n :InformationMeasureEstimator\n :IntervalBox\n :InvariantMeasure\n :IrregularLombScargle\n :Jackknife\n :JointRecurrenceMatrix\n :KDTree\n :Kaniadakis\n :KozachenkoLeonenko\n :Kraskov\n :LargestLinearRegion\n :LempelZiv76\n :LeonenkoProzantoSavani\n :LinearRegression\n :LocalRecurrenceRate\n :Lord\n :MFSBlackBoxOptim\n :MFSBruteForce\n :MLEntropy\n :MillerMadow\n :MissingDispersionPatterns\n :NLNS\n :NSAR2\n :NaiveKernel\n :NeighborNumber\n :OrdinalPatternEncoding\n :OrdinalPatterns\n :Outcome\n :OutcomeSpace\n :PairDistanceEncoding\n :ParallelDynamicalSystem\n :PartialRandomization\n :PartialRandomizationAAFT\n :PlaneCrossing\n :PlugIn\n :PoincareMap\n :PowerSpectrum\n :PredefinedDynamicalSystems\n :Probabilities\n :ProbabilitiesEstimator\n :ProjectedDynamicalSystem\n :PseudoPeriodic\n :PseudoPeriodicTwin\n :RAFM\n :RandomCascade\n :RandomFourier\n :RandomShuffle\n :RectangularBinEncoding\n :RectangularBinning\n :RecurrenceAnalysis\n :RecurrenceMatrix\n :RecurrenceThreshold\n :RecurrenceThresholdScaled\n :RecurrencesFindAndMatch\n :RecurrencesSeededContinuation\n :Regular\n :RelativeAmount\n :RelativeFirstDifferenceEncoding\n :RelativeMeanEncoding\n :RelativePartialRandomization\n :RelativePartialRandomizationAAFT\n :Renyi\n :RenyiExtropy\n :ReverseDispersion\n :SMatrix\n :SNLST\n :SVector\n :SampleEntropy\n :Schuermann\n :SequentialPairDistances\n :Shannon\n :ShannonExtropy\n :Shrinkage\n :ShuffleDimensions\n :SpatialDispersion\n :SpatialOrdinalPatterns\n :SpectralPartialRandomization\n :SpectralPartialRandomizationAAFT\n :StateSpaceSet\n :StateSpaceSets\n :StatisticalComplexity\n :StretchedExponential\n :StrictlyMinimumDistance\n :StroboscopicMap\n :SubdivisionBasedGrid\n :Surrogate\n :SurrogateTest\n :SymbolicAmplitudeAwarePermutation\n :SymbolicPermutation\n :SymbolicWeightedPermutation\n :Systems\n :TAAFT\n :TFTD\n :TFTDAAFT\n :TFTDIAAFT\n :TFTDRandomFourier\n :TFTS\n :TangentDynamicalSystem\n :TimeScaleMODWT\n :TimeseriesSurrogates\n :TransferOperator\n :Tsallis\n :TsallisExtropy\n :UniqueElements\n :UniqueElementsEncoding\n :ValueBinning\n :ValueHistogram\n :Vasicek\n :VisitationFrequency\n :WLS\n :WaveletOverlap\n :WeightedOrdinalPatterns\n :WithinRange\n :Zhu\n :ZhuSingh\n :aggregate_attractor_fractions\n :allcounts\n :allcounts_and_outcomes\n :allprobabilities\n :allprobabilities_and_outcomes\n :animate_attractors_continuation\n :autocor\n :automatic_Δt_basins\n :basin_entropy\n :basins_fractal_dimension\n :basins_fractal_test\n :basins_fractions\n :basins_of_attraction\n :beta_statistic\n :bisect_to_edge\n :boxassisted_correlation_dim\n :boxed_correlationsum\n :broomhead_king\n :codify\n :colored_noise\n :columns\n :complexity\n :complexity_normalized\n :continuation\n :convergence_and_basins_fractions\n :convergence_and_basins_of_attraction\n :convergence_time\n :convert_logunit\n :coordinates\n :correlationsum\n :counts\n :counts_and_outcomes\n :current_crossing_time\n :current_deviations\n :current_parameter\n :current_parameters\n :current_state\n :current_states\n :current_time\n :dataset_distance\n :datasets_sets_distances\n :decode\n :delay_afnn\n :delay_f1nn\n :delay_fnn\n :delay_ifnn\n :determinism\n :dimension\n :distancematrix\n :divergence\n :dl_average\n :dl_entropy\n :dl_max\n :dyca\n :dynamic_rule\n :edgetracking\n :embed\n :encode\n :entropy\n :entropy_approx\n :entropy_complexity\n :entropy_complexity_curves\n :entropy_dispersion\n :entropy_distribution\n :entropy_maximum\n :entropy_normalized\n :entropy_permutation\n :entropy_sample\n :entropy_wavelet\n :estimate_boxsizes\n :estimate_delay\n :estimate_gev_parameters\n :estimate_gev_scale\n :estimate_gpd_parameters\n :estimate_period\n :estimate_r0_buenoorovio\n :estimate_r0_theiler\n :excitability_threshold\n :exit_entry_times\n :expansionentropy\n :exponential_decay_fit\n :extract_attractors\n :extract_features\n :extremal_index_sueveges\n :extremevaltheory_dim\n :extremevaltheory_dims\n :extremevaltheory_dims_persistences\n :extremevaltheory_gpdfit_pvalues\n :extremevaltheory_local_dim_persistence\n :findlocalextrema\n :findlocalminima\n :first_return_times\n :fixedmass_correlation_dim\n :fixedmass_correlationsum\n :fixedpoints\n :gali\n :garcia_almeida_embedding\n :genembed\n :genentropy\n :generalized_dim\n :get_deviations\n :get_state\n :grassberger_proccacia_dim\n :grayscale\n :group_features\n :haussdorff_distance\n :heatmap_basins_attractors\n :heatmap_basins_attractors!\n :higuchi_dim\n :information\n :information_maximum\n :information_normalized\n :initial_parameters\n :initial_state\n :initial_states\n :initial_time\n :integrator\n :interactive_cobweb\n :interactive_orbitdiagram\n :interactive_poincaresos\n :interactive_poincaresos_scan\n :interactive_trajectory\n :interactive_trajectory_timeseries\n :interval\n :invariantmeasure\n :is_counting_based\n :isdeterministic\n :isdiscretetime\n :isinplace\n :kaplanyorke_dim\n :lambdamatrix\n :lambdaperms\n :laminarity\n :linear_region\n :linear_regions\n :linreg\n :local_growth_rates\n :lyapunov\n :lyapunov_from_data\n :lyapunovspectrum\n :match_attractor_ids!\n :match_basins_ids!\n :match_continuation!\n :match_statespacesets!\n :maxima\n :mdop_embedding\n :mdop_maximum_delay\n :mean_return_times\n :meanrecurrencetime\n :minima\n :minimal_fatal_shock\n :minimum_pairwise_distance\n :minmaxima\n :missing_outcomes\n :missing_probabilities\n :molteno_boxing\n :molteno_dim\n :n_statistic\n :nmprt\n :noiseradius\n :observe_state\n :optimal_separated_de\n :optimal_traditional_de\n :orbitdiagram\n :orthonormal\n :outcome_space\n :outcomes\n :parallel_integrator\n :parameter_name\n :pecora\n :pecuzal_embedding\n :periodicorbits\n :permentropy\n :plot_attractors_curves\n :plot_attractors_curves!\n :plot_basins_attractors_curves\n :plot_basins_attractors_curves!\n :plot_basins_curves\n :plot_basins_curves!\n :poincaremap\n :poincaresos\n :pointwise_correlationsums\n :pointwise_dimensions\n :predictability\n :prismdim_theiler\n :probabilities\n :probabilities!\n :probabilities_and_outcomes\n :produce_orbitdiagram\n :projected_integrator\n :pvalue\n :random_cycles\n :randomwalk\n :reconstruct\n :recurrence_threshold\n :recurrenceplot\n :recurrencerate\n :recurrencestructures\n :recursivecopy\n :referrenced_sciml_model\n :reinit!\n :rematch!\n :replacement_map\n :rna\n :rqa\n :rt_average\n :rt_entropy\n :rt_max\n :scaleod\n :selfmutualinfo\n :set_deviations!\n :set_distance\n :set_parameter!\n :set_parameters!\n :set_period!\n :set_state!\n :setsofsets_distance\n :setsofsets_distances\n :shaded_basins_heatmap\n :shaded_basins_heatmap!\n :skeletonize\n :slopefit\n :sorteddistances\n :standardize\n :state_name\n :statespace_sampler\n :step!\n :stochastic_indicator\n :subdivision_based_grid\n :successful_step\n :surrogate\n :surrogenerator\n :surroplot\n :surroplot!\n :swap_dict_keys!\n :takens_best_estimate_dim\n :tangent_integrator\n :test_wada_merge\n :testchaos01\n :textrecurrenceplot\n :tipping_probabilities\n :total_outcomes\n :trajectory\n :transfermatrix\n :transit_return_times\n :transitivity\n :trappingtime\n :trend\n :uncertainty_exponent\n :unique_keys\n :uzal_cost\n :uzal_cost_local\n :vl_average\n :vl_entropy\n :vl_max\n :windowed\n :yin\n :×\n :τrange"},{"id":36,"pagetitle":"Contributor Guide","title":"Contributor Guide","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contributors_guide/#Contributor-Guide","content":" Contributor Guide The ultimate goal for  DynamicalSystems.jl  is to be a useful  library  for scientists working on nonlinear dynamics and to make nonlinear dynamics accessible and reproducible. Of course, for such an ambitious goal to be achieved, many of us should try to work together to improve the library! If you want to help the cause, there are many ways to contribute to the  DynamicalSystems.jl  library: Just  use it ! Share it with your colleagues if it was useful for you, and report unexpected behaviour if you find any (see  here  for how). Suggest methods that you think should be included in our library. This should be done by opening a new issue that describes the method, gives references to papers using the method and also justifies why the method should be included. Please open an issue to the GitHub page of the submodule of  DynamicalSystems.jl  that you feel is the most related to the method. Contribute code by solving existing issues. The easiest issues to tackle are the ones with label \"good first issue\". Here is a list of all such issues from all submodules of  DynamicalSystems.jl :  link . Contribute code by implementing new methods! That is by far the most impactful way to contribute to the library. The individual packages that compose  DynamicalSystems.jl  have plenty of issues that outline new methods wanted by the library, that are likely not tagged as \"good first issues\" because they will likely require familiarity that goes beyond a complete beginner. You can tackle one of these if you want to contribute! Additionally, we strongly welcome contributions of brand new algorithms that have just been developed during research in nonlinear dynamics. In fact,  DynamicalSystems.jl  started with the vision that researchers would add their newly developed methods directly to the library when they publish the methods."},{"id":37,"pagetitle":"Contributor Guide","title":"Contributing Code","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contributors_guide/#Contributing-Code","content":" Contributing Code When contributing code, principles for writing good scientific code apply. We recommend the [Good Scientific Code workshop] material for teaching you this. You should keep these things in mind: In general, the speed of the implementation is important, but not as important as the  clarity of the implementation . One of cornerstones of all of  DynamicalSystems.jl  is to have clear and readable source code. Fortunately, Julia allows you to have perfectly readable code but also super fast ;) If necessary add comments to the code, so that somebody that knows the method, can also understand the code immediately. Try to design general, extendable functions instead of unnecessarily specialized to the case at hand. The documentation strings of the new API functions you contribute are the most important to make as good as possible. Please follow the convention of the documentation strings of DynamicalSystems.jl outlined below."},{"id":38,"pagetitle":"Contributor Guide","title":"Documentation string style","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/contributors_guide/#Documentation-string-style","content":" Documentation string style Documentation strings are the most important thing in your pull request/code. The number 1 priority of DynamicalSystems.jl is highest possible quality of documentation and utmost transparency, and the best way to achieve this is with good documentation strings. In DynamicalSystems.jl we recommend that documentation strings are structured in the following way (and this is also the recommendation we give in the  Good Scientific Code Workshop ). Clear call signature in code syntax, including expected input types if necessary. The call signature should ONLY include only the most important information, not list out in detail every keyword! Brief summary of the function [Optional] Return value and type if not obvious (almost always it is not obvious!) [Optional] References to related functions if sensible with the  @ref  command. [Optional] Keyword arguments list if the function has some with a  ## Keyword arguments  subsection. [Optional] Detailed discussion of functionality if function behavior is scientifically involved with a  ## Description  subsection. [Optional] Citations to relevant scientific papers with the  @cite  command. The syntax of the documentation strings follows Documenter.jl protocol. For an example docstring to use as a reference, you can use the  ApproximateEntropy : rendered documentation string source code documentation string"},{"id":41,"pagetitle":"Overarching tutorial","title":"Overarching tutorial for DynamicalSystems.jl","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#tutorial","content":" Overarching tutorial for DynamicalSystems.jl This page serves as a short but to-the-point introduction to the  DynamicalSystems.jl  library. It outlines the core components, and how they establish an interface that is used by the rest of the library. It also provides a couple of usage examples to connect the various packages of the library together. Going through this tutorial should take you about 20 minutes."},{"id":42,"pagetitle":"Overarching tutorial","title":"Installation","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Installation","content":" Installation To install  DynamicalSystems.jl , simply do: using Pkg; Pkg.add(\"DynamicalSystems\") As discussed in the  contents  page, this installs several packages for the Julia language, that are all exported under a common name. To use them, simply do: using DynamicalSystems in your Julia session."},{"id":43,"pagetitle":"Overarching tutorial","title":"Core components","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Core-components","content":" Core components The individual packages that compose  DynamicalSystems  interact flawlessly with each other because of the following two components: The  StateSpaceSet , which represents numerical data. They can be observed or measured from experiments, sampled trajectories of dynamical systems, or just unordered sets in a state space. A  StateSpaceSet  is a container of equally-sized points, representing multivariate timeseries or multivariate datasets. Timeseries, which are univariate sets, are represented by the  AbstractVector{<:Real}  Julia base type. The  DynamicalSystem , which is the abstract representation of a dynamical system with a known dynamic evolution rule.  DynamicalSystem  defines an extendable interface, but typically one uses existing implementations such as  DeterministicIteratedMap  or  CoupledODEs ."},{"id":44,"pagetitle":"Overarching tutorial","title":"Making dynamical systems","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Making-dynamical-systems","content":" Making dynamical systems In the majority of cases, to make a dynamical system one needs three things: The dynamic rule  f : A Julia function that provides the instructions of how to evolve the dynamical system in time. The state  u : An array-like container that contains the variables of the dynamical system and also defines the starting state of the system. The parameters  p : An arbitrary container that parameterizes  f . For most concrete implementations of  DynamicalSystem  there are two ways of defining  f, u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f . Autonomous vs non-autonomous systems Whether the dynamical system is autonomous ( f  doesn't depend on time) or not, it is still necessary to include  t  as an argument to  f . Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way."},{"id":45,"pagetitle":"Overarching tutorial","title":"Example: Henon map","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Example:-Henon-map","content":" Example: Henon map Let's make the Henon map, defined as \\[\\begin{aligned}\nx_{n+1} &= 1 - ax^2_n+y_n \\\\\ny_{n+1} & = bx_n\n\\end{aligned}\\] with parameters  $a = 1.4, b = 0.3$ . First, we define the dynamic rule as a standard Julia function. Since the dynamical system is only two-dimensional, we should use the  out-of-place  form that returns an  SVector  with the next state: using DynamicalSystems\n\nfunction henon_rule(u, p, n) # here `n` is \"time\", but we don't use it.\n    x, y = u # system state\n    a, b = p # system parameters\n    xn = 1.0 - a*x^2 + y\n    yn = b*x\n    return SVector(xn, yn)\nend henon_rule (generic function with 1 method) Then, we define initial state and parameters u0 = [0.2, 0.3]\np0 = [1.4, 0.3] 2-element Vector{Float64}:\n 1.4\n 0.3 Lastly, we give these three to the  DeterministicIteratedMap : henon = DeterministicIteratedMap(henon_rule, u0, p0) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          0\n state:         [0.2, 0.3]\n henon  is a  DynamicalSystem , one of the two core structures of the library. They can evolved interactively, and queried, using the interface defined by  DynamicalSystem . The simplest thing you can do with a  DynamicalSystem  is to get its trajectory: total_time = 10_000\nX, t = trajectory(henon, total_time) (2-dimensional StateSpaceSet{Float64} with 10001 points, 0:1:10000) X 2-dimensional StateSpaceSet{Float64} with 10001 points\n  0.2        0.3\n  1.244      0.06\n -1.10655    0.3732\n -0.341035  -0.331965\n  0.505208  -0.102311\n  0.540361   0.151562\n  0.742777   0.162108\n  0.389703   0.222833\n  1.01022    0.116911\n -0.311842   0.303065\n  ⋮         \n -0.582534   0.328346\n  0.853262  -0.17476\n -0.194038   0.255978\n  1.20327   -0.0582113\n -1.08521    0.36098\n -0.287758  -0.325562\n  0.558512  -0.0863275\n  0.476963   0.167554\n  0.849062   0.143089 X  is a  StateSpaceSet , the second of the core structures of the library. We'll see below how, and where, to use a  StateSpaceset , but for now let's just do a scatter plot using CairoMakie\nscatter(X[:, 1], X[:, 2])"},{"id":46,"pagetitle":"Overarching tutorial","title":"Example: Lorenz96","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Example:-Lorenz96","content":" Example: Lorenz96 Let's also make another dynamical system, the Lorenz96 model: \\[\\frac{dx_i}{dt} = (x_{i+1}-x_{i-2})x_{i-1} - x_i + F\\] for  $i \\in \\{1, \\ldots, N\\}$  and  $N+j=j$ . Here, instead of a discrete time map we have  $N$  coupled ordinary differential equations. However, creating the dynamical system works out just like above, but using  CoupledODEs  instead of  DeterministicIteratedMap . First, we make the dynamic rule function. Since this dynamical system can be arbitrarily high-dimensional, we prefer to use the  in-place  form for  f , overwriting in place the rate of change in a pre-allocated container. It is  customary  to append the name of functions that modify their arguments in-place with a bang ( ! ). function lorenz96_rule!(du, u, p, t)\n    F = p[1]; N = length(u)\n    # 3 edge cases\n    du[1] = (u[2] - u[N - 1]) * u[N] - u[1] + F\n    du[2] = (u[3] - u[N]) * u[1] - u[2] + F\n    du[N] = (u[1] - u[N - 2]) * u[N - 1] - u[N] + F\n    # then the general case\n    for n in 3:(N - 1)\n        du[n] = (u[n + 1] - u[n - 2]) * u[n - 1] - u[n] + F\n    end\n    return nothing # always `return nothing` for in-place form!\nend lorenz96_rule! (generic function with 1 method) then, like before, we define an initial state and parameters, and initialize the system N = 6\nu0 = range(0.1, 1; length = N)\np0 = [8.0]\nlorenz96 = CoupledODEs(lorenz96_rule!, u0, p0) 6-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      true\n dynamic rule:  lorenz96_rule!\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    [8.0]\n time:          0.0\n state:         [0.1, 0.28, 0.46, 0.64, 0.82, 1.0]\n and, again like before, we may obtain a trajectory the same way total_time = 12.5\nsampling_time = 0.02\nY, t = trajectory(lorenz96, total_time; Ttr = 2.2, Δt = sampling_time)\nY 6-dimensional StateSpaceSet{Float64} with 626 points\n  3.15368   -4.40493  0.0311581  0.486735  1.89895   4.15167\n  2.71382   -4.39303  0.395019   0.66327   2.0652    4.32045\n  2.25088   -4.33682  0.693967   0.879701  2.2412    4.46619\n  1.7707    -4.24045  0.924523   1.12771   2.42882   4.58259\n  1.27983   -4.1073   1.08656    1.39809   2.62943   4.66318\n  0.785433  -3.94005  1.18319    1.6815    2.84384   4.70147\n  0.295361  -3.74095  1.2205     1.96908   3.07224   4.69114\n -0.181932  -3.51222  1.20719    2.25296   3.3139    4.62628\n -0.637491  -3.25665  1.154      2.5267    3.56698   4.50178\n -1.06206   -2.9781   1.07303    2.7856    3.82827   4.31366\n  ⋮                                                  ⋮\n  3.17245    2.3759   3.01796    7.27415   7.26007  -0.116002\n  3.29671    2.71146  3.32758    7.5693    6.75971  -0.537853\n  3.44096    3.09855  3.66908    7.82351   6.13876  -0.922775\n  3.58387    3.53999  4.04452    8.01418   5.39898  -1.25074\n  3.70359    4.03513  4.45448    8.1137    4.55005  -1.5042\n  3.78135    4.57879  4.89677    8.09013   3.61125  -1.66943\n  3.80523    5.16112  5.36441    7.90891   2.61262  -1.73822\n  3.77305    5.7684   5.84318    7.53627   1.59529  -1.71018\n  3.6934     6.38507  6.30923    6.94454   0.61023  -1.59518 We can't scatterplot something 6-dimensional but we can visualize all timeseries fig = Figure()\nax = Axis(fig[1, 1]; xlabel = \"time\", ylabel = \"variable\")\nfor var in columns(Y)\n    lines!(ax, t, var)\nend\nfig"},{"id":47,"pagetitle":"Overarching tutorial","title":"ODE solving","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#ODE-solving","content":" ODE solving Continuous time dynamical systems are evolved through DifferentialEquations.jl. When initializing a  CoupledODEs  you can tune the solver properties to your heart's content using any of the  ODE solvers  and any of the  common solver options . For example: using OrdinaryDiffEq # accessing the ODE solvers\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\nlorenz96_vern = ContinuousDynamicalSystem(lorenz96_rule!, u0, p0; diffeq) 6-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      true\n dynamic rule:  lorenz96_rule!\n ODE solver:    Vern9\n ODE kwargs:    (abstol = 1.0e-9, reltol = 1.0e-9)\n parameters:    [8.0]\n time:          0.0\n state:         [0.1, 0.28, 0.46, 0.64, 0.82, 1.0]\n Y, t = trajectory(lorenz96_vern, total_time; Ttr = 2.2, Δt = sampling_time)\nY[end] 6-element SVector{6, Float64} with indices SOneTo(6):\n  3.8390248122407535\n  6.155709531170093\n  6.080625689047345\n  7.278588308970321\n  1.258215221210072\n -1.5297062916990858"},{"id":48,"pagetitle":"Overarching tutorial","title":"Using dynamical systems","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Using-dynamical-systems","content":" Using dynamical systems You may use the  DynamicalSystem  interface to develop algorithms that utilize dynamical systems with a known evolution rule. The two main packages of the library that do this are  ChaosTools  and  Attractors . For example, you may want to compute the Lyapunov spectrum of the Lorenz96 system from above. This is as easy as calling the  lyapunovspectrum  function with  lorenz96 steps = 10_000\nlyapunovspectrum(lorenz96, steps) 6-element Vector{Float64}:\n  0.9480080466096642\n  0.00012614364764332677\n -0.1568159270952503\n -0.747794014329633\n -1.4066157568705158\n -4.636905163494868 As expected, there is at least one positive Lyapunov exponent, because the system is chaotic, and at least one zero Lyapunov exponent, because the system is continuous time. Alternatively, you may want to estimate the basins of attraction of a multistable dynamical system. The Henon map is \"multistable\" in the sense that some initial conditions diverge to infinity, and some others converge to a chaotic attractor. Computing these basins of attraction is simple with  Attractors , and would work as follows: # define a state space grid to compute the basins on:\nxg = yg = range(-2, 2; length = 201)\n# find attractors using recurrences in state space:\nmapper = AttractorsViaRecurrences(henon, (xg, yg); sparse = false)\n# compute the full basins of attraction:\nbasins, attractors = basins_of_attraction(mapper; show_progress = false) ([-1 -1 … -1 -1; -1 -1 … -1 -1; … ; -1 -1 … -1 -1; -1 -1 … -1 -1], Dict{Int64, StateSpaceSet{2, Float64}}(1 => 2-dimensional StateSpaceSet{Float64} with 416 points)) fig, ax = heatmap(xg, yg, basins)\nx, y = columns(X) # attractor of Henon map\nscatter!(ax, x, y; color = \"black\")\nfig You could also be using a  DynamicalSystem  instance directly to build your own algorithm if it isn't already implemented (and then later contribute it so it  is  implemented ;) ). A dynamical system can be evolved forwards in time using  step! : henon 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          5\n state:         [-1.5266434026801804e8, -3132.7519146699206]\n Notice how the time is not 0, because  henon  has already been stepped when we called the function  basins_of_attraction  with it. We can step it more: step!(henon) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          6\n state:         [-3.262896110526e16, -4.579930208040541e7]\n step!(henon, 2) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          8\n state:         [-3.110262842032839e66, -4.4715262317959936e32]\n For more information on how to directly use  DynamicalSystem  instances, see the documentation of  DynamicalSystemsBase ."},{"id":49,"pagetitle":"Overarching tutorial","title":"State space sets","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#State-space-sets","content":" State space sets Let's recall that the output of the  trajectory  function is a  StateSpaceSet : X 2-dimensional StateSpaceSet{Float64} with 10001 points\n  0.2        0.3\n  1.244      0.06\n -1.10655    0.3732\n -0.341035  -0.331965\n  0.505208  -0.102311\n  0.540361   0.151562\n  0.742777   0.162108\n  0.389703   0.222833\n  1.01022    0.116911\n -0.311842   0.303065\n  ⋮         \n -0.582534   0.328346\n  0.853262  -0.17476\n -0.194038   0.255978\n  1.20327   -0.0582113\n -1.08521    0.36098\n -0.287758  -0.325562\n  0.558512  -0.0863275\n  0.476963   0.167554\n  0.849062   0.143089 It is printed like a matrix where each column is the timeseries of each dynamic variable. In reality, it is a vector of statically sized vectors (for performance reasons). When indexed with 1 index, it behaves like a vector of vectors X[1] 2-element SVector{2, Float64} with indices SOneTo(2):\n 0.2\n 0.3 X[2:5] 2-dimensional StateSpaceSet{Float64} with 4 points\n  1.244      0.06\n -1.10655    0.3732\n -0.341035  -0.331965\n  0.505208  -0.102311 When indexed with two indices, it behaves like a matrix X[2:5, 2] 4-element Vector{Float64}:\n  0.06\n  0.3732\n -0.3319651199999999\n -0.10231059085086688 When iterated, it iterates over the contained points for (i, point) in enumerate(X)\n    @show point\n    i > 5 && break\nend point = [0.2, 0.3]\npoint = [1.244, 0.06]\npoint = [-1.1065503999999997, 0.3732]\npoint = [-0.34103530283622296, -0.3319651199999999]\npoint = [0.5052077711071681, -0.10231059085086688]\npoint = [0.5403605603672313, 0.1515623313321504] map(point -> point[1] + point[2], X) 10001-element Vector{Float64}:\n  0.5\n  1.304\n -0.7333503999999997\n -0.6730004228362229\n  0.40289718025630117\n  0.6919228916993818\n  0.9048851501617762\n  0.6125365596336813\n  1.1271278272148746\n -0.008777065619615998\n  ⋮\n -0.2541879392427324\n  0.678501271515278\n  0.061940665344374896\n  1.145056192451011\n -0.7242249528790483\n -0.6133198017049188\n  0.47218423998951875\n  0.6445165778497133\n  0.9921511619004666 The columns of the set are obtained with the convenience  columns  function x, y = columns(X)\nsummary.((x, y)) (\"10001-element Vector{Float64}\", \"10001-element Vector{Float64}\")"},{"id":50,"pagetitle":"Overarching tutorial","title":"Using state space sets","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Using-state-space-sets","content":" Using state space sets Several packages of the library deal with  StateSpaceSets . You could use  ComplexityMeasures  to obtain the entropy, or other complexity measures, of a given set. Below, we obtain the entropy of the natural density of the chaotic attractor by partitioning into a histogram of approximately  50  bins per dimension: prob_est = ValueHistogram(50)\nentropy(prob_est, X) 7.825799208736613 Alternatively, you could use  FractalDimensions  to get the fractal dimensions of the chaotic attractor of the henon map using the Grassberger-Procaccia algorithm: grassberger_proccacia_dim(X) 1.2232922815092426 Or, you could obtain a recurrence matrix of a state space set with  RecurrenceAnalysis R = RecurrenceMatrix(Y, 8.0)\nRg = grayscale(R)\nrr = recurrencerate(R)\nheatmap(Rg; colormap = :grays,\n    axis = (title = \"recurrence rate = $(rr)\", aspect = 1,)\n)"},{"id":51,"pagetitle":"Overarching tutorial","title":"More nonlinear timeseries analysis","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#More-nonlinear-timeseries-analysis","content":" More nonlinear timeseries analysis A  trajectory  of a known dynamical system is one way to obtain a  StateSpaceSet . However, another common way is via a delay coordinates embedding of a measured/observed timeseries. For example, we could use  optimal_separated_de  from  DelayEmbeddings  to create an optimized delay coordinates embedding of a timeseries w = Y[:, 1] # first variable of Lorenz96\n𝒟, τ, e = optimal_separated_de(w)\n𝒟 5-dimensional StateSpaceSet{Float64} with 558 points\n  3.15369   -2.40036    1.60497   2.90499  5.72572\n  2.71384   -2.24811    1.55832   3.04987  5.6022\n  2.2509    -2.02902    1.50499   3.20633  5.38629\n  1.77073   -1.75077    1.45921   3.37699  5.07029\n  1.27986   -1.42354    1.43338   3.56316  4.65003\n  0.785468  -1.05974    1.43672   3.76473  4.12617\n  0.295399  -0.673567   1.47423   3.98019  3.50532\n -0.181891  -0.280351   1.54635   4.20677  2.80048\n -0.637447   0.104361   1.64932   4.44054  2.03084\n -1.06201    0.465767   1.77622   4.67654  1.22067\n  ⋮                                        \n  7.42111    9.27879   -1.23936   5.15945  3.25618\n  7.94615    9.22663   -1.64222   5.24344  3.34749\n  8.40503    9.13776   -1.81947   5.26339  3.46932\n  8.78703    8.99491   -1.77254   5.22631  3.60343\n  9.08701    8.77963   -1.51823   5.13887  3.72926\n  9.30562    8.47357   -1.08603   5.00759  3.82705\n  9.4488     8.06029   -0.514333  4.83928  3.88137\n  9.52679    7.52731    0.153637  4.6414   3.88458\n  9.55278    6.86845    0.873855  4.42248  3.83902 and compare fig = Figure()\naxs = [Axis3(fig[1, i]) for i in 1:2]\nfor (S, ax) in zip((Y, 𝒟), axs)\n    lines!(ax, S[:, 1], S[:, 2], S[:, 3])\nend\nfig Since  𝒟  is just another state space set, we could be using any of the above analysis pipelines on it just as easily. The last package to mention here is  TimeseriesSurrogates , which ties with all other observed/measured data analysis by providing a framework for confidence/hypothesis testing. For example, if we had a measured timeseries but we were not sure whether it represents a deterministic system with structure in the state space, or mostly noise, we could do a surrogate test. For this, we use  surrogenerator  and  RandomFourier  from  TimeseriesSurrogates , and the  generalized_dim  from  FractalDimensions  (because it performs better in noisy sets) x = X[:, 1] # Henon map timeseries\n# contaminate with noise\nusing Random: Xoshiro\nrng = Xoshiro(1234)\nx .+= randn(rng, length(x))/100\n# compute noise-contaminated fractal dim.\nΔ_orig = generalized_dim(embed(x, 2, 1)) 1.3801073957979793 And we do the surrogate test surrogate_method = RandomFourier()\nsgen = surrogenerator(x, surrogate_method, rng)\n\nΔ_surr = map(1:1000) do i\n    s = sgen()\n    generalized_dim(embed(s, 2, 1))\nend 1000-element Vector{Float64}:\n 1.8297218640747606\n 1.8449246422083758\n 1.827998413768852\n 1.8301078704767055\n 1.8107042515928138\n 1.83249783593657\n 1.8300954188512213\n 1.8416570396197014\n 1.8575804541517287\n 1.821435647618282\n ⋮\n 1.8768262063118628\n 1.8287938131103985\n 1.8435474417451545\n 1.8129026565561648\n 1.8551700924676993\n 1.8283378225272842\n 1.821134647531232\n 1.8369834750866052\n 1.8368699339310084 and visualize the test result fig, ax = hist(Δ_surr)\nvlines!(ax, Δ_orig)\nfig since the real value is outside the distribution we have confidence the data are not pure noise."},{"id":52,"pagetitle":"Overarching tutorial","title":"Integration with ModelingToolkit.jl","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Integration-with-ModelingToolkit.jl","content":" Integration with ModelingToolkit.jl DynamicalSystems.jl understands when a model has been generated via  ModelingToolkit.jl . The symbolic variables used in ModelingToolkit.jl can be used to access the state or parameters of the dynamical system. To access this functionality, the  DynamicalSystem  must be created from a  DEProblem  of the SciML ecosystem, and the  DEProblem  itself must be created from a ModelingToolkit.jl model. ProcessBasedModelling.jl ProcessBasedModelling.jl is an extension to ModelingToolkit.jl for creating models from a set of equations. It has been designed to be useful for scenarios applicable to a typical nonlinear dynamics analysis workflow, and provides better error messages during system construction than MTK. Have a look  at its docs ! Let's create a the Roessler system as an MTK model: using ModelingToolkit\n\n@variables t # use unitless time\nD = Differential(t)\n@mtkmodel Roessler begin\n    @parameters begin\n        a = 0.2\n        b = 0.2\n        c = 5.7\n    end\n    @variables begin\n        x(t) = 1.0\n        y(t) = 0.0\n        z(t) = 0.0\n        nlt(t) # nonlinear term\n    end\n    @equations begin\n        D(x) ~ -y -z\n        D(y) ~ x + a*y\n        D(z) ~ b + nlt\n        nlt ~ z*(x - c)\n    end\nend\n\n@mtkbuild model = Roessler() \\[ \\begin{align}\n\\frac{\\mathrm{d} x\\left( t \\right)}{\\mathrm{d}t} =&  - y\\left( t \\right) - z\\left( t \\right) \\\\\n\\frac{\\mathrm{d} y\\left( t \\right)}{\\mathrm{d}t} =& x\\left( t \\right) + a y\\left( t \\right) \\\\\n\\frac{\\mathrm{d} z\\left( t \\right)}{\\mathrm{d}t} =& b + \\mathrm{nlt}\\left( t \\right)\n\\end{align}\n \\] this model can then be made into an  ODEProblem : prob = ODEProblem(model) ODEProblem  with uType  Vector{Float64}  and tType  Nothing . In-place:  true \ntimespan: (nothing, nothing)\nu0: 3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0 (notice that because we specified initial values for all parameters and variables during the model creation  we do need to provide additional initial values) Now, this problem can be made into a  CoupledODEs : roessler = CoupledODEs(prob) 3-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      true\n dynamic rule:  f\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    ModelingToolkit.MTKParameters{Tuple{Vector{Float64}}, Tuple{}, Tuple{}, Tuple{}, Tuple{}, Nothing, Nothing}(([0.2, 0.2, 5.7],), (), (), (), (), nothing, nothing)\n time:          0.0\n state:         [1.0, 0.0, 0.0]\n This dynamical system instance can be used in the rest of the library like anything else. Additionally, you can \"observe\" referenced symbolic variables: observe_state(roessler, model.x) 1.0 observe_state(roessler, model.nlt) -0.0 These observables can also be used in the GUI visualization  interactive_trajectory_timeseries . You can also symbolically alter parameters current_parameter(roessler, model.c) 5.7 set_parameter!(roessler, model.c, 5.0) current_parameter(roessler, model.c) 5.0 This symbolic indexing can be given anywhere in the ecosystem where you would be altering the parameters."},{"id":53,"pagetitle":"Overarching tutorial","title":"Core components reference","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Core-components-reference","content":" Core components reference"},{"id":54,"pagetitle":"Overarching tutorial","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. source"},{"id":55,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description A  DynamicalSystem represents the time evolution of a state in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is typically a standard Julia function, see the online documentation for examples. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f . Finite  measured  or  sampled  data from a dynamical system are represented using  StateSpaceSet . Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. See also the DynamicalSystems.jl tutorial online for examples making dynamical systems. Integration with ModelingToolkit.jl Dynamical systems that have been constructed from  DEProblem s that themselves have been constructed from ModelingToolkit.jl keep a reference to the symbolic model and all symbolic variables. Accessing a  DynamicalSystem  using symbolic variables is possible via the functions  observe_state ,  set_state! ,  current_parameter  and  set_parameter! . The referenced MTK model corresponding to the dynamical system can be obtained with  model = referrenced_sciml_model(ds::DynamicalSystem) . See also the DynamicalSystems.jl tutorial online for an example. ModelingToolkit.jl v9 In ModelingToolkit.jl v9 the default  split  behavior of the parameter container is  true . This means that the parameter container is no longer a  Vector{Float64}  by default, which means that you cannot use integers to access parameters. It is recommended to keep  split = true  (default) and only access parameters via their symbolic parameter binding. Use  structural_simplify(sys; split = false)  to allow accessing parameters with integers again. API The API that  DynamicalSystem  employs is composed of the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can queried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - obtain information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state observe_state current_parameters current_parameter initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace successful_step referrenced_sciml_model API - alter status reinit! set_state! set_parameter! set_parameters! source"},{"id":56,"pagetitle":"Overarching tutorial","title":"Dynamical system implementations","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Dynamical-system-implementations","content":" Dynamical system implementations"},{"id":57,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see the DynamicalSystems.jl tutorial. source"},{"id":58,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.CoupledODEs","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see the DynamicalSystems.jl tutorial. DifferentialEquations.jl interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(; stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false),), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  . The convenience constructors  CoupledODEs(prob::ODEProblem [, diffeq])  and  CoupledODEs(ds::CoupledODEs [, diffeq])  are also available. To integrate with ModelingToolkit.jl, the dynamical system  must  be created via the  ODEProblem  (which itself is created via ModelingToolkit.jl), see the Tutorial for an example. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available to extract the problem. source"},{"id":59,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap . source"},{"id":60,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.PoincareMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap) source"},{"id":61,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\nprods = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(prods, [0.2, 0.4])\nstep!(prods)\ncurrent_state(prods) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\nprods = # same as in above example... source"},{"id":62,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.ArbitrarySteppable","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.ArbitrarySteppable","content":" DynamicalSystemsBase.ArbitrarySteppable  —  Type ArbitrarySteppable <: DiscreteTimeDynamicalSystem\nArbitrarySteppable(\n    model, step!, extract_state, extract_parameters, reset_model!;\n    isdeterministic = true, set_state = reinit!,\n) A dynamical system generated by an arbitrary \"model\" that can be stepped  in-place  with some function  step!(model)  for 1 step. The state of the model is extracted by the  extract_state(model) -> u  function The parameters of the model are extracted by the  extract_parameters(model) -> p  function. The system may be re-initialized, via  reinit! , with the  reset_model!  user-provided function that must have the call signature reset_model!(model, u, p) given a (potentially new) state  u  and parameter container  p , both of which will default to the initial ones in the  reinit!  call. ArbitrarySteppable  exists to provide the DynamicalSystems.jl interface to models from other packages that could be used within the DynamicalSystems.jl library.  ArbitrarySteppable  follows the  DynamicalSystem  interface with the following adjustments: initial_time  is always 0, as time counts the steps the model has taken since creation or last  reinit!  call. set_state!  is the same as  reinit!  by default. If not, the keyword argument  set_state  is a function  set_state(model, u)  that sets the state of the model to  u . The keyword  isdeterministic  should be set properly, as it decides whether downstream algorithms should error or not. source"},{"id":63,"pagetitle":"Overarching tutorial","title":"Learn more","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/tutorial/#Learn-more","content":" Learn more To learn more, you need to visit the documentation pages of the modules that compose DynamicalSystems.jl. See the  contents  page for more! DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":66,"pagetitle":"Animations, GUIs, Visuals","title":"Interactive GUIs, animations, visualizations","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#visualizations","content":" Interactive GUIs, animations, visualizations Using the functionality of package extensions in Julia v1.9+, DynamicalSystems.jl provides various visualization tools as soon as the  Makie  package comes into scope (i.e., when  using Makie  or any of its backends like  GLMakie ). The main functionality is  interactive_trajectory  that allows building custom GUI apps for visualizing the time evolution of dynamical systems. The remaining GUI applications in this page are dedicated to more specialized scenarios."},{"id":67,"pagetitle":"Animations, GUIs, Visuals","title":"Interactive- or animated trajectory evolution","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Interactive-or-animated-trajectory-evolution","content":" Interactive- or animated trajectory evolution The following GUI is obtained with the function  interactive_trajectory_timeseries  and the code snippet below it! using DynamicalSystems, GLMakie, ModelingToolkit\n# Import canonical time from MTK, however use the unitless version\nusing ModelingToolkit: t_nounits as t\n\n# Define the variables and parameters in symbolic format\n@parameters begin\n    a = 0.29\n    b = 0.14\n    c = 4.52\n    d = 1.0\nend\n@variables begin\n    x(t) = 10.0\n    y(t) = 0.0\n    z(t) = 1.0\n    nlt(t) # nonlinear term\nend\n\n# Create the equations of the model\neqs = [\n    Differential(t)(x) ~ -y - z,\n    Differential(t)(y) ~ x + a*y,\n    Differential(t)(z) ~ b + nlt - z*c,\n    nlt ~ d*z*x, # observed variable\n]\n\n# Create the model via ModelingToolkit\n@named roessler = ODESystem(eqs, t)\n# Do not split parameters so that integer indexing can be used as well\nmodel = structural_simplify(roessler; split = false)\n# Cast it into an `ODEProblem` and then into a `DynamicalSystem`.\n# Due to low-dimensionality it is preferred to cast into out of place\nprob = ODEProblem{false}(model, nothing, (0.0, Inf); u0_constructor = x->SVector(x...))\nds = CoupledODEs(prob)\n# If you have \"lost\" the model, use:\nmodel = referrenced_sciml_model(ds)\n\n# Define which parameters will be interactive during the simulation\nparameter_sliders = Dict(\n    # can use integer indexing\n    1 => 0:0.01:1,\n    # the global scope symbol\n    b => 0:0.01:1,\n    # the symbol obtained from the MTK model\n    model.c => 0:0.01:10,\n    # or a `Symbol` with same name as the parameter\n    # (which is the easiest and recommended way)\n    :d => 0.8:0.01:1.2,\n)\n\n# Define what variables will be visualized as timeseries\npower(u) = sqrt(u[1]*u[1] + u[2]*u[2])\nobservables = [\n    1,         # can use integer indexing\n    z,         # MTK state variable (called \"unknown\")\n    model.nlt, # MTK observed variable\n    :y,        # `Symbol` instance with same name as symbolic variable\n    power,     # arbitrary function of the state\n    x^2 - y^2, # arbitrary symbolic expression of symbolic variables\n]\n\n# Define what variables will be visualized as state space trajectory\n# same as above, any indexing works, but ensure to make the vector `Any`\n# so that integers are not converted to symbolic variables\nidxs = Any[1, y, 3]\n\nu0s = [\n    # we can specify dictionaries, each mapping the variable to its value\n    # un-specified variables get the value they currently have in `ds`\n    Dict(:x => -4, :y => -4, :z => 0.1),\n    Dict(:x => 4, :y => 3, :z => 0.1),\n    Dict(:x => -5.72),\n    Dict(:x => 5.72, :y => 0.28, :z => 0.21),\n]\n\nupdate_theme!(fontsize = 14)\ntail = 1000\n\nfig, dsobs = interactive_trajectory_timeseries(ds, observables, u0s;\n    parameter_sliders, Δt = 0.01, tail, idxs,\n    figure = (size = (1100, 650),)\n)\n\nstep!(dsobs, 2tail)\n\ndisplay(fig)"},{"id":68,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_trajectory_timeseries","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_trajectory_timeseries","content":" DynamicalSystems.interactive_trajectory_timeseries  —  Function interactive_trajectory_timeseries(ds::DynamicalSystem, fs, [, u0s]; kwargs...) → fig, dsobs Create a Makie  Figure  to visualize trajectories and timeseries of observables of  ds . This  Figure  can also be used as an interactive GUI to enable interactive control over parameters and time evolution. It can also be used to create videos, as well as customized animations, see below. fs  is a  Vector  of \"indices to observe\", i.e., anything that can be given to  observe_state . Each observation index will make a timeseries plot.  u0s  is a  Vector  of initial conditions. Each is evolved with a unique color and displayed both as a trajectory in state space and as an observed timeseries. Elements of  u0  can be either  Vector{Real}  encoding a full state or  Dict  to partially set a state from current state of  ds  (same as in  set_state! ). The trajectories from the initial conditions in  u0s  are all evolved and visualized in parallel. By default only the current state of the system is used.  u0s  can be anything accepted by a  ParallelDynamicalSystem . Return Return  fig, dsobs::DynamicalSystemObservable .  fig  is the created  Figure .  dsobs  facilities the creation of custom animations and/or interactive applications, see the custom animations section below. See also  interactive_trajectory . Interactivity and time stepping keywords GUI functionality is possible when the plotting backend is  GLMakie . Do  using GLMakie; GLMakie.activate!()  to ensure this is the chosen backend. add_controls = true : If  true , below the state space axis some buttons for animating the trajectories live are added: reset : results the parallel trajectories to their initial conditions run : when clicked it evolves the trajectories forwards in time indefinitely. click again to stop the evolution. step : when clicked it evolves the trajectories forwards in time for the amount of steps chosen by the slider to its right. The plotted trajectories can always be evolved manually using the custom animations etup that we describe below;  add_controls  only concerns the buttons and interactivity added to the created figure. parameter_sliders = nothing : If given, it must be a dictionary, mapping parameter indices (any valid index that can be given to  set_parameter! ) to ranges of parameter values. Each combination of index and range becomes a slider that can be interactively controlled to alter a system parameter on the fly during time evolution. Below the parameter sliders, three buttons are added for GUI usage: update : when clicked the chosen parameter values are propagated into the system u.r.s. : when clicked it is equivalent with clicking in order: \"update\", \"reset\", \"step\". reset p : when clicked it resets Parameters can also be altered using the custom animation setup that we describe below;  parameter_sliders  only conserns the buttons and interactivity added to the created figure. parameter_names = Dict(keys(ps) .=> string.(keys(ps))) : Dictionary mapping parameter keys to labels. Only used if  parameter_sliders  is given. Δt : Time step of time evolution. Defaults to 1 for discrete time, 0.01 for continuous time systems. For internal simplicity, continuous time dynamical systems are evolved non-adaptively with constant step size equal to  Δt . pause = nothing : If given, it must be a real number. This number is given to the  sleep  function, which is called between each plot update. Useful when time integration is computationally inexpensive and animation proceeds too fast. starting_step = 1 : the starting value of the \"step\" slider. Visualization keywords colors : The color for each initial condition (and resulting trajectory and timeseries). Needs to be a  Vector  of equal length as  u0s . tail = 1000 : Length of plotted trajectory (in units of  Δt ). fade = 0.5 : The trajectories in state space are faded towards full transparency. The alpha channel (transparency) scales as  t^fade  with  t  ranging from 0 to 1 (1 being the end of the trajectory). Use  fade = 1.0  for linear fading or  fade = 0  for no fading. Current default makes fading progress faster at trajectory start and slower at trajectory end. markersize = 15 : Size of markers of trajectory endpoints. For discrete systems half of that is used for the trajectory tail. plotkwargs = NamedTuple() : A named tuple of keyword arguments propagated to the state space plot ( lines  for continuous,  scatter  for discrete systems).  plotkwargs  can also be a vector of named tuples, in which case each initial condition gets different arguments. Statespace trajectory keywords idxs = 1:min(length(u0s[1]), 3) : Which variables to plot in a state space trajectory. Any index that can be given to  observe_state  can be given here. statespace_axis = true : Whether to create and display an axis for the trajectory plot. idxs = 1:min(length(u0s[1]), 3) : Which variables to plot in a state space trajectory. Any index that can be given to  observe_state  can be given here. If three indices are given, the trajectory plot is also 3D, otherwise 2D. lims : A tuple of tuples (min, max) for the axis limits. If not given, they are automatically deduced by evolving each of  u0s  1000  Δt  units and picking most extreme values (limits are  not  adjusted by default during the live animations). figure, axis : both can be named tuples with arbitrary keywords propagated to the generation of the  Figure  and state space  Axis  instances. Timeseries keywords linekwargs = NamedTuple() : Extra keywords propagated to the timeseries plots. Can also be a vector of named tuples, each one for each unique initial condition. timeseries_names : A vector of strings with length equal to  fs  giving names to the y-labels of the timeseries plots. timeseries_ylims : A vector of 2-tuples for the lower and upper limits of the y-axis of each timeseries plot. If not given it is deduced automatically similarly to  lims . timeunit = 1 : the units of time, if any. Sets the units of the timeseries x-axis. timelabel = \"time\" : label of the x-axis of the timeseries plots. Custom animations The second return argument  dsobs  is a  DynamicalSystemObservable . The trajectories plotted in the main panel are linked to observables that are fields of the  dsobs . Specifically, the field  dsobs.state_obserable  is an observable containing the final state of each of the trajectories, i.e., a vector of vectors like  u0s .  dsobs.param_observable  is an observable of the system parameters. These observables are triggered by the interactive GUI buttons (the first two when the system is stepped in time, the last one when the parameters are updated). However, these observables, and hence the corresponding plotted trajectories that are  map ed from these observables, can be updated via the formal API of  DynamicalSystem : step!(dsobs, n::Int = 1) will step the system for  n  steps of  Δt  time, and only update the plot on the last step.  set_parameter!(dsobs, index, value)  will update the system parameter and then trigger the parameter observable. Lastly,  set_state!(dsobs, new_u [, i])  will set the  i -th system state and clear the trajectory plot to the new initial condition. This information can be used to create custom animations and/or interactive apps. In principle, the only thing a user has to do is create new observables from the existing ones using e.g. the  on  function and plot these new observables. Various examples are provided in the online documentation. source"},{"id":69,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_trajectory","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_trajectory","content":" DynamicalSystems.interactive_trajectory  —  Function interactive_trajectory(ds::DynamicalSystem [, u0s]; kwargs...) → fig, dsobs Same as  interactive_trajectory_timeseries , but does not plot any timeseries only the trajectory in a (projected) state space. source"},{"id":70,"pagetitle":"Animations, GUIs, Visuals","title":"Example 1: interactive trajectory animation","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Example-1:-interactive-trajectory-animation","content":" Example 1: interactive trajectory animation using DynamicalSystems, CairoMakie\nF, G, a, b = 6.886, 1.347, 0.255, 4.0\nds = PredefinedDynamicalSystems.lorenz84(; F, G, a, b)\n\nu1 = [0.1, 0.1, 0.1] # periodic\nu2 = u1 .+ 1e-3     # fixed point\nu3 = [-1.5, 1.2, 1.3] .+ 1e-9 # chaotic\nu4 = [-1.5, 1.2, 1.3] .+ 21e-9 # chaotic 2\nu0s = [u1, u2, u3, u4]\n\nfig, dsobs = interactive_trajectory(\n    ds, u0s; tail = 1000, fade = true,\n    idxs = [1,3],\n)\n\nfig We could interact with this plot live, like in the example video above. We can also progress the visuals via code as instructed by  interactive_trajectory  utilizing the second returned argument  dsobs : step!(dsobs, 2000)\nfig (if you progress the visuals via code you probably want to give  add_controls = false  as a keyword to  interactive_trajectory )"},{"id":71,"pagetitle":"Animations, GUIs, Visuals","title":"Example 2: Adding parameter-dependent elements to a plot","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Example-2:-Adding-parameter-dependent-elements-to-a-plot","content":" Example 2: Adding parameter-dependent elements to a plot In this advanced example we add plot elements to the provided figure, and also utilize the parameter observable in  dsobs  to add animated plot elements that update whenever a parameter updates. The final product of this snippet is in fact the animation at the top of the docstring of  interactive_trajectory_panel . We start with an interactive trajectory panel of the Lorenz63 system, in which we also add sliders for interactively changing parameter values using DynamicalSystems, CairoMakie\n\nps = Dict(\n    1 => 1:0.1:30,\n    2 => 10:0.1:50,\n    3 => 1:0.01:10.0,\n)\npnames = Dict(1 => \"σ\", 2 => \"ρ\", 3 => \"β\")\n\nlims = ((-30, 30), (-30, 30), (0, 100))\n\nds = PredefinedDynamicalSystems.lorenz()\n\nu1 = [10,20,40.0]\nu3 = [20,10,40.0]\nu0s = [u1, u3]\n\nfig, dsobs = interactive_trajectory(\n    ds, u0s; parameter_sliders = ps, pnames, lims\n)\n\nfig If now one interactively clicked (if using GLMakie) the parameter sliders and then update, the system parameters would be updated accordingly. We can also add new plot elements that depend on the parameter values using the  dsobs : # Fixed points of the lorenz system (without the origin)\nlorenz_fixedpoints(ρ,β) = [\n    Point3f(sqrt(β*(ρ-1)), sqrt(β*(ρ-1)), ρ-1),\n    Point3f(-sqrt(β*(ρ-1)), -sqrt(β*(ρ-1)), ρ-1),\n]\n\n# add an observable trigger to the system parameters\nfpobs = map(dsobs.param_observable) do params\n    σ, ρ, β = params\n    return lorenz_fixedpoints(ρ, β)\nend\n\n# If we want to plot directly on the trajectory axis, we need to\n# extract it from the figure. The first entry of the figure is a grid layout\n# containing the axis and the GUI controls. The [1,1] entry of the layout\n# is the axis containing the trajectory plot\n\nax = content(fig[1,1][1,1])\nscatter!(ax, fpobs; markersize = 10, marker = :diamond, color = :red)\n\nfig Now, after the live animation \"run\" button is pressed, we can interactively change the parameter ρ and click update, in which case both the dynamical system's ρ parameter will change, but also the location of the red diamonds. We can also change the parameters non-interactively using  set_parameter! set_parameter!(dsobs, 2, 50.0)\n\nfig set_parameter!(dsobs, 2, 10.0)\n\nfig Note that the sliders themselves did not change, as this functionality is for \"offline\" creation of animations where one doesn't interact with sliders. The keyword  add_controls  should be given as  false  in such scenarios."},{"id":72,"pagetitle":"Animations, GUIs, Visuals","title":"Example 3: Observed timeseries of the system","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Example-3:-Observed-timeseries-of-the-system","content":" Example 3: Observed timeseries of the system using DynamicalSystems, CairoMakie\nusing LinearAlgebra: norm, dot\n\n# Dynamical system and initial conditions\nds = Systems.thomas_cyclical(b = 0.2)\nu0s = [[3, 1, 1.], [1, 3, 1.], [1, 1, 3.]] # must be a vector of states!\n\n# Observables we get timeseries of:\nfunction distance_from_symmetry(u)\n    v = SVector{3}(1/√3, 1/√3, 1/√3)\n    t = dot(v, u)\n    return norm(u - t*v)\nend\nfs = [3, distance_from_symmetry]\n\nfig, dsobs = interactive_trajectory_timeseries(ds, fs, u0s;\n    idxs = [1, 2], Δt = 0.05, tail = 500,\n    lims = ((-2, 4), (-2, 4)),\n    timeseries_ylims = [(-2, 4), (0, 5)],\n    add_controls = false,\n    figure = (size = (800, 400),)\n)\n\nfig we can progress the simulation: step!(dsobs, 200)\nfig or we can even make a nice video out of it: record(fig, \"thomas_cycl.mp4\", 1:100) do i\n    step!(dsobs, 10)\nend \"thomas_cycl.mp4\""},{"id":73,"pagetitle":"Animations, GUIs, Visuals","title":"Cobweb Diagrams","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Cobweb-Diagrams","content":" Cobweb Diagrams"},{"id":74,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_cobweb","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_cobweb","content":" DynamicalSystems.interactive_cobweb  —  Function interactive_cobweb(ds::DiscreteDynamicalSystem, prange, O::Int = 3; kwargs...) Launch an interactive application for exploring cobweb diagrams of 1D discrete dynamical systems. Two slides control the length of the plotted trajectory and the current parameter value. The parameter values are obtained from the given  prange . In the cobweb plot, higher order iterates of the dynamic rule  f  are plotted as well, starting from order 1 all the way to the given order  O . Both the trajectory in the cobweb, as well as any iterate  f  can be turned off by using some of the buttons. Keywords fkwargs = [(linewidth = 4.0, color = randomcolor()) for i in 1:O] : plotting keywords for each of the plotted iterates of  f trajcolor = :black : color of the trajectory pname = \"p\" : name of the parameter slider pindex = 1 : parameter index xmin = 0, xmax = 1 : limits the state of the dynamical system can take Tmax = 1000 : maximum trajectory length x0s = range(xmin, xmax; length = 101) : Possible values for the x0 slider. source The animation at the top of this section was done with using DynamicalSystems, GLMakie\n\n# the second range is a convenience for intermittency example of logistic\nrrange = 1:0.001:4.0\n# rrange = (rc = 1 + sqrt(8); [rc, rc - 1e-5, rc - 1e-3])\n\nlo = Systems.logistic(0.4; r = rrange[1])\ninteractive_cobweb(lo, rrange, 5)"},{"id":75,"pagetitle":"Animations, GUIs, Visuals","title":"Orbit Diagrams","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Orbit-Diagrams","content":" Orbit Diagrams Notice that orbit diagrams and bifurcation diagrams are different things in DynamicalSystems.jl"},{"id":76,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_orbitdiagram","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_orbitdiagram","content":" DynamicalSystems.interactive_orbitdiagram  —  Function interactive_orbitdiagram(\n    ds::DynamicalSystem, p_index, pmin, pmax, i::Int = 1;\n    u0 = nothing, parname = \"p\", title = \"\"\n) Open an interactive application for exploring orbit diagrams (ODs) of discrete time dynamical systems. Requires  DynamicalSystems . In essense, the function presents the output of  orbitdiagram  of the  i th variable of the  ds , and allows interactively zooming into it. Keywords control the name of the parameter, the initial state (used for  any  parameter) or whether to add a title above the orbit diagram. Interaction The application is separated in the \"OD plot\" (left) and the \"control panel\" (right). On the OD plot you can interactively click and drag with the left mouse button to select a region in the OD. This region is then  re-computed  at a higher resolution. The options at the control panel are straight-forward, with n  amount of steps recorded for the orbit diagram (not all are in the zoomed region!) t  transient steps before starting to record steps d  density of x-axis (the parameter axis) α  alpha value for the plotted points. Notice that at each update  n*t*d  steps are taken. You have to press  update  after changing these parameters. Press  reset  to bring the OD in the original state (and variable). Pressing  back  will go back through the history of your exploration History is stored when the \"update\" button is pressed or a region is zoomed in. You can even decide which variable to get the OD for by choosing one of the variables from the wheel! Because the y-axis limits can't be known when changing variable, they reset to the size of the selected variable. Accessing the data What is plotted on the application window is a  true  orbit diagram, not a plotting shorthand. This means that all data are obtainable and usable directly. Internally we always scale the orbit diagram to [0,1]² (to allow  Float64  precision even though plotting is  Float32 -based). This however means that it is necessary to transform the data in real scale. This is done through the function  scaleod  which accepts the 5 arguments returned from the current function: figure, oddata = interactive_orbitdiagram(...)\nps, us = scaleod(oddata) source"},{"id":77,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.scaleod","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.scaleod","content":" DynamicalSystems.scaleod  —  Function scaleod(oddata) -> ps, us Given the return values of  interactive_orbitdiagram , produce orbit diagram data scaled correctly in data units. Return the data as a vector of parameter values and a vector of corresponding variable values. source The animation at the top of this section was done with i = p_index = 1\nds, p_min, p_max, parname = Systems.henon(), 0.8, 1.4, \"a\"\nt = \"orbit diagram for the Hénon map\"\n\noddata = interactive_orbitdiagram(ds, p_index, p_min, p_max, i;\n                                  parname = parname, title = t)\n\nps, us = scaleod(oddata)"},{"id":78,"pagetitle":"Animations, GUIs, Visuals","title":"Interactive Poincaré Surface of Section","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Interactive-Poincaré-Surface-of-Section","content":" Interactive Poincaré Surface of Section"},{"id":79,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_poincaresos","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_poincaresos","content":" DynamicalSystems.interactive_poincaresos  —  Function interactive_poincaresos(cds, plane, idxs, complete; kwargs...) Launch an interactive application for exploring a Poincaré surface of section (PSOS) of the continuous dynamical system  cds . Requires  DynamicalSystems . The  plane  can only be the  Tuple  type accepted by  DynamicalSystems.poincaresos , i.e.  (i, r)  for the  i th variable crossing the value  r .  idxs  gives the two indices of the variables to be displayed, since the PSOS plot is always a 2D scatterplot. I.e.  idxs = (1, 2)  will plot the 1st versus 2nd variable of the PSOS. It follows that  plane[1] ∉ idxs  must be true. complete  is a three-argument  function  that completes the new initial state during interactive use, see below. The function returns:  figure, laststate  with the latter being an observable containing the latest initial  state . Keyword Arguments direction, rootkw  : Same use as in  DynamicalSystems.poincaresos . tfinal = (1000.0, 10.0^4)  : A 2-element tuple for the range of values for the total integration time (chosen interactively). color  : A  function  of the system's initial condition, that returns a color to plot the new points with. The color must be  RGBf/RGBAf .  A random color is chosen by default. labels = (\"u₁\" , \"u₂\")  : Scatter plot labels. scatterkwargs = () : Named tuple of keywords passed to  scatter . diffeq = NamedTuple()  : Any extra keyword arguments are passed into  init  of DiffEq. Interaction The application is a standard scatterplot, which shows the PSOS of the system, initially using the system's  u0 . Two sliders control the total evolution time and the size of the marker points (which is always in pixels). Upon clicking within the bounds of the scatter plot your click is transformed into a new initial condition, which is further evolved and its PSOS is computed and then plotted into the scatter plot. Your click is transformed into a full  D -dimensional initial condition through the function  complete . The first two arguments of the function are the positions of the click on the PSOS. The third argument is the value of the variable the PSOS is defined on. To be more exact, this is how the function is called: x, y = mouseclick; z = plane[2]\nnewstate = complete(x, y, z) The  complete  function can throw an error for ill-conditioned  x, y, z . This will be properly handled instead of breaking the application. This  newstate  is also given to the function  color  that gets a new color for the new points. source To generate the animation at the start of this section you can run using InteractiveDynamics, GLMakie, OrdinaryDiffEq, DynamicalSystems\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\n\nhh = Systems.henonheiles()\n\npotential(x, y) = 0.5(x^2 + y^2) + (x^2*y - (y^3)/3)\nenergy(x,y,px,py) = 0.5(px^2 + py^2) + potential(x,y)\nconst E = energy(get_state(hh)...)\n\nfunction complete(y, py, x)\n    V = potential(x, y)\n    Ky = 0.5*(py^2)\n    Ky + V ≥ E && error(\"Point has more energy!\")\n    px = sqrt(2(E - V - Ky))\n    ic = [x, y, px, py]\n    return ic\nend\n\nplane = (1, 0.0) # first variable crossing 0\n\n# Coloring points using the Lyapunov exponent\nfunction λcolor(u)\n    λ = lyapunovs(hh, 4000; u0 = u)[1]\n    λmax = 0.1\n    return RGBf(0, 0, clamp(λ/λmax, 0, 1))\nend\n\nstate, scene = interactive_poincaresos(hh, plane, (2, 4), complete;\nlabels = (\"q₂\" , \"p₂\"),  color = λcolor, diffeq...)"},{"id":80,"pagetitle":"Animations, GUIs, Visuals","title":"Scanning a Poincaré Surface of Section","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#Scanning-a-Poincaré-Surface-of-Section","content":" Scanning a Poincaré Surface of Section"},{"id":81,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_poincaresos_scan","ref":"/DynamicalSystemsDocs.jl/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_poincaresos_scan","content":" DynamicalSystems.interactive_poincaresos_scan  —  Function interactive_poincaresos_scan(A::StateSpaceSet, j::Int; kwargs...)\ninteractive_poincaresos_scan(As::Vector{StateSpaceSet}, j::Int; kwargs...) Launch an interactive application for scanning a Poincare surface of section of  A  like a \"brain scan\", where the plane that defines the section can be arbitrarily moved around via a slider. Return  figure, ax3D, ax2D . The input dataset must be 3 dimensional, and here the crossing plane is always chosen to be when the  j -th variable of the dataset crosses a predefined value. The slider automatically gets all possible values the  j -th variable can obtain. If given multiple datasets, the keyword  colors  attributes a color to each one, e.g.  colors = [JULIADYNAMICS_COLORS[mod1(i, 6)] for i in 1:length(As)] . The keywords  linekw, scatterkw  are named tuples that are propagated as keyword arguments to the line and scatter plot respectively, while the keyword  direction = -1  is propagated to the function  DyamicalSystems.poincaresos . source The animation at the top of this page was done with using GLMakie, DynamicalSystems\nusing OrdinaryDiffEq: Vern9\n\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\nds = PredefinedDynamicalSystems.henonheiles()\nds = CoupledODEs(ds, diffeq)\n\nu0s = [\n    [0.0, -0.25, 0.42081, 0.0],\n    [0.0, 0.1, 0.5, 0.0],\n    [0.0, -0.31596, 0.354461, 0.0591255]\n]\n# inputs\ntrs = [trajectory(ds, 10000, u0)[1][:, SVector(1,2,3)] for u0 ∈ u0s]\nj = 2 # the dimension of the plane\n\ninteractive_poincaresos_scan(trs, j; linekw = (transparency = true,))"},{"id":84,"pagetitle":"Numerical Data","title":"Numerical Data","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Numerical-Data","content":" Numerical Data"},{"id":85,"pagetitle":"Numerical Data","title":"StateSpaceSets","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets","content":" StateSpaceSets  —  Module StateSpaceSets.jl A Julia package that provides functionality for state space sets. These are collections of points of fixed, and known by type, size (called dimension). It is used in several projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . The main export of  StateSpaceSets  is the concrete type  StateSpaceSet . The package also provides functionality for distances, neighbor searches, sampling, and normalization. To install it you may run  import Pkg; Pkg.add(\"StateSpaceSets\") , however, there is no real reason to install this package directly as it is re-exported by all downstream packages that use it. previously StateSpaceSets.jl was part of DelayEmbeddings.jl source Timeseries and datasets The word \"timeseries\" can be confusing, because it can mean a univariate (also called scalar or one-dimensional) timeseries or a multivariate (also called multi-dimensional) timeseries. To resolve this confusion, in  DynamicalSystems.jl  we have the following convention:  \"timeseries\"  is always univariate! it refers to a one-dimensional vector of numbers, which exists with respect to some other one-dimensional vector of numbers that corresponds to a time vector. On the other hand, we use the word  \"dataset\"  is used to refer to a  multi-dimensional  timeseries, which is of course simply a group/set of one-dimensional timeseries represented as a  StateSpaceSet . In some documentation strings we use the word \"trajectory\" instead of \"dataset\", which means an ordered multivariate timeseries. This is typically the output of the function  trajectory , or the delay embedding of a timeseries via  embed , both of which are also represented as a  StateSpaceSet ."},{"id":86,"pagetitle":"Numerical Data","title":"StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSet","content":" StateSpaceSet Trajectories, and in general sets in state space, are represented by a structure called  StateSpaceSet  in  DynamicalSystems.jl  (while timeseries are always standard Julia  Vector s). It is recommended to always  standardize  datasets."},{"id":87,"pagetitle":"Numerical Data","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. source"},{"id":88,"pagetitle":"Numerical Data","title":"StateSpaceSets.standardize","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.standardize","content":" StateSpaceSets.standardize  —  Function standardize(d::StateSpaceSet) → r Create a standardized version of the input set where each column is transformed to have mean 0 and standard deviation 1. source standardize(x::AbstractVector{<:Real}) = (x - mean(x))/std(x) source In essence a  StateSpaceSet  is simply a wrapper for a  Vector  of  SVector s. However, it is visually represented as a matrix, similarly to how numerical data would be printed on a spreadsheet (with time being the  column  direction). It also offers a lot more functionality than just pretty-printing. Besides the examples in the documentation string, you can e.g. iterate over data points using DynamicalSystems\nhen = Systems.henon()\ndata = trajectory(hen, 10000) # this returns a dataset\nfor point in data\n    # stuff\nend Most functions from  DynamicalSystems.jl  that manipulate and use multidimensional data are expecting a  StateSpaceSet . This allows us to define efficient methods that coordinate well with each other, like e.g.  embed ."},{"id":89,"pagetitle":"Numerical Data","title":"StateSpaceSet Functions","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSet-Functions","content":" StateSpaceSet Functions"},{"id":90,"pagetitle":"Numerical Data","title":"StateSpaceSets.minima","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.minima","content":" StateSpaceSets.minima  —  Function minima(dataset) Return an  SVector  that contains the minimum elements of each timeseries of the dataset. source"},{"id":91,"pagetitle":"Numerical Data","title":"StateSpaceSets.maxima","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.maxima","content":" StateSpaceSets.maxima  —  Function maxima(dataset) Return an  SVector  that contains the maximum elements of each timeseries of the dataset. source"},{"id":92,"pagetitle":"Numerical Data","title":"StateSpaceSets.minmaxima","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.minmaxima","content":" StateSpaceSets.minmaxima  —  Function minmaxima(dataset) Return  minima(dataset), maxima(dataset)  without doing the computation twice. source"},{"id":93,"pagetitle":"Numerical Data","title":"StateSpaceSets.columns","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.columns","content":" StateSpaceSets.columns  —  Function columns(ssset) -> x, y, z, ... Return the individual columns of the state space set allocated as  Vector s. Equivalent with  collect(eachcol(ssset)) . source"},{"id":94,"pagetitle":"Numerical Data","title":"StateSpaceSet distances","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSet-distances","content":" StateSpaceSet distances"},{"id":95,"pagetitle":"Numerical Data","title":"Two datasets","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Two-datasets","content":" Two datasets"},{"id":96,"pagetitle":"Numerical Data","title":"StateSpaceSets.set_distance","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.set_distance","content":" StateSpaceSets.set_distance  —  Function set_distance(ssset1, ssset2 [, distance]) Calculate a distance between two  StateSpaceSet s, i.e., a distance defined between sets of points, as dictated by  distance . Possible  distance  types are: Centroid , which is the default, and 100s of times faster than the rest Hausdorff StrictlyMinimumDistance source"},{"id":97,"pagetitle":"Numerical Data","title":"StateSpaceSets.Hausdorff","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.Hausdorff","content":" StateSpaceSets.Hausdorff  —  Type Hausdorff(metric = Euclidean()) A distance that can be used in  set_distance . The  Hausdorff distance  is the greatest of all the distances from a point in one set to the closest point in the other set. The distance is calculated with the metric given to  Hausdorff  which defaults to Euclidean. Hausdorff  is 2x slower than  StrictlyMinimumDistance , however it is a proper metric in the space of sets of state space sets. source"},{"id":98,"pagetitle":"Numerical Data","title":"StateSpaceSets.Centroid","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.Centroid","content":" StateSpaceSets.Centroid  —  Type Centroid(metric = Euclidean()) A distance that can be used in  set_distance . The  Centroid  method returns the distance (according to  metric ) between the  centroids  (a.k.a. centers of mass) of the sets. metric  can be any function that takes in two static vectors are returns a positive definite number to use as a distance (and typically is a  Metric  from Distances.jl). source"},{"id":99,"pagetitle":"Numerical Data","title":"Sets of datasets","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Sets-of-datasets","content":" Sets of datasets"},{"id":100,"pagetitle":"Numerical Data","title":"StateSpaceSets.setsofsets_distances","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.setsofsets_distances","content":" StateSpaceSets.setsofsets_distances  —  Function setsofsets_distances(a₊, a₋ [, distance]) → distances Calculate distances between sets of  StateSpaceSet s. Here   a₊, a₋  are containers of  StateSpaceSet s, and the returned distances are dictionaries of distances. Specifically,  distances[i][j]  is the distance of the set in the  i  key of  a₊  to the  j  key of  a₋ . Notice that distances from  a₋  to  a₊  are not computed at all (assumming symmetry in the distance function). The  distance  can be as in  set_distance , or it can be an arbitrary function that takes as input two state space sets and returns any positive-definite number as their \"distance\". source"},{"id":101,"pagetitle":"Numerical Data","title":"StateSpaceSet I/O","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSet-I/O","content":" StateSpaceSet I/O Input/output functionality for an  AbstractStateSpaceSet  is already achieved using base Julia, specifically  writedlm  and  readdlm . To write and read a dataset, simply do: using DelimitedFiles\n\ndata = StateSpaceSet(rand(1000, 2))\n\n# I will write and read using delimiter ','\nwritedlm(\"data.txt\", data, ',')\n\n# Don't forget to convert the matrix to a StateSpaceSet when reading\ndata = StateSpaceSet(readdlm(\"data.txt\", ',', Float64))"},{"id":102,"pagetitle":"Numerical Data","title":"Neighborhoods","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Neighborhoods","content":" Neighborhoods Neighborhoods refer to the common act of finding points in a dataset that are nearby a given point (which typically belongs in the dataset).  DynamicalSystems.jl  bases this interface on  Neighborhood.jl . You can go to its documentation if you are interested in finding neighbors in a dataset for e.g. a custom algorithm implementation. For  DynamicalSystems.jl , what is relevant are the two types of neighborhoods that exist:"},{"id":103,"pagetitle":"Numerical Data","title":"Neighborhood.NeighborNumber","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Neighborhood.NeighborNumber","content":" Neighborhood.NeighborNumber  —  Type NeighborNumber(k::Int) <: SearchType Search type representing the  k  nearest neighbors of the query (or approximate neighbors, depending on the search structure)."},{"id":104,"pagetitle":"Numerical Data","title":"Neighborhood.WithinRange","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Neighborhood.WithinRange","content":" Neighborhood.WithinRange  —  Type WithinRange(r::Real) <: SearchType Search type representing all neighbors with distance  ≤ r  from the query (according to the search structure's metric)."},{"id":105,"pagetitle":"Numerical Data","title":"Theiler window","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Theiler-window","content":" Theiler window The Theiler window is a concept that is useful when finding neighbors in a dataset that is coming from the sampling of a continuous dynamical system. As demonstrated in the figure below, it tries to eliminate spurious \"correlations\" (wrongly counted neighbors) due to a potentially dense sampling of the trajectory (e.g. by giving small sampling time in  trajectory ). The figure below demonstrates a typical  WithinRange  search around the black point with index  i . Black, red and green points are found neighbors, but points within indices  j  that satisfy  |i-j| ≤ w  should  not  be counted as \"true\" neighbors. These neighbors are typically the same around  any  state space point, and thus wrongly bias calculations by providing a non-zero baseline of neighbors. For the sketch below,  w=3  would have been used. Typically a good choice for  w  coincides with the choice an optimal delay time, see  estimate_delay , for any of the timeseries of the dataset."},{"id":106,"pagetitle":"Numerical Data","title":"Samplers","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#Samplers","content":" Samplers"},{"id":107,"pagetitle":"Numerical Data","title":"StateSpaceSets.statespace_sampler","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.statespace_sampler","content":" StateSpaceSets.statespace_sampler  —  Function statespace_sampler(region [, seed = 42]) → sampler, isinside A function that facilitates sampling points randomly and uniformly in a state space  region . It generates two functions: sampler  is a 0-argument function that when called generates a random point inside a state space  region . The point is always a  Vector  for type stability irrespectively of dimension. Generally, the generated point should be  copied  if it needs to be stored. (i.e., calling  sampler()  utilizes a shared vector)  sampler  is a thread-safe function. isinside  is a 1-argument function that returns  true  if the given state space point is inside the  region . The  region  can be an instance of any of the following types (input arguments if not specified are vectors of length  D , with  D  the state space dimension): HSphere(radius::Real, center) : points  inside  the hypersphere (boundary excluded). Convenience method  HSphere(radius::Real, D::Int)  makes the center a  D -long vector of zeros. HSphereSurface(radius, center) : points on the hypersphere surface. Same convenience method as above is possible. HRectangle(mins, maxs) : points in [min, max) for the bounds along each dimension. The random number generator is always  Xoshiro  with the given  seed . source statespace_sampler(grid::NTuple{N, AbstractRange} [, seed]) If given a  grid  that is a tuple of  AbstractVector s, the minimum and maximum of the vectors are used to make an  HRectangle  region. source"},{"id":108,"pagetitle":"Numerical Data","title":"StateSpaceSets.HSphere","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.HSphere","content":" StateSpaceSets.HSphere  —  Type HSphere(r::Real, center::Vector)\nHSphere(r::Real, D::Int) A state space region denoting all points  within  a hypersphere. source"},{"id":109,"pagetitle":"Numerical Data","title":"StateSpaceSets.HSphereSurface","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.HSphereSurface","content":" StateSpaceSets.HSphereSurface  —  Type HSphereSurface(r::Real, center::Vector)\nHSphereSurface(r::Real, D::Int) A state space region denoting all points  on the surface  (boundary) of a hypersphere. source"},{"id":110,"pagetitle":"Numerical Data","title":"StateSpaceSets.HRectangle","ref":"/DynamicalSystemsDocs.jl/statespacesets/stable/#StateSpaceSets.HRectangle","content":" StateSpaceSets.HRectangle  —  Type HRectangle(mins::Vector, maxs::Vector) A state space region denoting all points  within  the hyperrectangle. source"},{"id":115,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.jl","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.jl","content":" DynamicalSystemsBase.jl"},{"id":116,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase","content":" DynamicalSystemsBase  —  Module DynamicalSystemsBase.jl A Julia package that defines the  DynamicalSystem  interface and many concrete implementations used in the DynamicalSystems.jl ecosystem. To install it, run  import Pkg; Pkg.add(\"DynamicalSystemsBase\") . Typically, you do not want to use  DynamicalSystemsBase  directly, as downstream analysis packages re-export it. All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. source !!! note \"Tutorial and examples at DynamicalSystems.jl docs!     Please visit the documentation of the main DynamicalSystems.jl docs for a tutorial and examples on using the interface."},{"id":117,"pagetitle":"DynamicalSystemsBase.jl","title":"The  DynamicalSystem  API","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#The-DynamicalSystem-API","content":" The  DynamicalSystem  API"},{"id":118,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description A  DynamicalSystem represents the time evolution of a state in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is typically a standard Julia function, see the online documentation for examples. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f . Finite  measured  or  sampled  data from a dynamical system are represented using  StateSpaceSet . Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. See also the DynamicalSystems.jl tutorial online for examples making dynamical systems. Integration with ModelingToolkit.jl Dynamical systems that have been constructed from  DEProblem s that themselves have been constructed from ModelingToolkit.jl keep a reference to the symbolic model and all symbolic variables. Accessing a  DynamicalSystem  using symbolic variables is possible via the functions  observe_state ,  set_state! ,  current_parameter  and  set_parameter! . The referenced MTK model corresponding to the dynamical system can be obtained with  model = referrenced_sciml_model(ds::DynamicalSystem) . See also the DynamicalSystems.jl tutorial online for an example. ModelingToolkit.jl v9 In ModelingToolkit.jl v9 the default  split  behavior of the parameter container is  true . This means that the parameter container is no longer a  Vector{Float64}  by default, which means that you cannot use integers to access parameters. It is recommended to keep  split = true  (default) and only access parameters via their symbolic parameter binding. Use  structural_simplify(sys; split = false)  to allow accessing parameters with integers again. API The API that  DynamicalSystem  employs is composed of the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can queried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - obtain information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state observe_state current_parameters current_parameter initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace successful_step referrenced_sciml_model API - alter status reinit! set_state! set_parameter! set_parameters! source"},{"id":119,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_state","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_state","content":" DynamicalSystemsBase.current_state  —  Function current_state(ds::DynamicalSystem) → u::AbstractArray Return the current state of  ds . This state is mutated when  ds  is mutated. See also  initial_state ,  observe_state . source"},{"id":120,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_state","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_state","content":" DynamicalSystemsBase.initial_state  —  Function initial_state(ds::DynamicalSystem) → u0 Return the initial state of  ds . This state is never mutated and is set when initializing  ds . source"},{"id":121,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.observe_state","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.observe_state","content":" DynamicalSystemsBase.observe_state  —  Function observe_state(ds::DynamicalSystem, i, u = current_state(ds)) → x::Real Return the state  u  of  ds observed  at \"index\"  i . Possibilities are: i::Int  returns the  i -th dynamic variable. i::Function  returns  f(current_state(ds)) . i::SymbolLike  returns the value of the corresponding symbolic variable.  This is valid only for dynamical systems referrencing a ModelingToolkit.jl model  which also has  i  as one of its listed variables (either uknowns or observed).  Here  i  can be anything can be anything  that could index the solution object  sol = ModelingToolkit.solve(...) ,  such as a  Num  or  Symbol  instance with the name of the symbolic variable.  In this case, a last fourth optional positional argument  t  defaults to   current_time(ds)  and is the time to observe the state at. Any symbolic expression involving variables present in the symbolic variables tracked by the system, e.g.,  i = x^2 - y  with  x, y  symbolic variables. For  ProjectedDynamicalSystem , this function assumes that the state of the system is the full state space state, not the projected one (this makes the most sense for allowing MTK-based indexing). Use  state_name  for an accompanying name. source"},{"id":122,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.state_name","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.state_name","content":" DynamicalSystemsBase.state_name  —  Function state_name(index)::String Return a name that matches the outcome of  observe_state  with  index . source"},{"id":123,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_parameters","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_parameters","content":" DynamicalSystemsBase.current_parameters  —  Function current_parameters(ds::DynamicalSystem) → p Return the current parameter container of  ds . This is mutated in functions that need to evolve  ds  across a parameter range. See also  initial_parameters ,  current_parameter ,  set_parameter! . source"},{"id":124,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_parameter","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_parameter","content":" DynamicalSystemsBase.current_parameter  —  Function current_parameter(ds::DynamicalSystem, index [,p]) Return the specific parameter of  ds  corresponding to  index , which can be anything given to  set_parameter! .  p  defaults to  current_parameters  and is the parameter container to extract the parameter from, which must match layout with its default value. Use  parameter_name  for an accompanying name. source"},{"id":125,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.parameter_name","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.parameter_name","content":" DynamicalSystemsBase.parameter_name  —  Function parameter_name(index)::String Return a name that matches the outcome of  current_parameter  with  index . source"},{"id":126,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_parameters","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_parameters","content":" DynamicalSystemsBase.initial_parameters  —  Function initial_parameters(ds::DynamicalSystem) → p0 Return the initial parameter container of  ds . This is never mutated and is set when initializing  ds . source"},{"id":127,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.isdeterministic","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.isdeterministic","content":" DynamicalSystemsBase.isdeterministic  —  Function isdeterministic(ds::DynamicalSystem) → true/false Return  true  if  ds  is deterministic, i.e., the dynamic rule contains no randomness. This is information deduced from the type of  ds . source"},{"id":128,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.isdiscretetime","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.isdiscretetime","content":" DynamicalSystemsBase.isdiscretetime  —  Function isdiscretetime(ds::DynamicalSystem) → true/false Return  true  if  ds  operates in discrete time, or  false  if it is in continuous time. This is information deduced from the type of  ds . source"},{"id":129,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.dynamic_rule","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.dynamic_rule","content":" DynamicalSystemsBase.dynamic_rule  —  Function dynamic_rule(ds::DynamicalSystem) → f Return the dynamic rule of  ds . This is never mutated and is set when initializing  ds . source"},{"id":130,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_time","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_time","content":" DynamicalSystemsBase.current_time  —  Function current_time(ds::DynamicalSystem) → t Return the current time that  ds  is at. This is mutated when  ds  is evolved. source"},{"id":131,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_time","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_time","content":" DynamicalSystemsBase.initial_time  —  Function initial_time(ds::DynamicalSystem) → t0 Return the initial time defined for  ds . This is never mutated and is set when initializing  ds . source"},{"id":132,"pagetitle":"DynamicalSystemsBase.jl","title":"SciMLBase.isinplace","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#SciMLBase.isinplace-Tuple{DynamicalSystem}","content":" SciMLBase.isinplace  —  Method isinplace(ds::DynamicalSystem) → true/false Return  true  if the dynamic rule of  ds  is in-place, i.e., a function mutating the state in place. If  true , the state is typically  Array , if  false , the state is typically  SVector . A front-end user will most likely not care about this information, but a developer may care. source"},{"id":133,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.successful_step","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.successful_step","content":" DynamicalSystemsBase.successful_step  —  Function successful_step(ds::DynamicalSystem) -> true/false Return  true  if the last  step!  call to  ds  was successful,  false  otherwise. For continuous time systems this uses DifferentialEquations.jl error checking, for discrete time it checks if any variable is  Inf  or  NaN . source"},{"id":134,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.referrenced_sciml_model","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.referrenced_sciml_model","content":" DynamicalSystemsBase.referrenced_sciml_model  —  Function referrenced_sciml_model(ds::DynamicalSystem) Return the ModelingToolkit.jl structurally-simplified model referrenced by  ds . Return  nothing  if there is no referrenced model. source"},{"id":135,"pagetitle":"DynamicalSystemsBase.jl","title":"SciMLBase.reinit!","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#SciMLBase.reinit!-Tuple{DynamicalSystem, AbstractDict}","content":" SciMLBase.reinit!  —  Method reinit!(ds::DynamicalSystem, u = initial_state(ds); kwargs...) → ds Reset the status of  ds , so that it is as if it has be just initialized with initial state  u . Practically every function of the ecosystem that evolves  ds  first calls this function on it. Besides the new state  u , you can also configure the keywords  t0 = initial_time(ds)  and  p = current_parameters(ds) . reinit!(ds::DynamicalSystem, u::AbstractDict; kwargs...) → ds If  u  is a  AbstractDict  (for partially setting specific state variables in  set_state! ), then the alterations are done in the state given by the keyword  reference_state = copy(initial_state(ds)) . reinit!(ds, ::Nothing; kwargs...) This method does nothing and leaves the system as is. This is so that downstream functions that call  reinit!  can still be used without resetting the system but rather continuing from its exact current state. source"},{"id":136,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_state!","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_state!","content":" DynamicalSystemsBase.set_state!  —  Function set_state!(ds::DynamicalSystem, u::AbstractArray{Real}) Set the state of  ds  to  u , which must match dimensionality with that of  ds . Also ensure that the change is notified to whatever integration protocol is used. source set_state!(ds::DynamicalSystem, value::Real, i) → u Set the  i th variable of  ds  to  value . The index  i  can be an integer or a symbolic-like index for systems that reference a ModelingToolkit.jl model. For example: i = :x # or `1` or `only(@variables(x))`\nset_state!(ds, 0.5, i) Warning:  this function should not be used with derivative dynamical systems such as Poincare/stroboscopic/projected dynamical systems. Use the method below to manipulate an array and give that to  set_state! . set_state!(u::AbstractArray, value, index, ds::DynamicalSystem) Modify the given state  u  and leave  ds  untouched. source set_state!(ds::DynamicalSystem, mapping::AbstractDict) Convenience version of  set_state!  that iteratively calls  set_state!(ds, val, i)  for all index-value pairs  (i, val)  in  mapping . This allows you to partially set only some state variables. source"},{"id":137,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_parameter!","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_parameter!","content":" DynamicalSystemsBase.set_parameter!  —  Function set_parameter!(ds::DynamicalSystem, index, value [, p]) Change a parameter of  ds  given the  index  it has in the parameter container and the  value  to set it to. This function works for any type of parameter container (array/dictionary/composite types) provided the  index  is appropriate type. The  index  can be a traditional Julia index (integer for arrays, key for dictionaries, or symbol for composite types). It can also be a symbolic variable or  Symbol  instance. This is valid only for dynamical systems referring a ModelingToolkit.jl model which also has  index  as one of its parameters. The last optional argument  p  defaults to  current_parameters  and is the parameter container whose value is changed at the given index. It must match layout with its default value. source"},{"id":138,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_parameters!","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_parameters!","content":" DynamicalSystemsBase.set_parameters!  —  Function set_parameters!(ds::DynamicalSystem, p = initial_parameters(ds)) Set the parameter values in the  current_parameters (ds)  to match those in  p . This is done as an in-place overwrite by looping over the keys of  p  hence  p  can be an arbitrary container mapping parameter indices to values (such as a  Vector{Real} ,  Vector{Pair} , or  AbstractDict ). The keys of  p  must be valid keys that can be given to  set_parameter! . source"},{"id":139,"pagetitle":"DynamicalSystemsBase.jl","title":"Time evolution","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#Time-evolution","content":" Time evolution"},{"id":140,"pagetitle":"DynamicalSystemsBase.jl","title":"CommonSolve.step!","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#CommonSolve.step!-Tuple{DynamicalSystem, Vararg{Any}}","content":" CommonSolve.step!  —  Method step!(ds::DiscreteTimeDynamicalSystem [, n::Integer]) → ds Evolve the discrete time dynamical system for 1 or  n  steps. step!(ds::ContinuousTimeDynamicalSystem, [, dt::Real [, stop_at_tdt]]) → ds Evolve the continuous time dynamical system for one integration step. Alternatively, if a  dt  is given, then progress the integration until there is a temporal difference  ≥ dt  (so, step  at least  for  dt  time). When  true  is passed to the optional third argument, the integration advances for exactly  dt  time. source"},{"id":141,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.trajectory","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.trajectory","content":" DynamicalSystemsBase.trajectory  —  Function trajectory(ds::DynamicalSystem, T [, u0]; kwargs...) → X, t Evolve  ds  for a total time of  T  and return its trajectory  X , sampled at equal time intervals, and corresponding time vector. Optionally provide a starting state  u0  which is  current_state(ds)  by default. The returned time vector is  t = (t0+Ttr):Δt:(t0+Ttr+T) . If time evolution diverged before  T , the remaining of the trajectory is set to the last valid point. trajectory  is a very simple function provided for convenience. For continuous time systems, it doesn't play well with callbacks, use  DifferentialEquations.solve  if you want a trajectory/timeseries that works with callbacks. Keyword arguments Δt :  Time step of value output. For discrete time systems it must be an integer. Defaults to  0.1  for continuous and  1  for discrete time systems. If you don't have access to unicode, the keyword  Dt  can be used instead. Ttr = 0 : Transient time to evolve the initial state before starting saving states. t0 = initial_time(ds) : Starting time. save_idxs::AbstractVector : Which variables to output in  X . It can be any type of index that can be given to  observe_state . Defaults to  1:dimension(ds)  (all dynamic variables). Note: if you mix integer and symbolic indexing be sure to initialize the array as  Any  so that integers  1, 2, ...  are not converted to symbolic expressions. source"},{"id":142,"pagetitle":"DynamicalSystemsBase.jl","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. source"},{"id":143,"pagetitle":"DynamicalSystemsBase.jl","title":"DeterministicIteratedMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DeterministicIteratedMap","content":" DeterministicIteratedMap"},{"id":144,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see the DynamicalSystems.jl tutorial. source"},{"id":145,"pagetitle":"DynamicalSystemsBase.jl","title":"CoupledODEs","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#CoupledODEs","content":" CoupledODEs"},{"id":146,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.CoupledODEs","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see the DynamicalSystems.jl tutorial. DifferentialEquations.jl interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(; stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false),), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  . The convenience constructors  CoupledODEs(prob::ODEProblem [, diffeq])  and  CoupledODEs(ds::CoupledODEs [, diffeq])  are also available. To integrate with ModelingToolkit.jl, the dynamical system  must  be created via the  ODEProblem  (which itself is created via ModelingToolkit.jl), see the Tutorial for an example. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available to extract the problem. source"},{"id":147,"pagetitle":"DynamicalSystemsBase.jl","title":"StroboscopicMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#StroboscopicMap","content":" StroboscopicMap"},{"id":148,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap . source"},{"id":149,"pagetitle":"DynamicalSystemsBase.jl","title":"PoincareMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#PoincareMap","content":" PoincareMap"},{"id":150,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.PoincareMap","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap) source"},{"id":151,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_crossing_time","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_crossing_time","content":" DynamicalSystemsBase.current_crossing_time  —  Function current_crossing_time(pmap::PoincareMap) → tcross Return the time of the latest crossing of the Poincare section. source"},{"id":152,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.poincaresos","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.poincaresos","content":" DynamicalSystemsBase.poincaresos  —  Function poincaresos(A::AbstractStateSpaceSet, plane; kwargs...) → P::StateSpaceSet Calculate the Poincaré surface of section of the given dataset with the given  plane  by performing linear interpolation betweeen points that sandwich the hyperplane. Argument  plane  and keywords  direction, warning, save_idxs  are the same as in  PoincareMap . source poincaresos(ds::CoupledODEs, plane, T = 1000.0; kwargs...) → P::StateSpaceSet Return the iterations of  ds  on the Poincaré surface of section with the  plane , by evolving  ds  up to a total of  T . Return a  StateSpaceSet  of the points that are on the surface of section. This function initializes a  PoincareMap  and steps it until its  current_crossing_time  exceeds  T . You can also use  trajectory  with  PoincareMap  to get a sequence of  N::Int  points instead. The keywords  Ttr, save_idxs  act as in  trajectory . See  PoincareMap  for  plane  and all other keywords. source"},{"id":153,"pagetitle":"DynamicalSystemsBase.jl","title":"TangentDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#TangentDynamicalSystem","content":" TangentDynamicalSystem"},{"id":154,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.CoreDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.CoreDynamicalSystem","content":" DynamicalSystemsBase.CoreDynamicalSystem  —  Type CoreDynamicalSystem Union type meaning either  DeterministicIteratedMap  or  CoupledODEs , which are the core systems whose dynamic rule  f  is known analytically. This type is used for deciding whether a creation of a  TangentDynamicalSystem  is possible or not. source"},{"id":155,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.TangentDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.TangentDynamicalSystem","content":" DynamicalSystemsBase.TangentDynamicalSystem  —  Type TangentDynamicalSystem <: DynamicalSystem\nTangentDynamicalSystem(ds::CoreDynamicalSystem; kwargs...) A dynamical system that bundles the evolution of  ds  (which must be an  CoreDynamicalSystem ) and  k  deviation vectors that are evolved according to the  dynamics in the tangent space  (also called linearized dynamics or the tangent dynamics). The state of  ds must  be an  AbstractVector  for  TangentDynamicalSystem . TangentDynamicalSystem  follows the  DynamicalSystem  interface with the following adjustments: reinit!  takes an additional keyword  Q0  (with same default as below) The additional functions  current_deviations  and  set_deviations!  are provided for the deviation vectors. Keyword arguments k  or  Q0 :  Q0  represents the initial deviation vectors (each column = 1 vector). If  k::Int  is given, a matrix  Q0  is created with the first  k  columns of the identity matrix. Otherwise  Q0  can be given directly as a matrix. It must hold that  size(Q, 1) == dimension(ds) . You can use  orthonormal  for random orthonormal vectors. By default  k = dimension(ds)  is used. u0 = current_state(ds) : Starting state. J  and  J0 : See section \"Jacobian\" below. Description Let  $u$  be the state of  ds , and  $y$  a deviation (or perturbation) vector. These two are evolved in parallel according to \\[\\begin{array}{rcl}\n\\frac{d\\vec{x}}{dt} &=& f(\\vec{x}) \\\\\n\\frac{dY}{dt} &=& J_f(\\vec{x}) \\cdot Y\n\\end{array}\n\\quad \\mathrm{or}\\quad\n\\begin{array}{rcl}\n\\vec{x}_{n+1} &=& f(\\vec{x}_n) \\\\\nY_{n+1} &=& J_f(\\vec{x}_n) \\cdot Y_n.\n\\end{array}\\] for continuous or discrete time respectively. Here  $f$  is the  dynamic_rule (ds)  and  $J_f$  is the Jacobian of  $f$ . Jacobian The keyword  J  provides the Jacobian function. It must be a Julia function in the same form as  f , the  dynamic_rule . Specifically,  J(u, p, n) -> M::SMatrix  for the out-of-place version or  J(M, u, p, n)  for the in-place version acting in-place on  M . In both cases  M  is the Jacobian matrix used for the evolution of the deviation vectors. By default  J = nothing .  In this case  J  is constructed automatically using the module  ForwardDiff , hence its limitations also apply here. Even though  ForwardDiff  is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so it is recommended. Additionally, automatic and in-place Jacobians cannot be time dependent. The keyword  J0  allows you to pass an initialized Jacobian matrix  J0 . This is useful for large in-place systems where only a few components of the Jacobian change during the time evolution.  J0  can be a sparse or any other matrix type. If not given, a matrix of zeros is used.  J0  is ignored for out of place systems. source"},{"id":156,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_deviations","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_deviations","content":" DynamicalSystemsBase.current_deviations  —  Function current_deviations(tands::TangentDynamicalSystem) Return the deviation vectors of  tands  as a matrix with each column a vector. source"},{"id":157,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_deviations!","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_deviations!","content":" DynamicalSystemsBase.set_deviations!  —  Function set_deviations!(tands::TangentDynamicalSystem, Q) Set the deviation vectors of  tands  to be  Q , a matrix with each column a vector. source"},{"id":158,"pagetitle":"DynamicalSystemsBase.jl","title":"StateSpaceSets.orthonormal","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#StateSpaceSets.orthonormal","content":" StateSpaceSets.orthonormal  —  Function orthonormal([T,] D, k) -> ws Return a matrix  ws  with  k  columns, each being an  D -dimensional orthonormal vector. T  is the return type and can be either  SMatrix  or  Matrix . If not given, it is  SMatrix  if  D*k < 100 , otherwise  Matrix . source"},{"id":159,"pagetitle":"DynamicalSystemsBase.jl","title":"ProjectedDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#ProjectedDynamicalSystem","content":" ProjectedDynamicalSystem"},{"id":160,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\nprods = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(prods, [0.2, 0.4])\nstep!(prods)\ncurrent_state(prods) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\nprods = # same as in above example... source"},{"id":161,"pagetitle":"DynamicalSystemsBase.jl","title":"ParallelDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#ParallelDynamicalSystem","content":" ParallelDynamicalSystem"},{"id":162,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.ParallelDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.ParallelDynamicalSystem","content":" DynamicalSystemsBase.ParallelDynamicalSystem  —  Type ParallelDynamicalSystem <: DynamicalSystem\nParallelDynamicalSystem(ds::DynamicalSystem, states::Vector{<:AbstractArray}) A struct that evolves several  states  of a given dynamical system in parallel  at exactly the same times . Useful when wanting to evolve several different trajectories of the same system while ensuring that they share parameters and time vector. This struct follows the  DynamicalSystem  interface with the following adjustments: The function  current_state  is called as  current_state(pds, i::Int = 1)  which returns the  i th state. Same for  initial_state . Similarly,  set_state!  obtains a third argument  i::Int = 1  to set the  i -th state. current_states  and  initial_states  can be used to get all parallel states. reinit!  takes in a vector of states (like  states ) for  u . ParallelDynamicalSystem(ds::DynamicalSystem, states::Vector{<:Dict}) For a dynamical system referring a MTK model, one can specify states as a vector of dictionaries to alter the current state of  ds  as in  set_state! . source"},{"id":163,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_states","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_states","content":" DynamicalSystemsBase.initial_states  —  Function initial_states(pds::ParallelDynamicalSystem) Return an iterator over the initial parallel states of  pds . source"},{"id":164,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_states","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_states","content":" DynamicalSystemsBase.current_states  —  Function current_states(pds::ParallelDynamicalSystem) Return an iterator over the parallel states of  pds . source"},{"id":165,"pagetitle":"DynamicalSystemsBase.jl","title":"ArbitrarySteppable","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#ArbitrarySteppable","content":" ArbitrarySteppable"},{"id":166,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.ArbitrarySteppable","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#DynamicalSystemsBase.ArbitrarySteppable","content":" DynamicalSystemsBase.ArbitrarySteppable  —  Type ArbitrarySteppable <: DiscreteTimeDynamicalSystem\nArbitrarySteppable(\n    model, step!, extract_state, extract_parameters, reset_model!;\n    isdeterministic = true, set_state = reinit!,\n) A dynamical system generated by an arbitrary \"model\" that can be stepped  in-place  with some function  step!(model)  for 1 step. The state of the model is extracted by the  extract_state(model) -> u  function The parameters of the model are extracted by the  extract_parameters(model) -> p  function. The system may be re-initialized, via  reinit! , with the  reset_model!  user-provided function that must have the call signature reset_model!(model, u, p) given a (potentially new) state  u  and parameter container  p , both of which will default to the initial ones in the  reinit!  call. ArbitrarySteppable  exists to provide the DynamicalSystems.jl interface to models from other packages that could be used within the DynamicalSystems.jl library.  ArbitrarySteppable  follows the  DynamicalSystem  interface with the following adjustments: initial_time  is always 0, as time counts the steps the model has taken since creation or last  reinit!  call. set_state!  is the same as  reinit!  by default. If not, the keyword argument  set_state  is a function  set_state(model, u)  that sets the state of the model to  u . The keyword  isdeterministic  should be set properly, as it decides whether downstream algorithms should error or not. source"},{"id":167,"pagetitle":"DynamicalSystemsBase.jl","title":"Parallelization","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#Parallelization","content":" Parallelization Since  DynamicalSystem s are mutable, one needs to copy them before parallelizing, to avoid having to deal with complicated race conditions etc. The simplest way is with  deepcopy . Here is an example block that shows how to parallelize calling some expensive function (e.g., calculating the Lyapunov exponent) over a parameter range using  Threads : ds = DynamicalSystem(f, u, p) # some concrete implementation\nparameters = 0:0.01:1\noutputs = zeros(length(parameters))\n\n# Since `DynamicalSystem`s are mutable, we need to copy to parallelize\nsystems = [deepcopy(ds) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, ds) # we can save 1 copy\n\nThreads.@threads for i in eachindex(parameters)\n    system = systems[Threads.threadid()]\n    set_parameter!(system, 1, parameters[i])\n    outputs[i] = expensive_function(system, args...)\nend"},{"id":168,"pagetitle":"DynamicalSystemsBase.jl","title":"Advanced example","ref":"/DynamicalSystemsDocs.jl/dynamicalsystemsbase/stable/#Advanced-example","content":" Advanced example This is an advanced example of making an in-place implementation of coupled  standard maps . It will utilize a handcoded Jacobian, a sparse matrix for the Jacobinan, a default initial Jacobian matrix, as well as function-like-objects as the dynamic rule. Coupled standard maps is a deterministic iterated map that can have arbitrary number of equations of motion, since you can couple  N  standard maps which are 2D maps, like so: \\[\\theta_{i}' = \\theta_i + p_{i}' \\\\\np_{i}' = p_i + k_i\\sin(\\theta_i) - \\Gamma \\left[\\sin(\\theta_{i+1} - \\theta_{i}) + \\sin(\\theta_{i-1} - \\theta_{i}) \\right]\\] To model this, we will make a dedicated  struct , which is parameterized on the number of coupled maps: using DynamicalSystemsBase\n\nstruct CoupledStandardMaps{N}\n    idxs::SVector{N, Int}\n    idxsm1::SVector{N, Int}\n    idxsp1::SVector{N, Int}\nend (what these fields are will become apparent later) We initialize the struct with the amount of standard maps we want to couple, and we also define appropriate parameters: M = 5  # couple number\nu0 = 0.001rand(2M) #initial state\nks = 0.9ones(M) # nonlinearity parameters\nΓ = 1.0 # coupling strength\np = (ks, Γ) # parameter container\n\n# Create struct:\nSV = SVector{M, Int}\nidxs = SV(1:M...) # indexes of thetas\nidxsm1 = SV(circshift(idxs, +1)...)  #indexes of thetas - 1\nidxsp1 = SV(circshift(idxs, -1)...)  #indexes of thetas + 1\n# So that:\n# x[i] ≡ θᵢ\n# x[[idxsp1[i]]] ≡ θᵢ+₁\n# x[[idxsm1[i]]] ≡ θᵢ-₁\ncsm = CoupledStandardMaps{M}(idxs, idxsm1, idxsp1) Main.CoupledStandardMaps{5}([1, 2, 3, 4, 5], [5, 1, 2, 3, 4], [2, 3, 4, 5, 1]) We will now use this struct to define a  function-like-object , a Type that also acts as a function function (f::CoupledStandardMaps{N})(xnew::AbstractVector, x, p, n) where {N}\n    ks, Γ = p\n    @inbounds for i in f.idxs\n\n        xnew[i+N] = mod2pi(\n            x[i+N] + ks[i]*sin(x[i]) -\n            Γ*(sin(x[f.idxsp1[i]] - x[i]) + sin(x[f.idxsm1[i]] - x[i]))\n        )\n\n        xnew[i] = mod2pi(x[i] + xnew[i+N])\n    end\n    return nothing\nend We will use  the same struct  to create a function for the Jacobian: function (f::CoupledStandardMaps{M})(\n    J::AbstractMatrix, x, p, n) where {M}\n\n    ks, Γ = p\n    # x[i] ≡ θᵢ\n    # x[[idxsp1[i]]] ≡ θᵢ+₁\n    # x[[idxsm1[i]]] ≡ θᵢ-₁\n    @inbounds for i in f.idxs\n        cosθ = cos(x[i])\n        cosθp= cos(x[f.idxsp1[i]] - x[i])\n        cosθm= cos(x[f.idxsm1[i]] - x[i])\n        J[i+M, i] = ks[i]*cosθ + Γ*(cosθp + cosθm)\n        J[i+M, f.idxsm1[i]] = - Γ*cosθm\n        J[i+M, f.idxsp1[i]] = - Γ*cosθp\n        J[i, i] = 1 + J[i+M, i]\n        J[i, f.idxsm1[i]] = J[i+M, f.idxsm1[i]]\n        J[i, f.idxsp1[i]] = J[i+M, f.idxsp1[i]]\n    end\n    return nothing\nend This is possible because the system state is a  Vector  while the Jacobian is a  Matrix , so multiple dispatch can differentiate between the two. Notice in addition, that the Jacobian function accesses  only half the elements of the matrix . This is intentional, and takes advantage of the fact that the other half is constant. We can leverage this further, by making the Jacobian a sparse matrix. Because the  DynamicalSystem  constructors allow us to give in a pre-initialized Jacobian matrix, we take advantage of that and create: using SparseArrays\nJ = zeros(eltype(u0), 2M, 2M)\n# Set ∂/∂p entries (they are eye(M,M))\n# And they dont change they are constants\nfor i in idxs\n    J[i, i+M] = 1\n    J[i+M, i+M] = 1\nend\nsparseJ = sparse(J)\n\ncsm(sparseJ, u0, p, 0) # apply Jacobian to initial state\nsparseJ 10×10 SparseArrays.SparseMatrixCSC{Float64, Int64} with 40 stored entries:\n  3.9  -1.0    ⋅     ⋅   -1.0  1.0   ⋅    ⋅    ⋅    ⋅ \n -1.0   3.9  -1.0    ⋅     ⋅    ⋅   1.0   ⋅    ⋅    ⋅ \n   ⋅   -1.0   3.9  -1.0    ⋅    ⋅    ⋅   1.0   ⋅    ⋅ \n   ⋅     ⋅   -1.0   3.9  -1.0   ⋅    ⋅    ⋅   1.0   ⋅ \n -1.0    ⋅     ⋅   -1.0   3.9   ⋅    ⋅    ⋅    ⋅   1.0\n  2.9  -1.0    ⋅     ⋅   -1.0  1.0   ⋅    ⋅    ⋅    ⋅ \n -1.0   2.9  -1.0    ⋅     ⋅    ⋅   1.0   ⋅    ⋅    ⋅ \n   ⋅   -1.0   2.9  -1.0    ⋅    ⋅    ⋅   1.0   ⋅    ⋅ \n   ⋅     ⋅   -1.0   2.9  -1.0   ⋅    ⋅    ⋅   1.0   ⋅ \n -1.0    ⋅     ⋅   -1.0   2.9   ⋅    ⋅    ⋅    ⋅   1.0 Now we are ready to create our dynamical system ds = DeterministicIteratedMap(csm, u0, p) 10-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      true\n dynamic rule:  CoupledStandardMaps\n parameters:    ([0.9, 0.9, 0.9, 0.9, 0.9], 1.0)\n time:          0\n state:         [0.00011273589303322529, 0.0007514049681577206, 0.0008177018097536296, 0.0005789873888723654, 0.0008625776625181157, 0.0009670392829645936, 0.00015679041799372917, 0.0009036962972810385, 0.0004955518300182536, 0.0002986173365405468]\n Of course, the reason we went through all this trouble was to make a  TangentDynamicalSystem , that can actually use the Jacobian function. tands = TangentDynamicalSystem(ds; J = csm, J0 = sparseJ, k = M) 10-dimensional TangentDynamicalSystem\n deterministic:     true\n discrete time:     true\n in-place:          true\n dynamic rule:      CoupledStandardMaps\n jacobian:          CoupledStandardMaps\n deviation vectors: 5\n parameters:        ([0.9, 0.9, 0.9, 0.9, 0.9], 1.0)\n time:              0\n state:             [0.00011273589303322529, 0.0007514049681577206, 0.0008177018097536296, 0.0005789873888723654, 0.0008625776625181157, 0.0009670392829645936, 0.00015679041799372917, 0.0009036962972810385, 0.0004955518300182536, 0.0002986173365405468]\n step!(tands, 5)\ncurrent_deviations(tands) 10×5 view(::Matrix{Float64}, :, 2:6) with eltype Float64:\n  3504.87   -2497.36     778.48     701.579  -2419.27\n -2654.38    3815.79   -2721.51     847.861    781.697\n   847.188  -2738.47    3851.11   -2690.72     800.244\n   737.202    799.768  -2578.3     3625.42   -2515.06\n -2369.01     699.892    733.387  -2403.18    3406.64\n  2850.52   -2072.59     666.987    590.373  -1994.89\n -2229.04    3160.12   -2295.83     736.108    670.2\n   735.44   -2312.73    3195.27   -2265.19     688.663\n   625.806    688.18   -2153.1     2970.36   -2090.12\n -1944.78     588.697    622.007  -1978.7     2752.62 (the deviation vectors will increase in magnitude rapidly because the dynamical system is chaotic) DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":171,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.jl","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.jl","content":" PredefinedDynamicalSystems.jl"},{"id":172,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems","content":" PredefinedDynamicalSystems  —  Module PredefinedDynamicalSystems.jl Module which contains pre-defined dynamical systems that can be used by the  DynamicalSystems.jl  library. To install it, run  import Pkg; Pkg.add(\"PredefinedDynamicalSystems\") . Predefined systems exist as functions that return a  DynamicalSystem  instance. They are accessed like: ds = PredefinedDynamicalSystems.lorenz(u0; ρ = 32.0) The alias  Systems  is also exported as a deprecation. This module is provided purely as a convenience. It does not have any actual tests, and it is not guaranteed to be stable in future versions. It is not recommended to use this module for anything else besides on-the-spot demonstrative examples. For some systems, a Jacobian function is also defined. The naming convention for the Jacobian function is  \\$(name)_jacob . So, for the above example we have  J = Systems.lorenz_jacob . All available systems are provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source All currently implemented predefined systems are listed below:"},{"id":173,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.antidots","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.antidots","content":" PredefinedDynamicalSystems.antidots  —  Function antidots([u]; B = 1.0, d0 = 0.3, c = 0.2) An antidot \"superlattice\" is a Hamiltonian system that corresponds to a smoothened periodic Sinai billiard with disk diameter  d0  and smooth factor  c [Datseris2019] . This version is the two dimensional classical form of the system, with quadratic dynamical rule and a perpendicular magnetic field. Notice that the dynamical rule is with respect to the velocity instead of momentum, i.e.: \\[\\begin{aligned}\n\\dot{x} &= v_x \\\\\n\\dot{y} &= v_y \\\\\n\\dot{v_x} &= B v_y - U_x \\\\\n\\dot{v_y} &= -B v_x - U_y \\\\\n\\end{aligned}\\] with  $U$  the potential energy: \\[U = \\left(\\tfrac{1}{c^4}\\right) \\left[\\tfrac{d_0}{2} + c - r_a\\right]^4\\] if  $r_a = \\sqrt{(x \\mod 1)^2 + (y \\mod 1)^2} < \\frac{d_0}{2} + c$  and 0 otherwise. That is, the potential is periodic with period 1 in both  $x, y$  and normalized such that for energy value of 1 it is a circle of diameter  $d_0$ . The magnetic field is also normalized such that for value  B = 1  the cyclotron diameter is 1. source"},{"id":174,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.arnoldcat","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.arnoldcat","content":" PredefinedDynamicalSystems.arnoldcat  —  Function arnoldcat(u0 = [0.001245, 0.00875]) \\[f(x,y) = (2x+y,x+y) \\mod 1\\] Arnold's cat map. A chaotic map from the torus into itself, used by Vladimir Arnold in the 1960s. [1] [1] : Arnol'd, V. I., & Avez, A. (1968). Ergodic problems of classical mechanics. source"},{"id":175,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.betatransformationmap","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.betatransformationmap","content":" PredefinedDynamicalSystems.betatransformationmap  —  Function betatransformationmap(u0 = 0.25; β=2.0)-> ds The beta transformation, also called the generalized Bernoulli map, or the βx map, is described by \\[\\begin{aligned}\nx_{n+1} = \\beta x (\\mod 1).\n\\end{aligned}\\] The parameter β controls the dynamics of the map. Its Lyapunov exponent can be analytically shown to be λ = ln(β)  [Ott2002] . At β=2, it becomes the dyadic transformation, also known as the bit shift map, the 2x mod 1 map, the Bernoulli map or the sawtooth map. The typical trajectory for this case is chaotic, though there are countably infinite periodic orbits  [Ott2002] . source"},{"id":176,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.chua","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.chua","content":" PredefinedDynamicalSystems.chua  —  Function chua(u0 = [0.7, 0.0, 0.0]; a = 15.6, b = 25.58, m0 = -8/7, m1 = -5/7) \\[\\begin{aligned}\n\\dot{x} &= a [y - h(x)]\\\\\n\\dot{y} &= x - y+z \\\\\n\\dot{z} &= b y\n\\end{aligned}\\] where  $h(x)$  is defined by \\[h(x) = m_1 x + \\frac 1 2 (m_0 - m_1)(|x + 1| - |x - 1|)\\] This is a 3D continuous system that exhibits chaos. Chua designed an electronic circuit with the expressed goal of exhibiting chaotic motion, and this system is obtained by rescaling the circuit units to simplify the form of the equation.  [Chua1992] The parameters are  $a$ ,  $b$ ,  $m_0$ , and  $m_1$ . Setting  $a = 15.6$ ,  $m_0 = -8/7$  and  $m_1 = -5/7$ , and varying the parameter  $b$  from  $b = 25$  to  $b = 51$ , one observes a classic period-doubling bifurcation route to chaos.  [Chua2007] The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":177,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.coupled_roessler","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.coupled_roessler","content":" PredefinedDynamicalSystems.coupled_roessler  —  Function coupled_roessler(u0=[1, -2, 0, 0.11, 0.2, 0.1];\nω1 = 0.18, ω2 = 0.22, a = 0.2, b = 0.2, c = 5.7, k1 = 0.115, k2 = 0.0) Two coupled Rössler oscillators, used frequently in the study of chaotic synchronization. The parameter container has the parameters in the same order as stated in this function's documentation string. The equations are: \\[\\begin{aligned}\n\\dot{x_1} &= -\\omega_1 y_1-z_1 \\\\\n\\dot{y_1} &= \\omega_1 x+ay_1 + k_1(y_2 - y_1) \\\\\n\\dot{z_1} &= b + z_1(x_1-c) \\\\\n\\dot{x_2} &= -\\omega_2 y_2-z_2 \\\\\n\\dot{y_2} &= \\omega_2 x+ay_2 + k_2(y_1 - y_2) \\\\\n\\dot{z_2} &= b + z_2(x_2-c) \\\\\n\\end{aligned}\\] source"},{"id":178,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.coupledstandardmaps","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.coupledstandardmaps","content":" PredefinedDynamicalSystems.coupledstandardmaps  —  Function coupledstandardmaps(M::Int, u0 = 0.001rand(2M); ks = ones(M), Γ = 1.0) \\[\\begin{aligned}\n\\theta_{i}' &= \\theta_i + p_{i}' \\\\\np_{i}' &= p_i + k_i\\sin(\\theta_i) - \\Gamma \\left[\n\\sin(\\theta_{i+1} - \\theta_{i}) + \\sin(\\theta_{i-1} - \\theta_{i})\n\\right]\n\\end{aligned}\\] A discrete system of  M  nonlinearly coupled standard maps, first introduced in [1] to study diffusion and chaos thresholds. The  total  dimension of the system is  2M . The maps are coupled through  Γ  and the  i -th map has a nonlinear parameter  ks[i] . The first  M  parameters are the  ks , the  M+1 th parameter is  Γ . The first  M  entries of the state are the angles, the last  M  are the momenta. [1] : H. Kantz & P. Grassberger, J. Phys. A  21 , pp 127–133 (1988) source"},{"id":179,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.double_pendulum","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.double_pendulum","content":" PredefinedDynamicalSystems.double_pendulum  —  Function double_pendulum(u0 = [π/2, 0, 0, 0.5];\n                G=10.0, L1 = 1.0, L2 = 1.0, M1 = 1.0, M2 = 1.0) Famous chaotic double pendulum system (also used for our logo!). Keywords are gravity ( G ), lengths of each rod ( L1  and  L2 ) and mass of each ball ( M1  and  M2 ). Everything is assumed in SI units. The variables order is  $[θ₁, ω₁, θ₂, ω₂]$  and they satisfy: \\[\\begin{aligned}\nθ̇₁ &= ω₁ \\\\\nω̇₁ &= [M₂ L₁ ω₁² \\sin φ \\cos φ + M₂ G \\sin θ₂ \\cos φ +\n       M₂ L₂ ω₂² \\sin φ - (M₁ + M₂) G \\sin θ₁] / (L₁ Δ) \\\\\nθ̇₂ &= ω₂ \\\\\nω̇₂ &= [-M₂ L₂ ω₂² \\sin φ \\cos φ + (M₁ + M₂) G \\sin θ₁ \\cos φ -\n         (M₁ + M₂) L₁ ω₁² \\sin φ - (M₁ + M₂) G \\sin Θ₂] / (L₂ Δ)\n\\end{aligned}\\] where  $φ = θ₂-θ₁$  and  $Δ = (M₁ + M₂) - M₂ \\cos² φ$ . Jacobian is created automatically (thus methods that use the Jacobian will be slower)! (please contribute the Jacobian in LaTeX :smile:) The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":180,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.duffing","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.duffing","content":" PredefinedDynamicalSystems.duffing  —  Function duffing(u0 = [0.1, 0.25]; ω = 2.2, f = 27.0, d = 0.2, β = 1) The (forced) duffing oscillator, that satisfies the equation \\[\\ddot{x} + d \\dot{x} + β x + x^3 = f \\cos(\\omega t)\\] with  f, ω  the forcing strength and frequency and  d  the damping. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":181,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.fitzhugh_nagumo","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.fitzhugh_nagumo","content":" PredefinedDynamicalSystems.fitzhugh_nagumo  —  Function fitzhugh_nagumo(u = 0.5ones(2); a=3.0, b=0.2, ε=0.01, I=0.0) Famous excitable system which emulates the firing of a neuron, with rule \\[\\begin{aligned}\n\\dot{v} &= av(v-b)(1-v) - w + I \\\\\n\\dot{w} &= \\varepsilon(v - w)\n\\end{aligned}\\] More details in the  Scholarpedia  entry. source"},{"id":182,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.forced_pendulum","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.forced_pendulum","content":" PredefinedDynamicalSystems.forced_pendulum  —  Function forced_pendulum(u0 = [0.1, 0.25]; ω = 2.2, f = 27.0, d = 0.2) The standard forced damped pendulum with a sine response force. duffing oscillator, that satisfies the equation \\[\\ddot{x} + d \\dot{x} + \\sin(x) = f \\cos(\\omega t)\\] with  f, ω  the forcing strength and frequency and  d  the damping. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":183,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.gissinger","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.gissinger","content":" PredefinedDynamicalSystems.gissinger  —  Function gissinger(u0 = [3, 0.5, 1.5]; μ = 0.119, ν = 0.1, Γ = 0.9) \\[\\begin{aligned}\n\\dot{Q} &= \\mu Q - VD \\\\\n\\dot{D} &= -\\nu D + VQ \\\\\n\\dot{V} &= \\Gamma -V + QD\n\\end{aligned}\\] A continuous system that models chaotic reversals due to Gissinger  [Gissinger2012] , applied to study the reversals of the magnetic field of the Earth. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":184,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.grebogi_map","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.grebogi_map","content":" PredefinedDynamicalSystems.grebogi_map  —  Function grebogi_map(u0 = [0.2, 0.]; a = 1.32, b=0.9, J₀=0.3) \\[\\begin{aligned}\n\\theta_{n+1} &= \\theta_n +   a\\sin 2 \\theta_n -b \\sin 4 \\theta_n -x_n\\sin \\theta_n\\\\\nx_{n+1} &= -J_0 \\cos \\theta_n\n\\end{aligned}\\] This map has two fixed point at  (0,-J_0)  and  (π,J_0)  which are attracting for  |1+2a-4b|<1 . There is a chaotic transient dynamics before the dynamical systems settles at a fixed point. This map illustrate the fractalization of the basins boundary and its uncertainty exponent  α  is roughly 0.2. source"},{"id":185,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.guckenheimer_holmes","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.guckenheimer_holmes","content":" PredefinedDynamicalSystems.guckenheimer_holmes  —  Function guckenheimer_holmes(u0=[-0.55582369,0.05181624,0.37766104];\n    a = 0.4,\n    b = 20.25,\n    c = 3,\n    d = 1.6,\n    e = 1.7,\n    f = 0.44) \\[\\begin{aligned}\n\\dot{x} &= ax - by + czx + dz(x^2 + y^2)\\\\\n\\dot{y} &= ay + bx + czy\\\\\n\\dot{z} &= e - z^2 - f(x^2 + y^2) - az^3\n\\end{aligned}\\] A nonlinear oscillator  [GuckenheimerHolmes1983] . source"},{"id":186,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.halvorsen","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.halvorsen","content":" PredefinedDynamicalSystems.halvorsen  —  Function halvorsen(u0=[-8.6807408,-2.4741399,0.070775762]; a = 1.4, b = 4.0) \\[\\begin{aligned}\n\\dot{x} &= -a*x - b*(y + z) - y^2\\\\\n\\dot{y} &= -a*y - b*(z + x) - z^2\\\\\n\\dot{z} &= -a*z - b*(x + y) - x^2\n\\end{aligned}\\] An algebraically-simple chaotic system with quadratic nonlinearity  [Sprott2010] . source"},{"id":187,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.henon","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.henon","content":" PredefinedDynamicalSystems.henon  —  Function henon(u0=zeros(2); a = 1.4, b = 0.3) \\[\\begin{aligned}\nx_{n+1} &= 1 - ax^2_n+y_n \\\\\ny_{n+1} & = bx_n\n\\end{aligned}\\] The Hénon map is a two-dimensional mapping due to Hénon [1] that can display a strange attractor (at the default parameters). In addition, it also displays many other aspects of chaos, like period doubling or intermittency, for other parameters. According to the author, it is a system displaying all the properties of the Lorentz system (1963) while being as simple as possible. Default values are the ones used in the original paper. The parameter container has the parameters in the same order as stated in this function's documentation string. [1] : M. Hénon, Commun.Math. Phys.  50 , pp 69 (1976) source"},{"id":188,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.henonheiles","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.henonheiles","content":" PredefinedDynamicalSystems.henonheiles  —  Function henonheiles(u0=[0, -0.25, 0.42081,0]) \\[\\begin{aligned}\n\\dot{x} &= p_x \\\\\n\\dot{y} &= p_y \\\\\n\\dot{p}_x &= -x -2 xy \\\\\n\\dot{p}_y &= -y - (x^2 - y^2)\n\\end{aligned}\\] The Hénon–Heiles system  [HénonHeiles1964]  is a conservative dynamical system and was introduced as a simplification of the motion of a star around a galactic center. It was originally intended to study the existence of a \"third integral of motion\" (which would make this 4D system integrable). In that search, the authors encountered chaos, as the third integral existed for only but a few initial conditions. The default initial condition is a typical chaotic orbit. The function  Systems.henonheiles_ics(E, n)  generates a grid of  n×n  initial conditions, all having the same energy  E . source"},{"id":189,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hindmarshrose","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hindmarshrose","content":" PredefinedDynamicalSystems.hindmarshrose  —  Function hindmarshrose(u0=[-1.0, 0.0, 0.0]; a=1, b=3, c=1, d=5, r=0.001, s=4, xr=-8/5, I=2.0) -> ds \\[\\begin{aligned}\n\\dot{x} &= y - ax^3 + bx^2 +I - z, \\\\\n\\dot{y} &= c - dx^2 -y, \\\\\n\\dot{z} &= r(s(x - x_r) - z)\n\\end{aligned}\\] The Hindmarsh-Rose model reproduces the bursting behavior of a neuron's membrane potential, characterized by a fast sequence of spikes followed by a quiescent period. The  x  variable describes the membane potential, whose behavior can be controlled by the applied current  I ; the  y  variable describes the sodium and potassium ionic currents, and  z  describes an adaptation current  [HindmarshRose1984] . The default parameter values are taken from  [HindmarshRose1984] , chosen to lead to periodic bursting. source"},{"id":190,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hindmarshrose_two_coupled","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hindmarshrose_two_coupled","content":" PredefinedDynamicalSystems.hindmarshrose_two_coupled  —  Function hindmarshrose_two_coupled(u0=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6];\na = 1.0, b = 3.0, d = 5.0, r = 0.001, s = 4.0, xr = -1.6, I = 4.0,\nk1 = -0.17, k2 = -0.17, k_el = 0.0, xv = 2.0) \\[\\begin{aligned}\n\\dot x_{i} = y_{i} + bx^{2}_{i} - ax^{3}_{i} - z_{i} + I - k_{i}(x_{i} - v_{s})\\Gamma(x_{j}) + k(x_{j} - x_{i})\\\\\n\\dot y_{i} = c - d x^{2}_{i} - y_{i}\\\\\n\\dot z_{i} = r[s(x_{i} - x_{R}) - z_{i}]\\\\\n\\i,j=1,2 (i\\neq j).\\\\\n\\end{aligned}\\] The two coupled Hindmarsh Rose element by chemical and electrical synapse. it is modelling the dynamics of a neuron's membrane potential. The default parameter values are taken from article \"Dragon-king-like extreme events in coupled bursting neurons\", DOI:https://doi.org/10.1103/PhysRevE.97.062311. source"},{"id":191,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hodgkinhuxley","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hodgkinhuxley","content":" PredefinedDynamicalSystems.hodgkinhuxley  —  Function hodgkinhuxley(u0=[-60.0, 0.0, 0.0, 0.0]; I = 12.0, Vna = 50.0, Vk = -77.0, Vl = -54.4, gna = 120.0,gk = 36.0, gl = 0.3) -> ds \\[\\begin{aligned}\nC_m \\frac{dV_m}{dt} &= -\\overline{g}_\\mathrm{K} n^4 (V_m - V_\\mathrm{K}) - \\overline{g}_\\mathrm{Na} m^3 h(V_m - V_\\mathrm{Na}) - \\overline{g}_l (V_m - Vl) + I\\\\\n\\dot{n} &= \\alpha_n(V_m)(1-n) - \\beta_n(V_m)n \\\\\n\\dot{m} &= \\alpha_m(V_m)(1-m) - \\beta_m(V_m)m \\\\\n\\dot{h} &= \\alpha_h(V_m)(1-h) - \\beta_h(V_m)h \\\\\n\\alpha_n(V_m) = \\frac{0.01(V+55)}{1 - \\exp(\\frac{1V+55}{10})} \\quad\n\\alpha_m(V_m) = \\frac{0.1(V+40)}{1 - \\exp(\\frac{V+40}{10})} \\quad\n\\alpha_h(V_m) = 0.07 \\exp(-\\frac{(V+65)}{20}) \\\\\n\\beta_n(V_m) = 0.125 \\exp(-\\frac{V+65}{80}) \\quad\n\\beta_m(V_m) = 4 \\exp(-\\frac{V+65}{18}) \\quad\n\\beta_h(V_m) = \\frac{1}{1 + \\exp(-\\frac{V+35}{10})}\n\\end{aligned}\\] The Nobel-winning four-dimensional dynamical system due to Hodgkin and Huxley  [HodgkinHuxley1952] , which describes the electrical spiking activity (action potentials) in neurons. A complete description of all parameters and variables is given in  [HodgkinHuxley1952] ,  [Ermentrout2010] , and  [Abbott2005] . The equations and default parameters used here are taken from  [Ermentrout2010] [Abbott2005] . They differ slightly from the original paper  [HodgkinHuxley1952] , since they were changed to shift the resting potential to -65 mV, instead of the 0mV in the original paper. Varying the injected current I from  I = -5   to   I = 12  takes the neuron from quiescent to a single spike, and to a tonic (repetitive) spiking. This is due to a subcritical Hopf bifurcation, which occurs close to  I = 9.5 . source"},{"id":192,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_bao","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_bao","content":" PredefinedDynamicalSystems.hyper_bao  —  Function function hyper_bao(u0 = [5.0, 8.0, 12.0, 21.0];\n    a = 36.0,\n    b = 3.0,\n    c = 20.5,\n    d = 0.1,\n    k = 21.0) \\[\\begin{aligned}\n\\dot{x} &= a (y - x) + w\\\\\n\\dot{y} &= c y - x z\\\\\n\\dot{z} &= x y - b z\\\\\n\\dot{w} &= k x - d y z\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Lu system[^Bo-Cheng2008]. [^Bo-Cheng2008]:     Bo-Cheng, B., & Zhong, L. (2008).     A hyperchaotic attractor coined from chaotic Lü system.     Chinese Physics Letters, 25(7), 2396. source"},{"id":193,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_cai","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_cai","content":" PredefinedDynamicalSystems.hyper_cai  —  Function function hyper_cai(u0 = [1.0, 1.0, 20.0, 10.0];\n    a = 27.5,\n    b = 3.0,\n    c = 19.3,\n    d = 2.9,\n    e = 3.3) \\[\\begin{aligned}\n\\dot{x} &= a (y - x)\\\\\n\\dot{y} &= b x + c y - x z + w\\\\\n\\dot{z} &= -d z + y^2\\\\\n\\dot{w} &= -e x\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Finance system [Cai2007] . source"},{"id":194,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_jha","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_jha","content":" PredefinedDynamicalSystems.hyper_jha  —  Function function hyper_jha(u0 = [0.1, 0.1, 0.1, 0.1];\n    a = 10.0,\n    b = 28.0,\n    c = 8/3,\n    d = 1.3) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + w\\\\\n\\dot{y} &= x*(b - z) - y\\\\\n\\dot{z} &= x*y - c*z\\\\\n\\dot{w} &= d*w -x*z\n\\end{aligned}\\] An extension of the Lorenz system showchasing hyperchaos [Hussain2015] . source"},{"id":195,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_lorenz","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_lorenz","content":" PredefinedDynamicalSystems.hyper_lorenz  —  Function function hyper_lorenz(u0 = [-10.0, -6.0, 0.0, 10.0];\n    a = 10.0,\n    b = 28.0,\n    c = 8/3,\n    d = -1.0) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + w\\\\\n\\dot{y} &= x*(b - z) - y\\\\\n\\dot{z} &= x*y - c*z\\\\\n\\dot{w} &= d*w -y*z\n\\end{aligned}\\] An extension of the Lorenz system showchasing hyperchaos [Wang2008] . An hyperchaotic system is characterized by two positive Lyapunov exponents. source"},{"id":196,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_lu","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_lu","content":" PredefinedDynamicalSystems.hyper_lu  —  Function function hyper_lu(u0 = [5.0, 8.0, 12.0, 21.0];\n    a = 36,\n    b = 3.0,\n    c = 20.0,\n    d = 1.3) \\[\\begin{aligned}\n\\dot{x} &= a (y - x) + w\\\\\n\\dot{y} &= c y - x z\\\\\n\\dot{z} &= x y - b z\\\\\n\\dot{w} &= d w + x z\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Lu system [Chen2006] . source"},{"id":197,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_pang","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_pang","content":" PredefinedDynamicalSystems.hyper_pang  —  Function function hyper_pang(u0 = [1.0, 1.0, 10.0, 1.0];\n    a = 36,\n    b = 3.0,\n    c = 20.0,\n    d = 2.0) \\[\\begin{aligned}\n\\dot{x} &= a (y - x)\\\\\n\\dot{y} &= -x z + c y + w\\\\\n\\dot{z} &= x y - b z\\\\\n\\dot{w} &= -d x - d y\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Lu system [Pang2011] . source"},{"id":198,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_qi","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_qi","content":" PredefinedDynamicalSystems.hyper_qi  —  Function function hyper_qi(u0 = [10.0, 15.0, 20.0, 22.0];\n    a = 50.0,\n    b = 24.0,\n    c = 13,\n    d = 8,\n    e = 33,\n    f = 30) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + y*z\\\\\n\\dot{y} &= b*(x + y) - xz\\\\\n\\dot{z} &= - c*z - e*w + x*y\\\\\n\\dot{w} &= -d*w + f*z +x*y\n\\end{aligned}\\] A hyperchaotic dynamical systems, showcasing a wide range of different behaviors, including rich bifurcations in different directions [Qi2008] . source"},{"id":199,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_roessler","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_roessler","content":" PredefinedDynamicalSystems.hyper_roessler  —  Function hyper_roessler(u0 = [-10.0, -6.0, 0.0, 10.0];\n    a = 0.25,\n    b = 3.0,\n    c = 0.5,\n    d = 0.05) \\[\\begin{aligned}\n\\dot{x} &= -y - z\\\\\n\\dot{y} &= x + a*y + w\\\\\n\\dot{z} &= b + x*z\\\\\n\\dot{w} &= -c*z + d*w\n\\end{aligned}\\] An extension of the Rössler system showchasing hyperchaos [Rossler1979] . An hyperchaotic system is characterized by two positive Lyapunov exponents. source"},{"id":200,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_wang","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_wang","content":" PredefinedDynamicalSystems.hyper_wang  —  Function function hyper_wang(u0 = [5.0, 1.0, 30.0, 1.0];\n    a = 10.0,\n    b = 40.0,\n    c = 2.5,\n    d = 10.6,\n    e = 4.0) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x)\\\\\n\\dot{y} &= -x*z + b*x + w\\\\\n\\dot{z} &= e*x^2 - c*z\\\\\n\\dot{w} &= -d*x\n\\end{aligned}\\] An extension of the Wang system showchasing hyperchaos [Wang2009] . source"},{"id":201,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_xu","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_xu","content":" PredefinedDynamicalSystems.hyper_xu  —  Function function hyper_xu(u0 = [2.0, -1.0, -2.0, -10.0];\n    a = 10.0,\n    b = 40.0,\n    c = 2.5,\n    d = 2.0,\n    e = 16.0) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + w\\\\\n\\dot{y} &= b*x + e*x*z\\\\\n\\dot{z} &= - c*z - x*y\\\\\n\\dot{w} &= x*z - d*y\n\\end{aligned}\\] A system showchasing hyperchaos [Letellier2007] . source"},{"id":202,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.ikedamap","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.ikedamap","content":" PredefinedDynamicalSystems.ikedamap  —  Function ikedamap(u0=[1.0, 1.0]; a=1.0, b=1.0, c=0.4, d =6.0) -> ds \\[\\begin{aligned}\nt &= c - \\frac{d}{1 + x_n^2 + y_n^2} \\\\\nx_{n+1} &= a + b(x_n \\cos(t) - y\\sin(t)) \\\\\ny_{n+1} &= b(x\\sin(t) + y \\cos(t))\n\\end{aligned}\\] The Ikeda map was proposed by Ikeda as a model to explain the propagation of light into a ring cavity  [Skiadas2008] . It generates a variety of nice-looking, interesting attractors. The default parameters are chosen to give a unique chaotic attractor. A double attractor can be obtained with parameters  [a,b,c,d] = [6, 0.9, 3.1, 6] , and a triple attractor can be obtained with  [a,b,c,d] = [6, 9, 2.22, 6] [Skiadas2008] . [Skiadas2008]  : \"Chaotic Modelling and Simulation: Analysis of Chaotic Models, Attractors and Forms\", CRC Press (2008). source"},{"id":203,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.kuramoto","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.kuramoto","content":" PredefinedDynamicalSystems.kuramoto  —  Function kuramoto(D = 20, u0 = range(0, 2π; length = D);\n    K = 0.3, ω = range(-1, 1; length = D)\n) The Kuramoto model [Kuramoto1975]  of  D  coupled oscillators with equation \\[\\dot{\\phi}_i = \\omega_i + \\frac{K}{D}\\sum_{j=1}^{D} \\sin(\\phi_j - \\phi_i)\\] source"},{"id":204,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.logistic","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.logistic","content":" PredefinedDynamicalSystems.logistic  —  Function logistic(x0 = 0.4; r = 4.0) \\[x_{n+1} = rx_n(1-x_n)\\] The logistic map is an one dimensional unimodal mapping due to May [1] and is used by many as the archetypal example of how chaos can arise from very simple equations. Originally intentend to be a discretized model of polulation dynamics, it is now famous for its bifurcation diagram, an immensely complex graph that that was shown be universal by Feigenbaum [2]. The parameter container has the parameters in the same order as stated in this function's documentation string. [1] : R. M. May, Nature  261 , pp 459 (1976) [2] : M. J. Feigenbaum, J. Stat. Phys.  19 , pp 25 (1978) source"},{"id":205,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz","content":" PredefinedDynamicalSystems.lorenz  —  Function lorenz(u0=[0.0, 10.0, 0.0]; σ = 10.0, ρ = 28.0, β = 8/3) -> ds \\[\\begin{aligned}\n\\dot{X} &= \\sigma(Y-X) \\\\\n\\dot{Y} &= -XZ + \\rho X -Y \\\\\n\\dot{Z} &= XY - \\beta Z\n\\end{aligned}\\] The famous three dimensional system due to Lorenz  [Lorenz1963] , shown to exhibit so-called \"deterministic nonperiodic flow\". It was originally invented to study a simplified form of atmospheric convection. Currently, it is most famous for its strange attractor (occuring at the default parameters), which resembles a butterfly. For the same reason it is also associated with the term \"butterfly effect\" (a term which Lorenz himself disliked) even though the effect applies generally to dynamical systems. Default values are the ones used in the original paper. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":206,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz84","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz84","content":" PredefinedDynamicalSystems.lorenz84  —  Function lorenz84(u = [0.1, 0.1, 0.1]; F=6.846, G=1.287, a=0.25, b=4.0) Lorenz-84's low order atmospheric general circulation model \\[\\begin{aligned}\n\\dot x = − y^2 − z^2 − ax + aF, \\\\\n\\dot y = xy − y − bxz + G, \\\\\n\\dot z = bxy + xz − z. \\\\\n\\end{aligned}\\] This system has interesting multistability property in the phase space. For the default parameter set we have four coexisting attractors that gives birth to interesting fractalized phase space as shown in  [Freire2008] . One can see this by doing: ds = Systems.lorenz84(rand(3))\nxg = yg = range(-1.0, 2.0; length=300)\nzg = range(-1.5, 1.5; length=30)\nbsn, att = basins_of_attraction((xg, yg, zg), ds; mx_chk_att=4) source"},{"id":207,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz96","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz96","content":" PredefinedDynamicalSystems.lorenz96  —  Function lorenz96(N::Int, u0 = rand(M); F=0.01) \\[\\frac{dx_i}{dt} = (x_{i+1}-x_{i-2})x_{i-1} - x_i + F\\] N  is the chain length,  F  the forcing. Jacobian is created automatically. (parameter container only contains  F ) source"},{"id":208,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz_bounded","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz_bounded","content":" PredefinedDynamicalSystems.lorenz_bounded  —  Function lorenz_bounded(u0=[-13.284881, -12.444334, 34.188198];\n    beta = 2.667,\n    r = 64.0,\n    rho = 28.0,\n    sigma = 10.0\n) \\[\\begin{aligned}\n\\dot{X} &= \\sigma(Y-X)f(X,Y,Z) \\\\\n\\dot{Y} &= (-XZ + \\rho X -Y)f(X,Y,Z) \\\\\n\\dot{Z} &= (XY - \\beta Z)f(X,Y,Z)\n\\end{aligned}\\] Lorenz system bounded by a confining potential  [SprottXiong2015] . source"},{"id":209,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenzdl","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenzdl","content":" PredefinedDynamicalSystems.lorenzdl  —  Function lorenzdl(u = [0.1, 0.1, 0.1]; R=4.7) Diffusionless Lorenz system: it is  probably  the simplest rotationnaly invariant chaotic flow. \\[\\begin{aligned}\n\\dot x = y − x, \\\\\n\\dot y = -xz, \\\\\n\\dot z = xy - R. \\\\\n\\end{aligned}\\] For  R=4.7  this system has two coexisting Malasoma strange attractors that are linked together as shown in  [Sprott2014] . The fractal boundary between the basins of attractor can be visualized with a Poincaré section at  z=0 : ds = Systems.lorenzdl()\nxg = yg = range(-10.0, 10.0; length=300)\npmap = poincaremap(ds, (3, 0.), Tmax=1e6; idxs = 1:2)\nbsn, att = basins_of_attraction((xg, yg), pmap) source"},{"id":210,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lotkavolterra","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lotkavolterra","content":" PredefinedDynamicalSystems.lotkavolterra  —  Function lotkavolterra(u0=[10.0, 5.0]; α = 1.5, β = 1, δ=1, γ=3) -> ds \\[\\begin{aligned}\n\\dot{x} &= \\alpha x - \\beta xy, \\\\\n\\dot{y} &= \\delta xy - \\gamma y\n\\end{aligned}\\] The famous Lotka-Volterra model is a simple ecological model describing the interaction between a predator and a prey species (or also parasite and host species). It has been used independently in fields such as epidemics, ecology, and economics  [Hoppensteadt2006] , and is not to be confused with the Competitive Lotka-Volterra model, which describes competitive interactions between species. The  x  variable describes the number of prey, while  y  describes the number of predator. The default parameters are taken from  [Weisstein] , which lead to typical periodic oscillations. source"},{"id":211,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.magnetic_pendulum","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.magnetic_pendulum","content":" PredefinedDynamicalSystems.magnetic_pendulum  —  Function magnetic_pendulum(u=[0.7,0.7,0,0]; d=0.3, α=0.2, ω=0.5, N=3, γs=fill(1.0,N)) Create a pangetic pendulum with  N  magnetics, equally distributed along the unit circle, with dynamical rule \\[\\begin{aligned}\n\\ddot{x} &= -\\omega ^2x - \\alpha \\dot{x} - \\sum_{i=1}^N \\frac{\\gamma_i (x - x_i)}{D_i^3} \\\\\n\\ddot{y} &= -\\omega ^2y - \\alpha \\dot{y} - \\sum_{i=1}^N \\frac{\\gamma_i (y - y_i)}{D_i^3} \\\\\nD_i &= \\sqrt{(x-x_i)^2  + (y-y_i)^2 + d^2}\n\\end{aligned}\\] where α is friction, ω is eigenfrequency, d is distance of pendulum from the magnet's plane and γ is the magnetic strength. source"},{"id":212,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.manneville_simple","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.manneville_simple","content":" PredefinedDynamicalSystems.manneville_simple  —  Function manneville_simple(x0 = 0.4; ε = 1.1) \\[x_{n+1} = [ (1+\\varepsilon)x_n + (1-\\varepsilon)x_n^2 ] \\mod 1\\] A simple 1D map due to Mannevile [Manneville1980]  that is useful in illustrating the concept and properties of intermittency. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":213,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.more_chaos_example","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.more_chaos_example","content":" PredefinedDynamicalSystems.more_chaos_example  —  Function more_chaos_example(u = rand(3)) A three dimensional chaotic system introduced in  [Sprott2020]  with rule \\[\\begin{aligned}\n\\dot{x} &= y \\\\\n\\dot{y} &= -x - \\textrm{sign}(z)y \\\\\n\\dot{z} &= y^2 - \\exp(-x^2)\n\\end{aligned}\\] It is noteworthy because its strange attractor is multifractal with fractal dimension ≈ 3. source"},{"id":214,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.morris_lecar","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.morris_lecar","content":" PredefinedDynamicalSystems.morris_lecar  —  Function morris_lecar(u0=[0.1, 0.1]; I = 0.15, V3 = 0.1, V1 = -0.00, V2 = 0.15, V4 = 0.1,\n    VCa = 1, VL = -0.5, VK = -0.7, gCa = 1.2, gK = 2, gL = 0.5, τ = 3) -> ds The Morris-Lecar model is ubiquitously used in computational neuroscience as a  simplified model for neuronal dynamics  (2D), and can also be in general as an excitable system  [IzhikevichBook] . It uses the formalism of the more complete Hodgkin-Huxley model (4D), and can be viewed as a simplification thereof, with variables V for the membrane potential and N for the recovery of the Potassium current. Its original parameters were obtained from experimental studies of the giant muscle fiber in the Pacific barnacle  [MorrisLecar1981] . Its evolution is given by: \\[\\begin{aligned}\n\\dot{V} &= -g_{Ca} M(V) (V - V_{Ca}) - g_K N (V - V_K) - g_L (V - V_L) + I \\\\\n\\dot{N} &= (-N + G(V)) / \tau \\\\\n\\end{aligned}\\] with \\[\\begin{aligned}\nM(V) = 0.5 (1 + \\tanh((x-V1)/V2)) \\\\\nG(V) = 0.5 (1 + \\tanh((x-V3)/V4)) \\\\\\] source"},{"id":215,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.multispecies_competition","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.multispecies_competition","content":" PredefinedDynamicalSystems.multispecies_competition  —  Function multispecies_competition(option = 1) A model of competition dynamics between multiple species from Huisman and Weissing [Huisman2001] . It highlights fundamental unpredictability by having extreme multistability, fractal basin boundaries and transient chaos. TODO: write here equations when we have access to the paper (not open access). TODO: Describe initial conditions and what option 1 means. source"},{"id":216,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.nld_coupled_logistic_maps","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.nld_coupled_logistic_maps","content":" PredefinedDynamicalSystems.nld_coupled_logistic_maps  —  Function nld_coupled_logistic_maps(D = 4, u0 = range(0, 1; length=D); λ = 1.2, k = 0.08) A high-dimensional discrete dynamical system that couples  D  logistic maps with a strongly nonlinear all-to-all coupling. For the default parameters it displays several co-existing attractors. The equations are: \\[u_i' = \\lambda - u_i^2 + k \\sum_{j\\ne i} (u_j^2 - u_i^2)\\] Here the prime  $'$  denotes next state. source"},{"id":217,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.nosehoover","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.nosehoover","content":" PredefinedDynamicalSystems.nosehoover  —  Function nosehoover(u0 = [0, 0.1, 0]) \\[\\begin{aligned}\n\\dot{x} &= y \\\\\n\\dot{y} &= yz - x \\\\\n\\dot{z} &= 1 - y^2\n\\end{aligned}\\] Three dimensional conservative continuous system, discovered in 1984 during investigations in thermodynamical chemistry by Nosé and Hoover, then rediscovered by Sprott during an exhaustive search as an extremely simple chaotic system.  [Hoover1995] See Chapter 4 of \"Elegant Chaos\" by J. C. Sprott.  [Sprott2010] source"},{"id":218,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.pomeau_manneville","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.pomeau_manneville","content":" PredefinedDynamicalSystems.pomeau_manneville  —  Function pomaeu_manneville(u0 = 0.2; z = 2.5) The Pomeau-Manneville map is a one dimensional discrete map which is characteristic for displaying intermittency [1]. Specifically, for z > 2 the average time between chaotic bursts diverges, while for z > 2.5, the map iterates are long range correlated [2]. Notice that here we are providing the \"symmetric\" version: \\[x_{n+1} = \\begin{cases}\n-4x_n + 3, & \\quad x_n \\in (0.5, 1] \\\\\nx_n(1 + |2x_n|^{z-1}), & \\quad |x_n| \\le 0.5 \\\\\n-4x_n - 3, & \\quad x_n \\in [-1, 0.5)\n\\end{cases}\\] [1] : Manneville & Pomeau, Comm. Math. Phys.  74  (1980) [2] : Meyer et al., New. J. Phys  20  (2019) source"},{"id":219,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.qbh","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.qbh","content":" PredefinedDynamicalSystems.qbh  —  Function qbh([u0]; A=1.0, B=0.55, D=0.4) A conservative dynamical system with rule \\[\\begin{aligned}\n\\dot{q}_0 &= A p_0 \\\\\n\\dot{q}_2 &= A p_2 \\\\\n\\dot{p}_0 &= -A q_0 -3 \\frac{B}{\\sqrt{2}} (q_2^2 - q_0^2) - D q_0 (q_0^2 + q_2^2) \\\\\n\\dot{p}_2 &= -q_2 [A + 3\\sqrt{2} B q_0 + D (q_0^2 + q_2^2)]\n\\end{aligned}\\] This dynamical rule corresponds to a Hamiltonian used in nuclear physics to study the quadrupole vibrations of the nuclear surface  [Eisenberg1975] [Baran1998] . \\[H(p_0, p_2, q_0, q_2) = \\frac{A}{2}\\left(p_0^2+p_2^2\\right)+\\frac{A}{2}\\left(q_0^2+q_2^2\\right)\n\t\t\t +\\frac{B}{\\sqrt{2}}q_0\\left(3q_2^2-q_0^2\\right) +\\frac{D}{4}\\left(q_0^2+q_2^2\\right)^2\\] The Hamiltonian has a similar structure with the Henon-Heiles one, but it has an added fourth order term and presents a nontrivial dependence of chaoticity with the increase of energy [^Micluta-Campeanu2018]. The default initial condition is chaotic. [^Micluta-Campeanu2018]:     Micluta-Campeanu S., Raportaru M.C., Nicolin A.I., Baran V., Rom. Rep. Phys.      70 , pp 105 (2018) source"},{"id":220,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.riddled_basins","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.riddled_basins","content":" PredefinedDynamicalSystems.riddled_basins  —  Function riddled_basins(u0=[0.5, 0.6, 0, 0]; γ=0.05, x̄ = 1.9, f₀=2.3, ω =3.5, x₀=1, y₀=0) → ds \\[\\begin{aligned}\n\\dot{x} &= v_x, \\quad \\dot{y} = v_z \\\\\n\\dot{v}_x &= -\\gamma v_x - [ -4x(1-x^2) +y^2] + f_0 \\sin(\\omega t)x_0 \\\\\n\\dot{v}_y &= -\\gamma v_y - 2y (x+\\bar{x}) + f_0 \\sin(\\omega t)y_0\n\\end{aligned}\\] This 5 dimensional (time-forced) dynamical system was used by Ott et al  [OttRiddled2014]  to analyze  riddled basins of attraction . This means nearby any point of a basin of attraction of an attractor A there is a point of the basin of attraction of another attractor B. source"},{"id":221,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.rikitake","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.rikitake","content":" PredefinedDynamicalSystems.rikitake  —  Function rikitake(u0 = [1, 0, 0.6]; μ = 1.0, α = 1.0) \\[\\begin{aligned}\n\\dot{x} &= -\\mu x +yz \\\\\n\\dot{y} &= -\\mu y +x(z-\\alpha) \\\\\n\\dot{z} &= 1 - xz\n\\end{aligned}\\] Rikitake's dynamo  [Rikitake1958]  is a system that tries to model the magnetic reversal events by means of a double-disk dynamo system. source"},{"id":222,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.roessler","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.roessler","content":" PredefinedDynamicalSystems.roessler  —  Function roessler(u0=[1, -2, 0.1]; a = 0.2, b = 0.2, c = 5.7) \\[\\begin{aligned}\n\\dot{x} &= -y-z \\\\\n\\dot{y} &= x+ay \\\\\n\\dot{z} &= b + z(x-c)\n\\end{aligned}\\] This three-dimensional continuous system is due to Rössler  [Rössler1976] . It is a system that by design behaves similarly to the  lorenz  system and displays a (fractal) strange attractor. However, it is easier to analyze qualitatively, as for example the attractor is composed of a single manifold. Default values are the same as the original paper. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":223,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.rulkovmap","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.rulkovmap","content":" PredefinedDynamicalSystems.rulkovmap  —  Function rulkovmap(u0=[1.0, 1.0]; α=4.1, β=0.001, σ=0.001) -> ds \\[\\begin{aligned}\nx_{n+1} &= \\frac{\\alpha}{1+x_n^2} + y_n  \\\\\ny_{n+1} &= y_n - \\sigma x_n - \\beta\n\\end{aligned}\\] The Rulkov map is a two-dimensional phenomenological model of a neuron capable of describing spikes and bursts. It was described by Rulkov  [Rulkov2002]  and is used in studies of neural networks due to its computational advantages, being fast to run. The parameters σ and β  are generally kept at  0.001 , while α is chosen to give the desired dynamics. The dynamics can be quiescent for α ∈ (0,2), spiking for α ∈ (2, 2.58), triangular bursting for α ∈ (2.58, 4), and rectangular bursting for α ∈ (4, 4.62)  [Rulkov2001] [Cao2013] . The default parameters are taken from  [Rulkov2001]  to lead to a rectangular bursting. [Rulkov2002]  : \"Modeling of spiking-bursting neural behavior using two-dimensional map\", Phys. Rev. E 65, 041922 (2002). [Rulkov2001]  : \"Regularization of Synchronized Chaotic Bursts\", Phys. Rev. Lett. 86, 183 (2001). [Cao2013]  : H. Cao and Y Wu, \"Bursting types and stable domains of Rulkov neuron network with mean field coupling\", International Journal of Bifurcation and Chaos,23:1330041 (2013). source"},{"id":224,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.sakarya","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.sakarya","content":" PredefinedDynamicalSystems.sakarya  —  Function sakarya(u0= [-2.8976045, 3.8877978, 3.07465];\n    a = 1,\n    b = 1,\n    m = 1\n) \\[\\begin{aligned}\n\\dot{x} &= ax + y + yz\\\\\n\\dot{y} &= - xz + yz \\\\\n\\dot{z} &= - z - mxy + b\n\\end{aligned}\\] A system presenting robust chaos that varies from single wing to double wings to four wings. Its attractor arises due to merging of two disjoint bistable attractors  [Li2015] . source"},{"id":225,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.shinriki","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.shinriki","content":" PredefinedDynamicalSystems.shinriki  —  Function shinriki(u0 = [-2, 0, 0.2]; R1 = 22.0) Shinriki oscillator with all other parameters (besides  R1 ) set to constants.  This is a stiff problem, be careful when choosing solvers and tolerances . source"},{"id":226,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.sprott_dissipative_conservative","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.sprott_dissipative_conservative","content":" PredefinedDynamicalSystems.sprott_dissipative_conservative  —  Function sprott_dissipative_conservative(u0 = [1.0, 0, 0]; a = 2, b = 1, c = 1) An interesting system due to Sprott [Sprott2014b]  where some initial conditios such as  [1.0, 0, 0]  lead to quasi periodic motion on a 2-torus, while for  [2.0, 0, 0]  motion happens on a (dissipative) chaotic attractor. The equations are: \\[\\begin{aligned}\n\\dot{x} &= y + axy + xz \\\\\n\\dot{y} &= 1 - 2x^2 + byz \\\\\n\\dot{z_1} &= cx - x^2 - y^2\n\\end{aligned}\\] In the original paper there were no parameters, which are added here for exploration purposes. source"},{"id":227,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.standardmap","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.standardmap","content":" PredefinedDynamicalSystems.standardmap  —  Function standardmap(u0=[0.001245, 0.00875]; k = 0.971635) \\[\\begin{aligned}\n\\theta_{n+1} &= \\theta_n + p_{n+1} \\\\\np_{n+1} &= p_n + k\\sin(\\theta_n)\n\\end{aligned}\\] The standard map (also known as Chirikov standard map) is a two dimensional, area-preserving chaotic mapping due to Chirikov [1]. It is one of the most studied chaotic systems and by far the most studied Hamiltonian (area-preserving) mapping. The map corresponds to the  Poincaré's surface of section of the kicked rotor system. Changing the non-linearity parameter  k  transitions the system from completely periodic motion, to quasi-periodic, to local chaos (mixed phase-space) and finally to global chaos. The default parameter  k  is the critical parameter where the golden-ratio torus is destroyed, as was calculated by Greene [2]. The e.o.m. considers the angle variable  θ  to be the first, and the angular momentum  p  to be the second, while both variables are always taken modulo 2π (the mapping is on the [0,2π)² torus). The parameter container has the parameters in the same order as stated in this function's documentation string. [1] : B. V. Chirikov, Preprint N.  267 , Institute of Nuclear Physics, Novosibirsk (1969) [2] : J. M. Greene, J. Math. Phys.  20 , pp 1183 (1979) source"},{"id":228,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.stommel_thermohaline","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.stommel_thermohaline","content":" PredefinedDynamicalSystems.stommel_thermohaline  —  Function stommel_thermohaline(u = [0.3, 0.2]; η1 = 3.0, η2 = 1, η3 = 0.3) Stommel's box model for Atlantic thermohaline circulation \\[\\begin{aligned}\n \\dot{T} &= \\eta_1 - T - |T-S| T \\\\\n \\dot{S} &= \\eta_2 - \\eta_3S - |T-S| S\n\\end{aligned}\\] Here  $T, S$  denote the dimensionless temperature and salinity differences respectively between the boxes (polar and equatorial ocean basins) and  $\\eta_i$  are parameters. source"},{"id":229,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.stuartlandau_oscillator","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.stuartlandau_oscillator","content":" PredefinedDynamicalSystems.stuartlandau_oscillator  —  Function stuartlandau_oscillator(u0=[1.0, 0.0]; μ=1.0, ω=1.0, b=1) -> ds The Stuart-Landau model describes a nonlinear oscillation near a Hopf bifurcation, and was proposed by Landau in 1944 to explain the transition to turbulence in a fluid  [Landau1944] . It can be written in cartesian coordinates as  [Deco2017] \\[\\begin{aligned}\n\\dot{x} &= (\\mu -x^2 -y^2)x - \\omega y - b(x^2+y^2)y \\\\\n\\dot{y} &= (\\mu -x^2 -y^2)y + \\omega x + b(x^2+y^2)x\n\\end{aligned}\\] The dynamical analysis of the system is greatly facilitated by putting it in polar coordinates, where it takes the normal form of the supercritical Hopf bifurcation)  [Strogatz2015] . \\[\\begin{aligned}\n\\dot{r} &= \\mu r - r^3, \\\\\n\\dot{\\theta} &= \\omega +br^2\n\\end{aligned}\\] The parameter  \\mu  serves as the bifurcation parameter,  \\omega  is the frequency of infinitesimal oscillations, and  b  controls the dependence of the frequency on the amplitude.  Increasing  \\mu  from negative to positive generates the supercritical Hopf bifurcation, leading from a stable spiral at the origin to a stable limit cycle with radius  \\sqrt(\\mu) . source"},{"id":230,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.swinging_atwood","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.swinging_atwood","content":" PredefinedDynamicalSystems.swinging_atwood  —  Function swinging_atwood(u0=[0.113296,1.5707963267948966,0.10992,0.17747]; m1=1.0, m2=4.5) \\[\\begin{aligned}\n\\dot{r} &= \\frac{p_r}{M+m}\\\\\n\\dot{p}_r &= -Mg + mg\\cos(\\theta)\\\\\n\\dot{\\theta} &= \\frac{p_{\\theta}}{mr^2}\\\\\n\\dot{p}_{\\theta} &= -mgr\\sin(\\theta)\n\\end{aligned}\\] A mechanical system consisting of two swinging weights connected by ropes and pulleys. This is only chaotic when  m2  is sufficiently larger than  m1 , and there are nonzero initial momenta  [Tufillaro1984] . source"},{"id":231,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.tentmap","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.tentmap","content":" PredefinedDynamicalSystems.tentmap  —  Function tentmap(u0 = 0.2; μ=2) -> ds The tent map is a piecewise linear, one-dimensional map that exhibits chaotic behavior in the interval  [0,1] [Ott2002] . Its simplicity allows it to be geometrically interpreted as generating a streching and folding process, necessary for chaos. The equations describing it are: \\[\\begin{aligned}\nx_{n+1} = \\begin{cases} \\mu x, \\quad &x_n < \\frac{1}{2} \\\\\n                         \\mu (1-x), \\quad &\\frac{1}{2} \\leq x_n\n            \\end{cases}\n\\end{aligned}\\] The parameter μ should be kept in the interval  [0,2] . At μ=2, the tent map can be brought to the logistic map with  r=4  by a change of coordinates. [Ott2002]  : E. Ott, \"Chaos in Dynamical Systems\" (2nd ed.) Cambridge: Cambridge University Press (2010). source"},{"id":232,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.thomas_cyclical","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.thomas_cyclical","content":" PredefinedDynamicalSystems.thomas_cyclical  —  Function thomas_cyclical(u0 = [1.0, 0, 0]; b = 0.2) \\[\\begin{aligned}\n\\dot{x} &= \\sin(y) - bx\\\\\n\\dot{y} &= \\sin(z) - by\\\\\n\\dot{z} &= \\sin(x) - bz\n\\end{aligned}\\] Thomas' cyclically symmetric attractor is a 3D strange attractor originally proposed by René Thomas [Thomas1999] . It has a simple form which is cyclically symmetric in the x, y, and z variables and can be viewed as the trajectory of a frictionally dampened particle moving in a 3D lattice of forces. For more see the  Wikipedia page . Reduces to the labyrinth system for  b=0 , see See discussion in Section 4.4.3 of \"Elegant Chaos\" by J. C. Sprott. source"},{"id":233,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.towel","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.towel","content":" PredefinedDynamicalSystems.towel  —  Function towel(u0 = [0.085, -0.121, 0.075]) \\[\\begin{aligned}\nx_{n+1} &= 3.8 x_n (1-x_n) -0.05 (y_n +0.35) (1-2z_n) \\\\\ny_{n+1} &= 0.1 \\left[ \\left( y_n +0.35 \\right)\\left( 1+2z_n\\right) -1 \\right]\n\\left( 1 -1.9 x_n \\right) \\\\\nz_{n+1} &= 3.78 z_n (1-z_n) + b y_n\n\\end{aligned}\\] The folded-towel map is a hyperchaotic mapping due to Rössler [1]. It is famous for being a mapping that has the smallest possible dimensions necessary for hyperchaos, having two positive and one negative Lyapunov exponent. The name comes from the fact that when plotted looks like a folded towel, in every projection. Default values are the ones used in the original paper. [1] : O. E. Rössler, Phys. Lett.  71A , pp 155 (1979) source"},{"id":234,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.ueda","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.ueda","content":" PredefinedDynamicalSystems.ueda  —  Function ueda(u0 = [3.0, 0]; k = 0.1, B = 12.0) \\[\\ddot{x} + k \\dot{x} + x^3 = B\\cos{t}\\] Nonautonomous Duffing-like forced oscillation system, discovered by Ueda in It is one of the first chaotic systems to be discovered. The stroboscopic plot in the (x, ̇x) plane with period 2π creates a \"broken-egg attractor\" for k = 0.1 and B = 12. Figure 5 of  [Ruelle1980]  is reproduced by using Plots\nds = Systems.ueda()\na = trajectory(ds, 2π*5e3, dt = 2π)\nscatter(a[:, 1], a[:, 2], markersize = 0.5, title=\"Ueda attractor\") For more forced oscillation systems, see Chapter 2 of \"Elegant Chaos\" by J. C. Sprott.  [Sprott2010] source"},{"id":235,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.ulam","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.ulam","content":" PredefinedDynamicalSystems.ulam  —  Function ulam(N = 100, u0 = cos.(1:N); ε = 0.6) A discrete system of  N  unidirectionally coupled maps on a circle, with equations \\[x^{(m)}_{n+1} = f(\\varepsilon x_n^{(m-1)} + (1-\\varepsilon)x_n^{(m)});\\quad f(x) = 2 - x^2\\] source"},{"id":236,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.vanderpol","ref":"/DynamicalSystemsDocs.jl/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.vanderpol","content":" PredefinedDynamicalSystems.vanderpol  —  Function vanderpol(u0=[0.5, 0.0]; μ=1.5, F=1.2, T=10) -> ds \\[\\begin{aligned}\n\\ddot{x} -\\mu (1-x^2) \\dot{x} + x = F \\cos(2\\pi t / T)\n\\end{aligned}\\] The forced van der Pol oscillator is an oscillator with a nonlinear damping term driven by a sinusoidal forcing. It was proposed by Balthasar van der Pol, in his studies of nonlinear electrical circuits used in the first radios  [Kanamaru2007] [Strogatz2015] . The unforced oscillator ( F = 0 ) has stable oscillations in the form of a limit cycle with a slow buildup followed by a sudden discharge, which van der Pol called relaxation oscillations  [Strogatz2015] [vanderpol1926] . The forced oscillator ( F > 0 ) also has periodic behavior for some parameters, but can additionally have chaotic behavior. The van der Pol oscillator is a specific case of both the FitzHugh-Nagumo neural model  [Kanamaru2007] . The default damping parameter is taken from  [Strogatz2015]  and the forcing parameters are taken from  [Kanamaru2007] , which generate periodic oscillations. Setting  $\\mu=8.53$  generates chaotic oscillations. source Datseris2019 G. Datseris  et al ,  New Journal of Physics 2019 Chua1992 Chua, Leon O. \"The genesis of Chua's circuit\", 1992. Chua2007 Leon O. Chua (2007) \"Chua circuit\", Scholarpedia, 2(10):1488. Gissinger2012 C. Gissinger, Eur. Phys. J. B  85 , 4, pp 1-12 (2012) Grebogi1983 C. Grebogi, S. W. McDonald, E. Ott and J. A. Yorke, Final state sensitivity: An obstruction to predictability, Physics Letters A, 99, 9, 1983 GuckenheimerHolmes1983 Guckenheimer, John, and Philip Holmes (1983). Nonlinear oscillations, dynamical systems, and bifurcations of vector fields. Vol. 42. Springer Science & Business Media. Sprott2010 Sprott, Julien C (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific, 2010. HénonHeiles1964 Hénon, M. & Heiles, C., The Astronomical Journal  69 , pp 73–79 (1964) HindmarshRose1984 J. L. Hindmarsh and R. M. Rose (1984) \"A model of neuronal bursting using three coupled first order differential equations\", Proc. R. Soc. Lond. B 221, 87-102. HodgkinHuxley1952 A. L. Hodgkin, A.F. Huxley J. Physiol., pp. 500-544 (1952). Ermentrout2010 G. Bard Ermentrout, and David H. Terman, \"Mathematical Foundations of Neuroscience\", Springer (2010). Abbott2005 L. F. Abbott, and P. Dayan, \"Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems\", MIT Press (2005). Cai2007 Cai, G., & Huang, J. (2007). A new finance chaotic attractor. International Journal of Nonlinear Science, 3(3), 213-220. Hussain2015 Hussain, I., Gondal, M. A., & Hussain, A. (2015). Construction of dynamical non-linear components based on lorenz system and symmetric group of permutations. 3D Research, 6, 1-6. Wang2008 Wang, X., & Wang, M. (2008). A hyperchaos generated from Lorenz system. Physica A: Statistical Mechanics and its Applications, 387(14), 3751-3758. Chen2006 Chen, A., Lu, J., Lü, J., & Yu, S. (2006). Generating hyperchaotic Lü attractor via state feedback control. Physica A: Statistical Mechanics and its Applications, 364, 103-110. Pang2011 Pang, S., & Liu, Y. (2011). A new hyperchaotic system from the Lü system and its control. Journal of Computational and Applied Mathematics, 235(8), 2775-2789. Qi2008 Qi, G., van Wyk, M. A., van Wyk, B. J., & Chen, G. (2008). On a new hyperchaotic system. Physics Letters A, 372(2), 124-136. Rossler1979 Rossler, O. (1979). An equation for hyperchaos. Physics Letters A, 71(2-3), 155-157. Wang2009 Wang, Z., Sun, Y., van Wyk, B. J., Qi, G., & van Wyk, M. A. (2009). A 3-D four-wing attractor and its analysis. Brazilian Journal of Physics, 39, 547-553. Letellier2007 Letellier, C., & Rossler, O. E. (2007). Hyperchaos. Scholarpedia, 2(8), 1936. Kuramoto1975 Kuramoto, Yoshiki. International Symposium on Mathematical Problems in Theoretical Physics. 39. Lorenz1963 E. N. Lorenz, J. atmos. Sci.  20 , pp 130 (1963) Freire2008 J. G. Freire  et al ,  Multistability, phase diagrams, and intransitivity in the Lorenz-84 low-order atmospheric circulation model, Chaos 18, 033121 (2008) SprottXiong2015 Sprott, J. C., & Xiong, A. (2015). Classifying and quantifying basins of attraction. Chaos: An Interdisciplinary Journal of Nonlinear Science, 25(8), 083101. Sprott2014 J. C. Sprott,  Simplest Chaotic Flows with Involutional Symmetries, Int. Jour. Bifurcation and Chaos 24, 1450009 (2014) Hoppensteadt2006 Frank Hoppensteadt (2006) \"Predator-prey model\", Scholarpedia, 1(10):1563. Weisstein Weisstein, Eric W., \"Lotka-Volterra Equations.\" From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/Lotka-VolterraEquations.html Manneville1980 Manneville, P. (1980). Intermittency, self-similarity and 1/f spectrum in dissipative dynamical systems.  Journal de Physique, 41(11), 1235–1243 Sprott2020 Sprott, J.C. 'Do We Need More Chaos Examples?', Chaos Theory and Applications 2(2),1-3, 2020 IzhikevichBook Izhikevich, E. M., Dynamical systems in neuroscience: The geometry of excitability and bursting, 2007, MIT Press. MorrisLecar1981 Morris, C. and Lecar, H,  Voltage oscillations in the barnacle giant muscle fiber, 1981 . Huisman2001 Huisman & Weissing 2001, Fundamental Unpredictability in Multispecies Competition  The American Naturalist Vol. 157, No. 5. Hoover1995 Hoover, W. G. (1995). Remark on ‘‘Some simple chaotic flows’’.  Physical Review E ,  51 (1), 759. Sprott2010 Sprott, J. C. (2010).  Elegant chaos: algebraically simple chaotic flows . World Scientific. Eisenberg1975 Eisenberg, J.M., & Greiner, W., Nuclear theory 2 rev ed. Netherlands: North-Holland pp 80 (1975) Baran1998 Baran V. and Raduta A. A., International Journal of Modern Physics E,  7 , pp 527–551 (1998) OttRiddled2014 Ott. et al.,  The transition to chaotic attractors with riddled basins Rikitake1958 T. Rikitake Math. Proc. Camb. Phil. Soc.  54 , pp 89–105, (1958) Rössler1976 O. E. Rössler, Phys. Lett.  57A , pp 397 (1976) Li2015 Li, Chunbiao, et al (2015). A novel four-wing strange attractor born in bistability. IEICE Electronics Express 12.4. Sprott2014b J. C. Sprott. Physics Letters A, 378 Stommel1961 Stommel, Thermohaline convection with two stable regimes of flow. Tellus, 13(2) Landau1944 L. D. Landau, \"On the problem of turbulence, In Dokl. Akad. Nauk SSSR (Vol. 44, No. 8, pp. 339-349) (1944). Deco2017 G. Deco et al \"The dynamics of resting fluctuations in the brain: metastability and its dynamical cortical core\",  Sci Rep 7, 3095 (2017). Strogatz2015 Steven H. Strogatz \"Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering\", Boulder, CO :Westview Press, a member of the Perseus Books Group (2015). Tufillaro1984 Tufillaro, Nicholas B.; Abbott, Tyler A.; Griffiths, David J. (1984). Swinging Atwood's Machine. American Journal of Physics. 52 (10): 895–903. Thomas1999 Thomas, R. (1999).  International Journal of Bifurcation and Chaos ,  9 (10), 1889-1905. Ruelle1980 Ruelle, David, ‘Strange Attractors’, The Mathematical Intelligencer, 2.3 (1980), 126–37 Sprott2010 Sprott, J. C. (2010).  Elegant chaos: algebraically simple chaotic flows . World Scientific. Kanamaru2007 Takashi Kanamaru (2007) \"Van der Pol oscillator\", Scholarpedia, 2(1):2202. Strogatz2015 Steven H. Strogatz (2015) \"Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering\", Boulder, CO :Westview Press, a member of the Perseus Books Group. vanderpol1926 B. Van der Pol (1926), \"On relaxation-oscillations\", The London, Edinburgh and Dublin Phil. Mag. & J. of Sci., 2(7), 978–992."},{"id":241,"pagetitle":"ChaosTools.jl","title":"ChaosTools.jl","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#ChaosTools.jl","content":" ChaosTools.jl"},{"id":242,"pagetitle":"ChaosTools.jl","title":"ChaosTools","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#ChaosTools","content":" ChaosTools  —  Module ChaosTools.jl A Julia module that offers various tools for analysing nonlinear dynamics and chaotic behaviour. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"ChaosTools\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. ChaosTools.jl is the jack-of-all-trades package of the DynamicalSystems.jl library: methods that are not extensive enough to be a standalone package are added here. You should see the full DynamicalSystems.jl library for other packages that may contain functionality you are looking for but did not find in ChaosTools.jl. source Accompanying textbook A good background for understanding the methods of ChaosTools.jl is the following textbook:  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022."},{"id":243,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.jl reference","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.jl-reference","content":" DynamicalSystemsBase.jl reference As many docstrings in ChaosTools.jl point to the different  DynamicalSystem  types, they are also provided here for reference. DynamicalSystem DeterministicIteratedMap CoupledODEs CoreDynamicalSystem StroboscopicMap PoincareMap TangentDynamicalSystem ParallelDynamicalSystem ProjectedDynamicalSystem reinit!"},{"id":244,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has on for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description Note The documentation of  DynamicalSystem  follows chapter 1 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. A  ds::DynamicalSystem representes a flow Φ in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is a standard Julia function, see below. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f  defined as a standard Julia function.  Observed  or  measured  data from a dynamical system are represented using  StateSpaceSet  and are finite. Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. Construction instructions on  f  and  u Most of the concrete implementations of  DynamicalSystem , with the exception of  ArbitrarySteppable , have two ways of implementing the dynamic rule  f , and as a consequence the type of the state  u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f . Autonomous vs non-autonomous systems Whether the dynamical system is autonomous ( f  doesn't depend on time) or not, it is still necessary to include  t  as an argument to  f . Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way. You can also convert any system to autonomous by making time an additional variable. If the system is non-autonomous, its  effective dimensionality  is  dimension(ds)+1 . API The API that the interface of  DynamicalSystem  employs is the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can quieried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state current_parameters initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace succesful_step API - alter status reinit! set_state! set_parameter! set_parameters!"},{"id":245,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem ."},{"id":246,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.CoupledODEs","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem . DifferentialEquations.jl keyword arguments and interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(; stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false),), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  , however the majority of downstream functions in DynamicalSystems.jl assume that  f  is differentiable. The convenience constructor  CoupledODEs(prob::ODEProblem, diffeq)  and  CoupledODEs(ds::CoupledODEs, diffeq)  are also available. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available."},{"id":247,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.CoreDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.CoreDynamicalSystem","content":" DynamicalSystemsBase.CoreDynamicalSystem  —  Type CoreDynamicalSystem Union type meaning either  DeterministicIteratedMap  or  CoupledODEs , which are the core systems whose dynamic rule  f  is known analytically. This type is used for deciding whether a creation of a  TangentDynamicalSystem  is possible or not."},{"id":248,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap ."},{"id":249,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.PoincareMap","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap)"},{"id":250,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.TangentDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.TangentDynamicalSystem","content":" DynamicalSystemsBase.TangentDynamicalSystem  —  Type TangentDynamicalSystem <: DynamicalSystem\nTangentDynamicalSystem(ds::CoreDynamicalSystem; kwargs...) A dynamical system that bundles the evolution of  ds  (which must be an  CoreDynamicalSystem ) and  k  deviation vectors that are evolved according to the  dynamics in the tangent space  (also called linearized dynamics or the tangent dynamics). The state of  ds must  be an  AbstractVector  for  TangentDynamicalSystem . TangentDynamicalSystem  follows the  DynamicalSystem  interface with the following adjustments: reinit!  takes an additional keyword  Q0  (with same default as below) The additional functions  current_deviations  and  set_deviations!  are provided for the deviation vectors. Keyword arguments k  or  Q0 :  Q0  represents the initial deviation vectors (each column = 1 vector). If  k::Int  is given, a matrix  Q0  is created with the first  k  columns of the identity matrix. Otherwise  Q0  can be given directly as a matrix. It must hold that  size(Q, 1) == dimension(ds) . You can use  orthonormal  for random orthonormal vectors. By default  k = dimension(ds)  is used. u0 = current_state(ds) : Starting state. J  and  J0 : See section \"Jacobian\" below. Description Let  $u$  be the state of  ds , and  $y$  a deviation (or perturbation) vector. These two are evolved in parallel according to \\[\\begin{array}{rcl}\n\\frac{d\\vec{x}}{dt} &=& f(\\vec{x}) \\\\\n\\frac{dY}{dt} &=& J_f(\\vec{x}) \\cdot Y\n\\end{array}\n\\quad \\mathrm{or}\\quad\n\\begin{array}{rcl}\n\\vec{x}_{n+1} &=& f(\\vec{x}_n) \\\\\nY_{n+1} &=& J_f(\\vec{x}_n) \\cdot Y_n.\n\\end{array}\\] for continuous or discrete time respectively. Here  $f$  is the  dynamic_rule (ds)  and  $J_f$  is the Jacobian of  $f$ . Jacobian The keyword  J  provides the Jacobian function. It must be a Julia function in the same form as  f , the  dynamic_rule . Specifically,  J(u, p, n) -> M::SMatrix  for the out-of-place version or  J(M, u, p, n)  for the in-place version acting in-place on  M . in both cases  M  is a matrix whose columns are the deviation vectors. By default  J = nothing .  In this case  J  is constructed automatically using the module  ForwardDiff , hence its limitations also apply here. Even though  ForwardDiff  is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so it is recommended. Additionally, automatic and in-place Jacobians cannot be time dependent. The keyword  J0  allows you to pass an initialized Jacobian matrix  J0 . This is useful for large in-place systems where only a few components of the Jacobian change during the time evolution.  J0  can be a sparse or any other matrix type. If not given, a matrix of zeros is used.  J0  is ignored for out of place systems."},{"id":251,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.ParallelDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.ParallelDynamicalSystem","content":" DynamicalSystemsBase.ParallelDynamicalSystem  —  Type ParallelDynamicalSystem <: DynamicalSystem\nParallelDynamicalSystem(ds::DynamicalSystem, states::Vector{<:AbstractArray}) A struct that evolves several  states  of a given dynamical system in parallel  at exactly the same times . Useful when wanting to evolve several different trajectories of the same system while ensuring that they share parameters and time vector. This struct follows the  DynamicalSystem  interface with the following adjustments: The function  current_state  is called as  current_state(pds, i::Int = 1)  which returns the  i th state. Same for  initial_state . Similarly,  set_state!  obtains a third argument  i::Int = 1  to set the  i -th state. current_states  and  initial_states  can be used to get all parallel states. reinit!  takes in a vector of states (like  states ) for  u ."},{"id":252,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\npds = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(pds, [0.2, 0.4])\nstep!(pds)\nget_state(pds) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\npds = # same as in above example..."},{"id":253,"pagetitle":"ChaosTools.jl","title":"SciMLBase.reinit!","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/#SciMLBase.reinit!-Tuple{DynamicalSystem, Vararg{Any}}","content":" SciMLBase.reinit!  —  Method reinit!(ds::DynamicalSystem, u = initial_state(ds); kwargs...) → ds Reset the status of  ds , so that it is as if it has be just initialized with initial state  u . Practically every function of the ecosystem that evolves  ds  first calls this function on it. Besides the new initial state  u , you can also configure the keywords  t0 = initial_time(ds)  and  p = current_parameters(ds) . Note the default settings: the state and time are the initial, but the parameters are the current. The special method  reinit!(ds, ::Nothing; kwargs...)  is also available, which does nothing and leaves the system as is. This is so that downstream functions that call  reinit!  can still be used without resetting the system but rather continuing from its exact current state. DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":256,"pagetitle":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#Detecting-and-Categorizing-Chaos","content":" Detecting & Categorizing Chaos Being able to detect and distinguish chaotic from regular behavior is crucial in the study of dynamical systems. Most of the time a positive maximum  lyapunov  exponent and a bounded system indicate chaos. However, the convergence of the Lyapunov exponent can be slow, or even misleading, as the types of chaotic behavior vary with respect to their predictability. There are some alternatives, some more efficient and some more accurate in characterizing chaotic and regular motion."},{"id":257,"pagetitle":"Detecting & Categorizing Chaos","title":"Generalized Alignment Index","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#Generalized-Alignment-Index","content":" Generalized Alignment Index \"GALI\" for sort, is a method that relies on the fact that initially orthogonal deviation vectors tend to align towards the direction of the maximum Lyapunov exponent for chaotic motion. It is one of the most recent and cheapest methods for distinguishing chaotic and regular behavior, introduced first in 2007 by Skokos, Bountis & Antonopoulos."},{"id":258,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.gali","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#ChaosTools.gali","content":" ChaosTools.gali  —  Function gali(ds::DynamicalSystem, T, k::Int; kwargs...) -> GALI_k, t Compute  $\\text{GALI}_k$ [Skokos2007]  for a given  k  up to time  T . Return  $\\text{GALI}_k(t)$  and time vector  $t$ . The third argument sets the order of  gali .  gali  function simply initializes a  TangentDynamicalSystem  with  k  deviation vectors and calls the method below. This means that the automatic Jacobian is used by default. Initialize manually a  TangentDynamicalSystem  if you have a hand-coded Jacobian. Keyword arguments threshold = 1e-12 : If  GALI_k  falls below the  threshold  iteration is terminated. Δt = 1 : Time-step between deviation vector normalizations. For continuous systems this is approximate. u0 : Initial state for the system. Defaults to  current_state(ds) . Description The Generalized Alignment Index,  $\\text{GALI}_k$ , is an efficient (and very fast) indicator of chaotic or regular behavior type in  $D$ -dimensional Hamiltonian systems ( $D$  is number of variables). The  asymptotic  behavior of  $\\text{GALI}_k(t)$  depends critically on the type of orbit resulting from the initial condition. If it is a chaotic orbit, then \\[\\text{GALI}_k(t) \\sim\n\\exp\\left[\\sum_{j=1}^k (\\lambda_1 - \\lambda_j)t \\right]\\] with  $\\lambda_j$  being the  j -th Lyapunov exponent (see  lyapunov ,  lyapunovspectrum ). If on the other hand the orbit is regular, corresponding to movement in  $d$ -dimensional torus with  $1 \\le d \\le D/2$  then it holds \\[\\text{GALI}_k(t) \\sim\n    \\begin{cases}\n      \\text{const.}, & \\text{if} \\;\\; 2 \\le k \\le d  \\; \\; \\text{and}\n      \\; \\;d > 1 \\\\\n      t^{-(k - d)}, & \\text{if} \\;\\;  d < k \\le D - d \\\\\n      t^{-(2k - D)}, & \\text{if} \\;\\;  D - d < k \\le D\n    \\end{cases}\\] Traditionally, if  $\\text{GALI}_k(t)$  does not become less than the  threshold  until  T  the given orbit is said to be chaotic, otherwise it is regular. Our implementation is not based on the original paper, but rather in the method described in [Skokos2016b] , which uses the product of the singular values of  $A$ , a matrix that has as  columns  the deviation vectors. source gali(tands::TangentDynamicalSystem, T; threshold = 1e-12, Δt = 1) The low-level method that is called by  gali(ds::DynamicalSystem, ...) . Use this method for looping over different initial conditions or parameters by calling  reinit!  to  tands . The order of  $\\text{GALI}_k$  computed is the amount of deviation vectors in  tands . Also use this method if you have a hand-coded Jacobian to pass when creating  tands . source"},{"id":259,"pagetitle":"Detecting & Categorizing Chaos","title":"GALI example","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#GALI-example","content":" GALI example As an example let's use the Henon-Heiles system using ChaosTools, CairoMakie\nusing OrdinaryDiffEq: Vern9\n\nfunction henonheiles_rule(u, p, t)\n    SVector(u[3], u[4],\n        -u[1] - 2u[1]*u[2],\n        -u[2] - (u[1]^2 - u[2]^2),\n    )\nend\nfunction henonheiles_jacob(u, p, t)\n    SMatrix{4,4}(0, 0, -1 - 2u[2], -2u[1], 0, 0,\n     -2u[1], -1 + 2u[2], 1, 0, 0, 0, 0, 1, 0, 0)\nend\n\nu0=[0, -0.25, 0.42081, 0]\nΔt = 1.0\ndiffeq = (abstol=1e-9, retol=1e-9, alg = Vern9(), maxiters = typemax(Int))\nsp = [0, .295456, .407308431, 0] # stable periodic orbit: 1D torus\nqp = [0, .483000, .278980390, 0] # quasiperiodic orbit: 2D torus\nch = [0, -0.25, 0.42081, 0]      # chaotic orbit\nds = CoupledODEs(henonheiles_rule, sp) 4-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  henonheiles_rule\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    SciMLBase.NullParameters()\n time:          0.0\n state:         [0.0, 0.295456, 0.407308431, 0.0]\n Let's see what happens with a quasi-periodic orbit: tr = trajectory(ds, 10000.0, qp; Δt)[1]\nfig, ax = scatter(tr[:,1], tr[:,3]; label=\"qp\", markersize=2)\naxislegend(ax)\n\nax = Axis(fig[1,2]; yscale = log)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; u0 = qp, Δt)\n    logt = log.(t)\n    lines!(ax, logt, g; label=\"GALI_$(k)\")\n    if k == 2\n        lines!(ax, logt, 1 ./ t.^(2k-4); label=\"slope -$(2k-4)\")\n    else\n        lines!(ax, logt, 100 ./ t.^(2k-4); label=\"slope -$(2k-4)\")\n    end\nend\nylims!(ax, 1e-12, 2)\nfig And here is GALI of a continuous system with a chaotic orbit tr = trajectory(ds, 10000.0, ch; Δt)[1]\nfig, ax = scatter(tr[:,1], tr[:,3]; label=\"ch\", markersize=2, color = (Main.COLORS[1], 0.5))\naxislegend(ax)\n\nax = Axis(fig[1,2]; yscale = log)\nls = lyapunovspectrum(ds, 5000; Δt, u0 = ch)\nfor k in [2,3,4]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = ch, Δt)\n    lines!(t, exp.(-ex.*t); label=\"exp. k=$k\")\n    lines!(t, g; label=\"GALI_$(k)\")\nend\nylims!(ax, 1e-16, 1)\nfig"},{"id":260,"pagetitle":"Detecting & Categorizing Chaos","title":"Using GALI","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#Using-GALI","content":" Using GALI No-one in their right mind would try to fit power-laws in order to distinguish between chaotic and regular behavior, like the above examples. These were just proofs that the method works as expected. The most common usage of  $\\text{GALI}_k$  is to define a (sufficiently) small amount of time and a (sufficiently) small threshold and see whether  $\\text{GALI}_k$  stays below it, for a (sufficiently) big  $k$ . For example, we utilize parallel integration of  TangentDynamicalSystem  to compute  $GALI$  for many initial conditions and produce a color-coded map of regular and chaotic orbits of the standard map. The following is an example of advanced usage: using ChaosTools, CairoMakie\n# Initialize `TangentDynamicalSystem`\n@inbounds function standardmap_rule(x, par, n)\n    theta = x[1]; p = x[2]\n    p += par[1]*sin(theta)\n    theta += p\n    return mod2pi.(SVector(theta, p))\nend\n@inbounds standardmap_jacob(x, p, n) = SMatrix{2,2}(\n    1 + p[1]*cos(x[1]), p[1]*cos(x[1]), 1, 1\n)\nds = DeterministicIteratedMap(standardmap_rule, ones(2), [1.0])\ntands = TangentDynamicalSystem(ds; J = standardmap_jacob)\n# Collect initial conditions\ndens = 101\nθs = ps = range(0, stop = 2π, length = dens)\nics = vec(SVector{2, Float64}.(Iterators.product(θs, ps)))\n# Initialize as many systems as threads\nsystems = [deepcopy(tands) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, tands)\n# Perform threaded loop\nregularity = zeros(size(ics))\nThreads.@threads for i in eachindex(ics)\n    u0 = ics[i]\n    system = systems[Threads.threadid()]\n    reinit!(system, u0)\n    regularity[i] = gali(system, 500)[2][end]\nend\n# Visualize\nfig, ax, sc = scatter(ics; color = regularity)\nColorbar(fig[1,2], sc; label = \"regularity\")\nfig"},{"id":261,"pagetitle":"Detecting & Categorizing Chaos","title":"Predictability of a chaotic system","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#Predictability-of-a-chaotic-system","content":" Predictability of a chaotic system Even if a system is \"formally\" chaotic, it can still be in phases where it is partially predictable, because the correlation coefficient between nearby trajectories vanishes very slowly with time.  Wernecke, Sándor & Gros  have developed an algorithm that allows one to classify a dynamical system to one of three categories: strongly chaotic, partially predictable chaos or regular (called  laminar  in their paper). We have implemented their algorithm in the function  predictability . Note that we set up the implementation to always return regular behavior for negative Lyapunov exponent. You may want to override this for research purposes."},{"id":262,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.predictability","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#ChaosTools.predictability","content":" ChaosTools.predictability  —  Function predictability(ds::CoreDynamicalSystem; kwargs...) -> chaos_type, ν, C Determine whether  ds  displays strongly chaotic, partially-predictable chaotic or regular behaviour, using the method by Wernecke et al. described in [Wernecke2017] . Return the type of the behavior, the cross-distance scaling coefficient  ν  and the correlation coefficient  C . Typical values for  ν ,  C  and  chaos_type  are given in Table 2 of [Wernecke2017] : chaos_type ν C :SC 0 0 :PPC 0 1 :REG 1 1 If none of these conditions apply, the return value is  :IND  (for indeterminate). Keyword arguments Ttr = 200 : Extra transient time to evolve the system before sampling from  the trajectory. Should be  Int  for discrete systems. T_sample = 1e4 : Time to evolve the system for taking samples. Should be  Int  for discrete systems. n_samples = 500 : Number of samples to take for use in calculating statistics. λ_max = lyapunov(ds, 5000) : Value to use for largest Lyapunov exponent for finding the Lyapunov prediction time. If it is less than zero a regular result is returned immediately. d_tol = 1e-3 : tolerance distance to use for calculating Lyapunov prediction time. T_multiplier = 10 : Multiplier from the Lyapunov prediction time to the evaluation time. T_max = Inf : Maximum time at which to evaluate trajectory distance. If the internally  computed evaluation time is larger than  T_max , stop at  T_max  instead.   It is strongly recommended to manually set this! δ_range = 10.0 .^ (-9:-6) : Range of initial condition perturbation distances  to use to determine scaling  ν . ν_threshold = C_threshold = 0.5 : Thresholds for scaling coefficients (they become 0 or 1 if they are less or more than the threshold). Description The algorithm samples points from a trajectory of the system to be used as initial conditions. Each of these initial conditions is randomly perturbed by a distance  δ , and the trajectories for both the original and perturbed initial conditions are evolved up to the 'evaluation time'  T  (see below its definition). The average (over the samples) distance and cross-correlation coefficient of the state at time  T  is computed. This is repeated for a range of  δ  (defined by  δ_range ), and linear regression is used to determine how the distance and cross-correlation scale with  δ , allowing for identification of chaos type. The evaluation time  T  is calculated as  T = T_multiplier*Tλ , where the Lyapunov prediction time  Tλ = log(d_tol/δ)/λ_max . This may be very large if the  λ_max  is small, e.g. when the system is regular, so this internally computed time  T  can be overridden by a smaller  T_max  set by the user. Performance Notes For continuous systems, it is likely that the  maxiters  used by the integrators needs to be increased, e.g. to 1e9. This is part of the  diffeq  kwargs. In addition, be aware that this function does a  lot  of internal computations. It is operating in a different speed than e.g.  lyapunov . source"},{"id":263,"pagetitle":"Detecting & Categorizing Chaos","title":"Example Hénon Map","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#Example-Hénon-Map","content":" Example Hénon Map We will create something similar to figure 2 of the paper, but for the Hénon map. fig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"a\", ylabel = L\"x\")\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhe = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\nas = 0.8:0.01:1.225\nod = orbitdiagram(he, 1, 1, as; n = 2000, Ttr = 2000)\ncolors = Dict(:REG => \"blue\", :PPC => \"green\", :SC => \"red\")\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    chaos_type, ν, C = predictability(he; T_max = 400000, Ttr = 2000)\n    scatter!(ax, a .* ones(length(od[i])), od[i];\n    color = (colors[chaos_type], 0.05), markersize = 2)\nend\nax.title = \"predictability of Hénon map\"\nfig"},{"id":264,"pagetitle":"Detecting & Categorizing Chaos","title":"The 0-1 test for chaos","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#The-0-1-test-for-chaos","content":" The 0-1 test for chaos The methods mentioned in this page so far require a  DynamicalSystem  instance. But of course this is not always the case. The so-called \"0 to 1\" test for chaos, by Gottwald & Melbourne, takes as an input a timeseries and outputs a boolean  true  if the timeseries is chaotic or  false  if it is not. Notice that the method does have a lot of caveats, so you should read the review paper before using. Also, it doesn't work for noisy data."},{"id":265,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.testchaos01","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#ChaosTools.testchaos01","content":" ChaosTools.testchaos01  —  Function testchaos01(x::Vector [, cs, N0]) -> chaotic? Perform the so called \"0-1\" test for chaos introduced by Gottwald and Melbourne [Gottwald2016]  on the timeseries  x . Return  true  if  x  is chaotic,  false  otherwise. Description This method tests if the given timeseries is chaotic or not by transforming it into a two-dimensional diffusive process like so: \\[p_n = \\sum_{j=1}^{n}\\phi_j \\cos(j c),\\quad q_n = \\sum_{j=1}^{n}\\phi_j \\sin(j c)\\] If the timeseries is chaotic, the mean square displacement of the process grows as  sqrt(length(x)) , while it stays constant if the timeseries is regular. The implementation here computes  K , a coefficient measuring the growth of the mean square displacement, and simply checks if  K > 0.5 .  K  is the median of  $K_c$  over given  c , see the reference. If you want to access the various  Kc  you should call the method  testchaos01(x, c::Real, N0)  which returns  Kc . In fact, the high level method is just  median(testchaos01(x, c, N0) for c in cs) > 0.5 . cs  defaults to  3π/5*rand(100) + π/4  and  N0 , the length of the two-dimensional process, is  N0 = length(x)/10 . For data sampled from continuous dynamical systems, some care must be taken regarding the values of  cs . Also note that this method performs rather poorly with even the slight amount of noise, returning  true  for even small amounts of noise noisy timeseries. Some possibilities to alleviate this exist, but are context specific on the application. See  [Gottwald2016]  for more info. source"},{"id":266,"pagetitle":"Detecting & Categorizing Chaos","title":"Expansion entropy","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#Expansion-entropy","content":" Expansion entropy The expansion entropy is a quantity that is suggested by B. Hunt and E. Ott as a measure that can define chaos (so far no widely accepted definition of chaos exists). Positive expansion entropy means chaos."},{"id":267,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/chaos_detection/#ChaosTools.expansionentropy","content":" ChaosTools.expansionentropy  —  Function expansionentropy(ds::DynamicalSystem, sampler, isinside; kwargs...) Calculate the expansion entropy [Hunt2015]  of  ds , in the restraining region  $S$  by estimating the slope (via linear regression) of the curve  $\\log E_{t0+T, t0}(f, S)$  versus  $T$  (using  linear_region ). This is an approximation of the expansion entropy  $H_0$ , according to [Hunt2015] . Return  $T$ ,   $\\log E$  and the calculated slope. sampler  is a 0-argument function that generates a random initial conditions of  ds  and  isinside  is a 1-argument function that given a state it returns true if the state is inside the restraining region. Typically  sampler, isinside  are the output of  statespace_sampler . Keyword arguments N = 1000 : Number of samples taken at each batch (same as  $N$  of  [Hunt2015] ). steps = 40 : The maximal steps for which the system will be run. batches = 100 : Number of batches to run the calculation, see below. Δt = 1 : Time evolution step size. J = nothing : Jacobian function given to  TangentDynamicalSystem . Description N  samples are initialized and propagated forwards in time (along with their tangent space). At every time  $t$  in  [t0+Δt, t0+2Δt, ..., t0+steps*Δt]  we calculate  $H$ : \\[H[t] = \\log E_{t0+T, t0}(f, S),\\] with \\[E_{t0+T, t0}(f, S) = \\frac 1 N \\sum_{i'} G(Df_{t0+t, t0}(x_i))\\] (using same notation as  [Hunt2015] ). In principle  $E$  is the average largest possible growth ratio within the restraining region (sampled by the initial conditions). The summation is only over  $x_i$  that stay inside the region  $S$  defined by the boolean function  isinside . This process is done by the  ChaosTools.expansionentropy_sample  function. Then, this is repeated for  batches  amount of times, as recommended in [Hunt2015] . From all these batches, the mean and std of  $H$  is computed at every time point. This is done by the  expansionentropy_batch  function. When plotted versus  $t$ , these create the curves and error bars of e.g. Figs 2, 3 of [1]. This function  expansionentropy  simply returns the slope of the biggest linear region of the curve  $H$  versus  $t$ , which approximates the expansion entropy  $H_0$ . It is therefore  recommended  to use  expansionentropy_batch  directly and evaluate the result yourself, as this step is known to be inaccurate for non-chaotic systems (where  $H$  fluctuates strongly around 0). source Skokos2007 Skokos, C. H.  et al. , Physica D  231 , pp 30–54 (2007) Skokos2016b Skokos, C. H.  et al. ,  Chaos Detection and Predictability  - Chapter 5 (section 5.3.1 and ref. [85] therein), Lecture Notes in Physics  915 , Springer (2016) Wernecke2017 Wernecke, H., Sándor, B. & Gros, C.  How to test for partially predictable chaos .  Scientific Reports  7 , (2017) . Gottwald2016 Gottwald & Melbourne, “The 0-1 test for chaos: A review”  Lect. Notes Phys., vol. 915, pp. 221–247, 2016. Hunt2015 Hunt & Ott, ‘Defining Chaos’,  Chaos 25.9 (2015)"},{"id":270,"pagetitle":"Dimensionality reduction","title":"Dimensionality reduction","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/dimreduction/#Dimensionality-reduction","content":" Dimensionality reduction"},{"id":271,"pagetitle":"Dimensionality reduction","title":"Broomhead-King Coordinates","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/dimreduction/#Broomhead-King-Coordinates","content":" Broomhead-King Coordinates"},{"id":272,"pagetitle":"Dimensionality reduction","title":"ChaosTools.broomhead_king","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/dimreduction/#ChaosTools.broomhead_king","content":" ChaosTools.broomhead_king  —  Function broomhead_king(s::AbstractVector, d::Int) -> U, S, Vtr Return the Broomhead-King coordinates of a timeseries  s  by performing  svd  on high-dimensional delay embedding if  s  with dimension  d  with minimum delay. Description Broomhead and King coordinates is an approach proposed in  [Broomhead1987]  that applies the Karhunen–Loève theorem to delay coordinates embedding with smallest possible delay. The function performs singular value decomposition on the  d -dimensional matrix  $X$  of  $s$ , \\[X = \\frac{1}{\\sqrt{N}}\\left(\n\\begin{array}{cccc}\nx_1 & x_2 & \\ldots & x_d \\\\\nx_2 & x_3 & \\ldots & x_{d+1}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nx_{N-d+1} & x_{N-d+2} &\\ldots & x_N\n\\end{array}\n\\right) = U\\cdot S \\cdot V^{tr}.\\] where  $x := s - \\bar{s}$ . The columns of  $U$  can then be used as a new coordinate system, and by considering the values of the singular values  $S$  you can decide how many columns of  $U$  are \"important\". source This alternative/improvement of the traditional delay coordinates can be a very powerful tool. An example where it shines is noisy data where there is the effect of superficial dimensions due to noise. Take the following example where we produce noisy data from a system and then use Broomhead-King coordinates as an alternative to \"vanilla\" delay coordinates: using ChaosTools, CairoMakie\n\nfunction gissinger_rule(u, p, t)\n    μ, ν, Γ = p\n    du1 = μ*u[1] - u[2]*u[3]\n    du2 = -ν*u[2] + u[1]*u[3]\n    du3 = Γ - u[3] + u[1]*u[2]\n    return SVector{3}(du1, du2, du3)\nend\n\ngissinger = CoupledODEs(gissinger_rule, ones(3), [0.112, 0.1, 0.9])\nX, t = trajectory(gissinger, 1000.0; Ttr = 100, Δt = 0.1)\nx = X[:, 1]\n\nL = length(x)\ns = x .+ 0.5rand(L) #add noise\n\nU, S = broomhead_king(s, 20)\nsummary(U) \"9982×20 Matrix{Float64}\" Now let's simply compare the above result with the one you get from doing a standard delay coordinates embedding using DelayEmbeddings: embed, estimate_delay\n\nfig = Figure()\naxs = [Axis3(fig[1, i]) for i in 1:2]\nlines!(axs[1], U[:, 1], U[:, 2], U[:, 3])\naxs[1].title = \"Broomhead-King of s\"\n\nR = embed(s, 3, estimate_delay(x, \"mi_min\"))\nlines!(axs[2], columns(R)...)\naxs[2].title = \"2D embedding of s\"\nfig we have used the same system as in the  Delay Coordinates Embedding  example, and picked the optimal delay time of  τ = 30  (for same  Δt = 0.05 ). Regardless, the vanilla delay coordinates is much worse than the Broomhead-King coordinates."},{"id":273,"pagetitle":"Dimensionality reduction","title":"DyCA - Dynamical Component Analysis","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/dimreduction/#DyCA-Dynamical-Component-Analysis","content":" DyCA - Dynamical Component Analysis"},{"id":274,"pagetitle":"Dimensionality reduction","title":"ChaosTools.dyca","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/dimreduction/#ChaosTools.dyca","content":" ChaosTools.dyca  —  Function dyca(data, eig_threshold) -> eigenvalues, proj_mat, projected_data Compute the Dynamical Component analysis (DyCA) of the given  data [Uhl2018]  used for dimensionality reduction. Return the eigenvalues, projection matrix, and reduced-dimension data (which are just  data*proj_mat ). Keyword Arguments norm_eigenvectors=false : if true, normalize the eigenvectors Description Dynamical Component Analysis (DyCA) is a method to detect projection vectors to reduce the dimensionality of multi-variate, high-dimensional deterministic datasets. Unlike methods like PCA or ICA that make a stochasticity assumption, DyCA relies on a determinacy assumption on the time-series and is based on the solution of a generalized eigenvalue problem. After choosing an appropriate eigenvalue threshold and solving the eigenvalue problem, the obtained eigenvectors are used to project the high-dimensional dataset onto a lower dimension. The obtained eigenvalues measure the quality of the assumption of linear determinism for the investigated data. Furthermore, the number of the generalized eigenvalues with a value of approximately 1.0 are a measure of the number of linear equations contained in the dataset. This property is useful in detecting regions with highly deterministic parts in the time-series and also as a preprocessing step for reservoir computing of high-dimensional spatio-temporal data. The generalised eigenvalue problem we solve is: \\[C_1 C_0^{-1} C_1^{\\top} \\bar{u} = \\lambda C_2 \\bar{u}\n\\] where  $C_0$  is the correlation matrix of the data with itself,  $C_1$  the correlation matrix of the data with its derivative, and  $C_2$  the correlation matrix of the derivative of the data with itself. The eigenvectors  $\\bar{u}$  with eigenvalues approximately 1 and their  $C_1^{-1} C_2 u$  counterpart, form the space where the data is projected onto. source Broomhead1987 Broomhead, Jones, King, J. Phys. A  20 , 9, pp L563 (1987) Uhl2018 B Seifert, K Korn, S Hartmann, C Uhl,  Dynamical Component Analysis (DYCA): Dimensionality Reduction for High-Dimensional Deterministic Time-Series , 10.1109/mlsp.2018.8517024, 2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)"},{"id":277,"pagetitle":"Lyapunov Exponents","title":"Lyapunov Exponents","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Lyapunov-Exponents","content":" Lyapunov Exponents Lyapunov exponents measure exponential rates of separation of nearby trajectories in the flow of a dynamical system. The concept of these exponents is best explained in Chapter 3 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. The explanations of the chapter directly utilize the code of the functions in this page."},{"id":278,"pagetitle":"Lyapunov Exponents","title":"Lyapunov Spectrum","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Lyapunov-Spectrum","content":" Lyapunov Spectrum The function  lyapunovspectrum  calculates the entire spectrum of the Lyapunov exponents of a system:"},{"id":279,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.lyapunovspectrum","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#ChaosTools.lyapunovspectrum","content":" ChaosTools.lyapunovspectrum  —  Function lyapunovspectrum(ds::DynamicalSystem, N, k = dimension(ds); kwargs...) -> λs Calculate the spectrum of Lyapunov exponents  [Lyapunov1992]  of  ds  by applying a QR-decomposition on the parallelepiped defined by the deviation vectors, in total for  N  evolution steps. Return the spectrum sorted from maximum to minimum. The third argument  k  is optional, and dictates how many lyapunov exponents to calculate (defaults to  dimension(ds) ). See also  lyapunov ,  local_growth_rates . Note:  This function simply initializes a  TangentDynamicalSystem  and calls the method below. This means that the automatic Jacobian is used by default. Initialize manually a  TangentDynamicalSystem  if you have a hand-coded Jacobian. Keyword arguments u0 = current_state(ds) : State to start from. Ttr = 0 : Extra transient time to evolve the system before application of the algorithm. Should be  Int  for discrete systems. Both the system and the deviation vectors are evolved for this time. Δt = 1 : Time of individual evolutions between successive orthonormalization steps. For continuous systems this is approximate. show_progress = false : Display a progress bar of the process. Description The method we employ is \"H2\" of  [Geist1990] , originally stated in  [Benettin1980] , and explained in educational form in  [DatserisParlitz2022] . The deviation vectors defining a  D -dimensional parallelepiped in tangent space are evolved using the tangent dynamics of the system (see  TangentDynamicalSystem ). A QR-decomposition at each step yields the local growth rate for each dimension of the parallelepiped. At each step the parallelepiped is re-normalized to be orthonormal. The growth rates are then averaged over  N  successive steps, yielding the lyapunov exponent spectrum. source lyapunovspectrum(tands::TangentDynamicalSystem, N::Int; Ttr, Δt, show_progress) The low-level method that is called by  lyapunovspectrum(ds::DynamicalSystem, ...) . Use this method for looping over different initial conditions or parameters by calling  reinit!  to  tands . Also use this method if you have a hand-coded Jacobian to pass when creating  tands . source"},{"id":280,"pagetitle":"Lyapunov Exponents","title":"Example","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Example","content":" Example For example, the Lyapunov spectrum of the  folded towel map  is calculated as: using ChaosTools\nfunction towel_rule(x, p, n)\n    @inbounds x1, x2, x3 = x[1], x[2], x[3]\n    SVector( 3.8*x1*(1-x1) - 0.05*(x2+0.35)*(1-2*x3),\n    0.1*( (x2+0.35)*(1-2*x3) - 1 )*(1 - 1.9*x1),\n    3.78*x3*(1-x3)+0.2*x2 )\nend\nfunction towel_jacob(x, p, n)\n    row1 = SVector(3.8*(1 - 2x[1]), -0.05*(1-2x[3]), 0.1*(x[2] + 0.35))\n    row2 = SVector(-0.19((x[2] + 0.35)*(1-2x[3]) - 1),  0.1*(1-2x[3])*(1-1.9x[1]),  -0.2*(x[2] + 0.35)*(1-1.9x[1]))\n    row3 = SVector(0.0,  0.2,  3.78(1-2x[3]))\n    return vcat(row1', row2', row3')\nend\n\nds = DeterministicIteratedMap(towel_rule, [0.085, -0.121, 0.075], nothing)\ntands = TangentDynamicalSystem(ds; J = towel_jacob)\n\nλλ = lyapunovspectrum(tands, 10000) 3-element Vector{Float64}:\n  0.43224475747701546\n  0.3722615352085776\n -3.296655735650821 lyapunovspectrum  also works for continuous time systems and will auto-generate a Jacobian function if one is not give. For example, function lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nlor = CoupledODEs(lorenz_rule, fill(10.0, 3), [10, 32, 8/3])\nλλ = lyapunovspectrum(lor, 10000; Δt = 0.1) 3-element Vector{Float64}:\n   0.9888670182160189\n   0.003980822649737176\n -14.659449798405003 lyapunovspectrum  is also very fast: using BenchmarkTools\nds = DeterministicIteratedMap(towel_rule, [0.085, -0.121, 0.075], nothing)\ntands = TangentDynamicalSystem(ds; J = towel_jacob)\n\n@btime lyapunovspectrum($tands, 10000)   966.500 μs (10 allocations: 576 bytes) # on my laptop Here is an example of using  reinit!  to efficiently iterate over different parameter values, and parallelize via  Threads , to compute the exponents over a given parameter range. using ChaosTools, CairoMakie\n\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\nds = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\ntands = TangentDynamicalSystem(ds; J = henon_jacob)\n\nas = 0.8:0.005:1.225;\nλs = zeros(length(as), 2)\n\n# Since `DynamicalSystem`s are mutable, we need to copy to parallelize\nsystems = [deepcopy(tands) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, tands)\n\nThreads.@threads for i in eachindex(as)\n    system = systems[Threads.threadid()]\n    set_parameter!(system, 1, as[i])\n    λs[i, :] .= lyapunovspectrum(system, 10000; Ttr = 500)\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"a\", ylabel = L\"\\lambda\")\nfor j in 1:2\n    lines!(ax, as, λs[:, j])\nend\nfig"},{"id":281,"pagetitle":"Lyapunov Exponents","title":"Maximum Lyapunov Exponent","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Maximum-Lyapunov-Exponent","content":" Maximum Lyapunov Exponent It is possible to get only the maximum Lyapunov exponent simply by giving  1  as the third argument of  lyapunovspectrum . However, there is a second algorithm that calculates the maximum exponent:"},{"id":282,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.lyapunov","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#ChaosTools.lyapunov","content":" ChaosTools.lyapunov  —  Function lyapunov(ds::DynamicalSystem, Τ; kwargs...) -> λ Calculate the maximum Lyapunov exponent  λ  using a method due to Benettin  [Benettin1976] , which simply evolves two neighboring trajectories (one called \"given\" and one called \"test\") while constantly rescaling the test one. T   denotes the total time of evolution (should be  Int  for discrete time systems). See also  lyapunovspectrum ,  local_growth_rates . Keyword arguments show_progress = false : Display a progress bar of the process. u0 = initial_state(ds) : Initial condition. Ttr = 0 : Extra \"transient\" time to evolve the trajectories before starting to measure the exponent. Should be  Int  for discrete systems. d0 = 1e-9 : Initial & rescaling distance between the two neighboring trajectories. d0_lower = 1e-3*d0 : Lower distance threshold for rescaling. d0_upper = 1e+3*d0 : Upper distance threshold for rescaling. Δt = 1 : Time of evolution between each check rescaling of distance. For continuous time systems this is approximate. inittest = (u1, d0) -> u1 .+ d0/sqrt(length(u1)) : A function that given  (u1, d0)  initializes the test state with distance  d0  from the given state  u1   ( D  is the dimension of the system). This function can be used when you want to avoid the test state appearing in a region of the phase-space where it would have e.g. different energy or escape to infinity. Description Two neighboring trajectories with initial distance  d0  are evolved in time. At time  $t_i$  if their distance  $d(t_i)$  either exceeds the  d0_upper , or is lower than  d0_lower , the test trajectory is rescaled back to having distance  d0  from the reference one, while the rescaling keeps the difference vector along the maximal expansion/contraction direction:  $u_2 \\to u_1+(u_2−u_1)/(d(t_i)/d_0)$ . The maximum Lyapunov exponent is the average of the time-local Lyapunov exponents \\[\\lambda = \\frac{1}{t_{n} - t_0}\\sum_{i=1}^{n}\n\\ln\\left( a_i \\right),\\quad a_i = \\frac{d(t_{i})}{d_0}.\\] Performance notes This function simply initializes a  ParallelDynamicalSystem  and calls the method below. source lyapunov(pds::ParallelDynamicalSystem, T; Ttr, Δt, d0, d0_upper, d0_lower) The low-level method that is called by  lyapunov(ds::DynamicalSystem, ...) . Use this method for looping over different initial conditions or parameters by calling  reinit!  to  pds . source For example: using ChaosTools\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\nλ = lyapunov(henon, 10000; d0 = 1e-7, d0_upper = 1e-4, Ttr = 100) 0.42018736282059616"},{"id":283,"pagetitle":"Lyapunov Exponents","title":"Local Growth Rates","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Local-Growth-Rates","content":" Local Growth Rates"},{"id":284,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.local_growth_rates","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#ChaosTools.local_growth_rates","content":" ChaosTools.local_growth_rates  —  Function local_growth_rates(ds::DynamicalSystem, points::Dataset; kwargs...) → λlocal Compute the local exponential growth rate(s) of perturbations of the dynamical system  ds  for initial conditions given in  points . For each initial condition  u ∈ points ,  S  total perturbations are created and evolved exactly for time  Δt . The exponential local growth rate is defined simply by  log(g/g0)/Δt  with  g0  the initial perturbation size and  g  the size after  Δt . Thus,  λlocal  is a matrix of size  (length(points), S) . This function is a modification of  lyapunov . It uses the full nonlinear dynamics and a  ParallelDynamicalSystem  to evolve the perturbations, but does not do any re-scaling, thus allowing probing state and time dependence of perturbation growth. The actual growth is given by  exp(λlocal * Δt) . The output of this function is sometimes called \"Nonlinear Local Lyapunov Exponent\". Keyword arguments S = 100 Δt = 5 perturbation : If given, it should be a function  perturbation(ds, u, j)  that outputs a perturbation vector (preferrably  SVector ) given the system, current initial condition  u  and the counter  j ∈ 1:S . If not given, a random perturbation is generated with norm given by the keyword  e = 1e-6 . source Here is a simple example using the Henon map using ChaosTools\nusing Statistics, CairoMakie\n\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhe = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\npoints = trajectory(he, 2000; Ttr = 100)[1]\n\nλlocal = local_growth_rates(he, points; Δt = 1)\n\nλmeans = mean(λlocal; dims = 2)\nλstds = std(λlocal; dims = 2)\nx, y = columns(points)\nfig, ax, obj = scatter(x, y; color = vec(λmeans))\nColorbar(fig[1,2], obj)\nfig"},{"id":285,"pagetitle":"Lyapunov Exponents","title":"Lyapunov exponent from data","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Lyapunov-exponent-from-data","content":" Lyapunov exponent from data"},{"id":286,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.lyapunov_from_data","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#ChaosTools.lyapunov_from_data","content":" ChaosTools.lyapunov_from_data  —  Function lyapunov_from_data(R::Dataset, ks; kwargs...) For the given dataset  R , which is expected to represent a trajectory of a dynamical system, calculate and return  E(k) , which is the average logarithmic distance between states of a neighborhood that are evolved in time for  k  steps ( k  must be integer). The slope of  E  vs  k  approximates the maximum Lyapunov exponent. Typically  R  is the result of delay coordinates embedding of a timeseries (see DelayEmbeddings.jl). Keyword arguments refstates = 1:(length(R) - ks[end]) : Vector of indices that notes which states of the dataset should be used as \"reference states\", which means that the algorithm is applied for all state indices contained in  refstates . w::Int = 1 : The  Theiler window . ntype = NeighborNumber(1) : The neighborhood type. Either  NeighborNumber  or  WithinRange . See  Neighborhoods  for more info. distance = FirstElement() : Specifies what kind of distance function is used in the logarithmic distance of nearby states. Allowed distances values are  FirstElement()  or  Euclidean() , see below for more info. The metric for finding neighbors is always the Euclidean one. Description If the dataset exhibits exponential divergence of nearby states, then it should hold \\[E(k) \\approx \\lambda\\cdot k \\cdot \\Delta t + E(0)\\] for a  well defined region  in the  $k$  axis, where  $\\lambda$  is the approximated maximum Lyapunov exponent.  $\\Delta t$  is the time between samples in the original timeseries. You can use  linear_region  with arguments  (ks .* Δt, E)  to identify the slope (=  $\\lambda$ ) immediately, assuming you have chosen sufficiently good  ks  such that the linear scaling region is bigger than the saturated region. The algorithm used in this function is due to Parlitz [Skokos2016] , which itself expands upon Kantz [Kantz1994] . In sort, for each reference state a neighborhood is evaluated. Then, for each point in this neighborhood, the logarithmic distance between reference state and neighborhood state(s) is calculated as the \"time\" index  k  increases. The average of the above over all neighborhood states over all reference states is the returned result. If the  distance  is  Euclidean()  then use the Euclidean distance of the full  D -dimensional points (distance  $d_E$  in ref. [Skokos2016] ). If however the  distance  is  FirstElement() , calculate the absolute distance of  only the first elements  of the points of  R  (distance  $d_F$  in ref. [Skokos2016] , useful when  R  comes from delay embedding). source"},{"id":287,"pagetitle":"Lyapunov Exponents","title":"Neighborhood.NeighborNumber","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Neighborhood.NeighborNumber","content":" Neighborhood.NeighborNumber  —  Type NeighborNumber(k::Int) <: SearchType Search type representing the  k  nearest neighbors of the query (or approximate neighbors, depending on the search structure)."},{"id":288,"pagetitle":"Lyapunov Exponents","title":"Neighborhood.WithinRange","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/lyapunovs/#Neighborhood.WithinRange","content":" Neighborhood.WithinRange  —  Type WithinRange(r::Real) <: SearchType Search type representing all neighbors with distance  ≤ r  from the query (according to the search structure's metric). Let's apply the method to a timeseries from a continuous time system. In this case, one must be a bit more thoughtful when choosing parameters. The following example helps the users get familiar with the process: using ChaosTools, CairoMakie\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nds = CoupledODEs(lorenz_rule, fill(10.0, 3), [10, 32, 8/3])\n# create a timeseries of 1 dimension\nΔt = 0.05\nx = trajectory(ds, 1000.0; Ttr = 10, Δt)[1][:, 1] 20001-element Vector{Float64}:\n   4.080373146944597\n   4.240648211827648\n   4.995736623578444\n   6.360963428912875\n   8.36866320098022\n  10.871010893289172\n  13.195165389449697\n  14.068943434493336\n  12.64585168056817\n   9.674355464223451\n   ⋮\n  -7.766435939250359\n -10.779066585434036\n -13.875176556262863\n -15.33559998218722\n -13.671249922644769\n  -9.817896236698331\n  -5.976667861675601\n  -3.289232024597773\n  -1.7613819625138338 From prior knowledge of the system, we know we need to use  k  up to about  150 . However, due to the dense time sampling, we don't have to compute for every  k  in the range  0:150 . Instead, we can use ks = 0:4:150 0:4:148 Now we plot some example computations using delay embeddings to \"reconstruct\" the chaotic attractor using DelayEmbeddings: embed\nfig = Figure()\nax = Axis(fig[1,1]; xlabel=\"k (0.05×t)\", ylabel=\"E - E(0)\")\nntype = NeighborNumber(5) #5 nearest neighbors of each state\n\nfor d in [4, 8], τ in [7, 15]\n    r = embed(x, d, τ)\n\n    # E1 = lyapunov_from_data(r, ks1; ntype)\n    # λ1 = ChaosTools.linreg(ks1 .* Δt, E1)[2]\n    # plot(ks1,E1.-E1[1], label = \"dense, d=$(d), τ=$(τ), λ=$(round(λ1, 3))\")\n\n    E2 = lyapunov_from_data(r, ks; ntype)\n    λ2 = ChaosTools.linreg(ks .* Δt, E2)[2]\n    lines!(ks, E2.-E2[1]; label = \"d=$(d), τ=$(τ), λ=$(round(λ2, digits = 3))\")\nend\naxislegend(ax; position = :lt)\nax.title = \"Continuous Reconstruction Lyapunov\"\nfig As you can see, using  τ = 15  is not a great choice! The estimates with  τ = 7  though are very good (the actual value is around  λ ≈ 0.89... ). Notice that above a linear regression was done over the whole curves, which doesn't make sense. One should identify a linear scaling region and extract the slope of that one. The function  linear_region  from  FractalDimensions.jl  does this! Lyapunov1992 A. M. Lyapunov,  The General Problem of the Stability of Motion , Taylor & Francis (1992) Geist1990 K. Geist  et al. , Progr. Theor. Phys.  83 , pp 875 (1990) Benettin1980 G. Benettin  et al. , Meccanica  15 , pp 9-20 & 21-30 (1980) DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics Benettin1976 G. Benettin  et al. , Phys. Rev. A  14 , pp 2338 (1976) Skokos2016 Skokos, C. H.  et al. ,  Chaos Detection and Predictability  - Chapter 1 (section 1.3.2), Lecture Notes in Physics  915 , Springer (2016) Kantz1994 Kantz, H., Phys. Lett. A  185 , pp 77–87 (1994)"},{"id":291,"pagetitle":"Orbit diagrams","title":"Orbit diagrams","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/orbitdiagram/#Orbit-diagrams","content":" Orbit diagrams An orbit diagram is a way to visualize the asymptotic behaviour of a map, when a parameter of the system is changed. In practice an orbit diagram is a simple plot that plots the last  n  states of a dynamical system at a given parameter, repeated for all parameters in a range of interest. While this concept can apply to any kind of system, it makes most sense in discrete time dynamical systems. See Chapter 4 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022, for a more involved discussion on orbit diagrams for both discrete and continuous time systems."},{"id":292,"pagetitle":"Orbit diagrams","title":"ChaosTools.orbitdiagram","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/orbitdiagram/#ChaosTools.orbitdiagram","content":" ChaosTools.orbitdiagram  —  Function orbitdiagram(ds::DynamicalSystem, i, p_index, pvalues; kwargs...) → od Compute the orbit diagram (sometimes wrongly called bifurcation diagram) of the given dynamical system, saving the  i  variable(s) for parameter values  pvalues . The  p_index  specifies which parameter to change via  set_parameter!(ds, p_index, pvalue) . Works for any kind of  DynamicalSystem , although it mostly makes sense with one of  DeterministicIteratedMap, StroboscopicMap, PoincareMap . An orbit diagram is simply a collection of the last  n  states of  ds  as  ds  is evolved. This is done for each parameter value. i  can be  Int  or  AbstractVector{Int} . If  i  is  Int ,  od  is a vector of vectors. Else  od  is a vector of vectors of vectors. Each entry od  od  are the points at each parameter value, so that  length(od) == length(pvalues)  and  length(od[j]) == n, ∀ j . Keyword arguments n::Int = 100 : Amount of points to save for each parameter value. Δt = 1 : Stepping time between saving points. u0 = nothing : Specify an initial state. If  nothing , the previous state after each parameter is used to seed the new initial condition at the new parameter (with the very first state being the system's state). This makes convergence to the attractor faster, necessitating smaller  Ttr . Otherwise  u0  can be a standard state, or a vector of states, so that a specific state is used for each parameter. Ttr::Int = 10 : Each orbit is evolved for  Ttr  first before saving output. ulims = (-Inf, Inf) : only record system states within  ulims  (only valid if  i isa Int ). Iteration continues until  n  states fall within  ulims . show_progress = false : Display a progress bar (counting the parameter values). periods = nothing : Only valid if  ds isa StroboscopicMap . If given, it must be a a container with same layout as  pvalues . Provides a value for the  period  for each parameter value. Useful in case the orbit diagram is produced versus a driving frequency. source"},{"id":293,"pagetitle":"Orbit diagrams","title":"Deterministic iterated map","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/orbitdiagram/#Deterministic-iterated-map","content":" Deterministic iterated map For example, let's compute the famous orbit diagram of the logistic map: using ChaosTools, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nlogistic = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\n\ni = 1\nparameter = 1\npvalues = 2.5:0.004:4\nn = 2000\nTtr = 2000\noutput = orbitdiagram(logistic, i, parameter, pvalues; n, Ttr)\n\nL = length(pvalues)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= pvalues[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfig, ax = scatter(x, y; axis = (xlabel = L\"r\", ylabel = L\"x\"),\n    markersize = 0.8, color = (\"black\", 0.05),\n)\nax.title = \"Logistic map orbit diagram\"\nxlims!(ax, pvalues[1], pvalues[end]); ylims!(ax,0,1)\nfig"},{"id":294,"pagetitle":"Orbit diagrams","title":"Stroboscopic map","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/orbitdiagram/#Stroboscopic-map","content":" Stroboscopic map The beauty of  orbitdiagram  is that it can be directly applied to any kind of  DynamicalSystem . The most useful cases are the already seen  DeterministicIteratedMap , but also  PoincareMap  and  StroboscopicMap . Here is an example of the orbit diagram for the Duffing oscillator (making the same as Figure 9.2 of   Nonlinear Dynamics , Datseris & Parlitz, Springer 2022). using ChaosTools, CairoMakie\n\nfunction duffing_rule(u,p,t)\n    d, a, ω = p\n    du1 =  u[2]\n    du2 =  -u[1] - u[1]*u[1]*u[1] - d*u[2] + a*sin(ω*t)\n    return SVector(du1, du2)\nend\nT0 = 25.0\np0 = [0.1, 7, 2π/T0]\nu0 = [1.1, 1.1]\nds = CoupledODEs(duffing_rule, u0, p0)\nduffing = StroboscopicMap(ds, T0)\n\n# We want to change both the parameter `ω`, but also the\n# period of the stroboscopic map. `orbitdiagram` allows this!\nTrange = range(8, 26; length = 201)\nωrange = @. 2π / Trange\nn = 200\noutput = orbitdiagram(duffing, 1, 3, ωrange; n, u0, Ttr = 100, periods = Trange)\n\nL = length(Trange)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= Trange[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfig, ax = scatter(x, y; axis = (xlabel = L\"T\", ylabel = L\"u_1\"),\n    markersize = 8, color = (\"blue\", 0.25),\n)\nylims!(ax, -1, 1)\nfig Pro tip: to actually make Fig. 9.2 you'd have to do two modifications: first, pass  periods = Trange ./ 2 , so that points are recorded every half period. Then, at the very end, do  y[2:2:end] .= -y[2:2:end]  so that the symmetric orbits are recorded as well"},{"id":297,"pagetitle":"Fixed points & Periodicity","title":"Fixed points & Periodicity","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#Fixed-points-and-Periodicity","content":" Fixed points & Periodicity"},{"id":298,"pagetitle":"Fixed points & Periodicity","title":"Fixed points","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#Fixed-points","content":" Fixed points"},{"id":299,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.fixedpoints","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#ChaosTools.fixedpoints","content":" ChaosTools.fixedpoints  —  Function fixedpoints(ds::CoreDynamicalSystem, box, J = nothing; kwargs...) → fp, eigs, stable Return all fixed points  fp  of the given out-of-place  ds  (either  DeterministicIteratedMap  or  CoupledODEs ) that exist within the state space subset  box  for parameter configuration  p . Fixed points are returned as a  StateSpaceSet . For convenience, a vector of the Jacobian eigenvalues of each fixed point, and whether the fixed points are stable or not, are also returned. box  is an appropriate  IntervalBox  from IntervalRootFinding.jl. E.g. for a 3D system it would be something like v, z = -5..5, -2..2   # 1D intervals, can use `interval(-5, 5)` instead\nbox = v × v × z       # `\\times = ×`, or use `IntervalBox(v, v, z)` instead J  is the Jacobian of the dynamic rule of  ds . It is like in  TangentDynamicalSystem , however in this case automatic Jacobian estimation does not work, hence a hand-coded version must be given. Internally IntervalRootFinding.jl is used and as a result we are guaranteed to find all fixed points that exist in  box , regardless of stability. Since IntervalRootFinding.jl returns an interval containing a unique fixed point, we return the midpoint of the interval as the actual fixed point. Naturally, limitations inherent to IntervalRootFinding.jl apply here. The output of  fixedpoints  can be used in the  BifurcationKit.jl  as a start of a continuation process. See also  periodicorbits . Keyword arguments method = IntervalRootFinding.Krawczyk  configures the root finding method, see the docs of IntervalRootFinding.jl for all possibilities. tol = 1e-15  is the root-finding tolerance. warn = true  throw a warning if no fixed points are found. source A rather simple example of the fixed points can be demonstrated using E.g., the Lorenz-63 system, whose fixed points can be calculated analytically to be the following three \\[(0,0,0) \\\\\n\\left( \\sqrt{\\beta(\\rho-1)}, \\sqrt{\\beta(\\rho-1)}, \\rho-1 \\right) \\\\\n\\left( -\\sqrt{\\beta(\\rho-1)}, -\\sqrt{\\beta(\\rho-1)}, \\rho-1 \\right) \\\\\\] So, let's calculate using ChaosTools\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\nfunction lorenz_jacob(u, p, t)\n    σ, ρ, β = p\n    return SMatrix{3,3}(-σ, ρ - u[3], u[2], σ, -1, u[1], 0, -u[1], -β)\nend\n\nρ, β = 30.0, 10/3\nlorenz = CoupledODEs(lorenz_rule, 10ones(3), [10.0, ρ, β])\n# Define the box within which to find fixed points:\nx = y = interval(-20, 20)\nz = interval(0, 40)\nbox = x × y × z\n\nfp, eigs, stable = fixedpoints(lorenz, box, lorenz_jacob)\nfp 3-dimensional StateSpaceSet{Float64} with 3 points\n  9.83192       9.83192      29.0\n -9.83192      -9.83192      29.0\n  3.23647e-17   3.23647e-17   3.43473e-16 and compare this with the analytic ones: lorenzfp(ρ, β) = [\n    SVector(0, 0, 0.0),\n    SVector(sqrt(β*(ρ-1)), sqrt(β*(ρ-1)), ρ-1),\n    SVector(-sqrt(β*(ρ-1)), -sqrt(β*(ρ-1)), ρ-1),\n]\n\nlorenzfp(ρ, β) 3-element Vector{SVector{3, Float64}}:\n [0.0, 0.0, 0.0]\n [9.83192080250175, 9.83192080250175, 29.0]\n [-9.83192080250175, -9.83192080250175, 29.0]"},{"id":300,"pagetitle":"Fixed points & Periodicity","title":"Stable and Unstable Periodic Orbits of Maps","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#Stable-and-Unstable-Periodic-Orbits-of-Maps","content":" Stable and Unstable Periodic Orbits of Maps Chaotic behavior of low dimensional dynamical systems is affected by the position and the stability properties of the  periodic orbits  of a dynamical system. Finding unstable (or stable) periodic orbits of a discrete mapping analytically rapidly becomes impossible for higher orders of fixed points. Fortunately there is a numeric algorithm due to Schmelcher & Diakonos which allows such a computation. Notice that even though the algorithm can find stable fixed points, it is mainly aimed at  unstable  ones. The functions  periodicorbits  and  lambdamatrix  implement the algorithm:"},{"id":301,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.periodicorbits","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#ChaosTools.periodicorbits","content":" ChaosTools.periodicorbits  —  Function periodicorbits(ds::DeterministicIteratedMap,\n               o, ics [, λs, indss, singss]; kwargs...) -> FP Find fixed points  FP  of order  o  for the map  ds  using the algorithm due to Schmelcher & Diakonos [Schmelcher1997] .  ics  is a collection of initial conditions (container of vectors) to be evolved. Optional arguments The optional arguments  λs, indss, singss must be containers  of appropriate values, besides  λs  which can also be a number. The elements of those containers are passed to:  lambdamatrix(λ, inds, sings) , which creates the appropriate  $\\mathbf{\\Lambda}_k$  matrix. If these arguments are not given, a random permutation will be chosen for them, with  λ=0.001 . Keyword arguments maxiters::Int = 100000 : Maximum amount of iterations an i.c. will be iterated  before claiming it has not converged. disttol = 1e-10 : Distance tolerance. If the 2-norm of a previous state with  the next one is  ≤ disttol  then it has converged to a fixed point. inftol = 10.0 : If a state reaches  norm(state) ≥ inftol  it is assumed that  it has escaped to infinity (and is thus abandoned). roundtol::Int = 4 : The found fixed points are rounded  to  roundtol  digits before pushed into the list of returned fixed points  FP ,   if  they are not already contained in  FP .  This is done so that  FP  doesn't contain duplicate fixed points (notice  that this has nothing to do with  disttol ). Description The algorithm used can detect periodic orbits by turning fixed points of the original map  ds  to stable ones, through the transformation \\[\\mathbf{x}_{n+1} = \\mathbf{x}_n +\n\\mathbf{\\Lambda}_k\\left(f^{(o)}(\\mathbf{x}_n) - \\mathbf{x}_n\\right)\\] The index  $k$  counts the various possible  $\\mathbf{\\Lambda}_k$ . Performance notes All  initial conditions are evolved for  all $\\mathbf{\\Lambda}_k$  which can very quickly lead to long computation times. source"},{"id":302,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.lambdamatrix","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#ChaosTools.lambdamatrix","content":" ChaosTools.lambdamatrix  —  Function lambdamatrix(λ, inds::Vector{Int}, sings) -> Λk Return the matrix  $\\mathbf{\\Lambda}_k$  used to create a new dynamical system with some unstable fixed points turned to stable in the function  periodicorbits . Arguments λ<:Real  : the multiplier of the  $C_k$  matrix, with  0<λ<1 . inds::Vector{Int}  : The  i th entry of this vector gives the  row  of the nonzero element of the  i th column of  $C_k$ . sings::Vector{<:Real}  : The element of the  i th column of  $C_k$  is +1 if  signs[i] > 0  and -1 otherwise ( sings  can also be  Bool  vector). Calling  lambdamatrix(λ, D::Int)  creates a random  $\\mathbf{\\Lambda}_k$  by randomly generating an  inds  and a  signs  from all possible combinations. The  collections  of all these combinations can be obtained from the function  lambdaperms . Description Each element of  inds must be unique  such that the resulting matrix is orthogonal and represents the group of special reflections and permutations. Deciding the appropriate values for  λ, inds, sings  is not trivial. However, in ref. [Pingel2000]  there is a lot of information that can help with that decision. Also, by appropriately choosing various values for  λ , one can sort periodic orbits from e.g. least unstable to most unstable, see [Diakonos1998]  for details. source"},{"id":303,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.lambdaperms","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#ChaosTools.lambdaperms","content":" ChaosTools.lambdaperms  —  Function lambdaperms(D) -> indperms, singperms Return two collections that each contain all possible combinations of indices (total of  $D!$ ) and signs (total of  $2^D$ ) for dimension  D  (see  lambdamatrix ). source"},{"id":304,"pagetitle":"Fixed points & Periodicity","title":"Standard Map example","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#Standard-Map-example","content":" Standard Map example For example, let's find the fixed points of the  Systems.standardmap  of order 2, 3, 4, 5, 6 and 8. We will use all permutations for the  signs  but only one for the  inds . We will also only use one  λ  value, and a 21×21 density of initial conditions. First, initialize everything using ChaosTools\n\nfunction standardmap_rule(x, k, n)\n    theta = x[1]; p = x[2]\n    p += k[1]*sin(theta)\n    theta += p\n    return SVector(mod2pi(theta), mod2pi(p))\nend\n\nstandardmap = DeterministicIteratedMap(standardmap_rule, rand(2), [1.0])\nxs = range(0, stop = 2π, length = 11); ys = copy(xs)\nics = [SVector{2}(x,y) for x in xs for y in ys]\n\n# All permutations of [±1, ±1]:\nsingss = lambdaperms(2)[2] # second entry are the signs\n\n# I know from personal research I only need this `inds`:\nindss = [[1,2]] # <- must be container of vectors!\n\nλs = 0.005 # <- only this allowed to not be vector (could also be vector)\n\norders = [2, 3, 4, 5, 6, 8]\nALLFP = Dataset{2, Float64}[]\n\nstandardmap 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  standardmap_rule\n parameters:    [1.0]\n time:          0\n state:         [0.2786210281505589, 0.5087967151139846]\n Then, do the necessary computations for all orders for o in orders\n    FP = periodicorbits(standardmap, o, ics, λs, indss, singss)\n    push!(ALLFP, FP)\nend Plot the phase space of the standard map using CairoMakie\niters = 1000\ndataset = trajectory(standardmap, iters)[1]\nfor x in xs\n    for y in ys\n        append!(dataset, trajectory(standardmap, iters, [x, y])[1])\n    end\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"\\theta\", ylabel = L\"p\",\n    limits = ((xs[1],xs[end]), (xs[1],xs[end]))\n)\nscatter!(ax, dataset[:, 1], dataset[:, 2]; markersize = 1, color = \"black\")\nfig and finally, plot the fixed points markers = [:diamond, :utriangle, :rect, :pentagon, :hexagon, :circle]\n\nfor i in 1:6\n    FP = ALLFP[i]\n    o = orders[i]\n    scatter!(ax, columns(FP)...; marker=markers[i], color = Cycled(i),\n        markersize = 30 - 2i, strokecolor = \"grey\", strokewidth = 1, label = \"order $o\"\n    )\nend\naxislegend(ax)\nfig Okay, this output is great, and we can tell that it is correct because: Fixed points of order  $n$  are also fixed points of order  $2n, 3n, 4n, ...$ Besides fixed points of previous orders,  original  fixed points of order  $n$  come in (possible multiples of)  $2n$ -sized pairs (see e.g. order 5). This is a direct consequence of the Poincaré–Birkhoff theorem."},{"id":305,"pagetitle":"Fixed points & Periodicity","title":"Estimating the Period","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#Estimating-the-Period","content":" Estimating the Period The function  estimate_period  offers ways for estimating the period (either exact for periodic timeseries, or approximate for near-periodic ones) of a given timeseries. We offer five methods to estimate periods, some of which work on evenly sampled data only, and others which accept any data. The figure below summarizes this: "},{"id":306,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.estimate_period","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#ChaosTools.estimate_period","content":" ChaosTools.estimate_period  —  Function estimate_period(v::Vector, method, t=0:length(v)-1; kwargs...) Estimate the period of the signal  v , with accompanying time vector  t , using the given  method . If  t  is an AbstractArray, then it is iterated through to ensure that it's evenly sampled (if necessary for the algorithm).  To avoid this, you can pass any  AbstractRange , like a  UnitRange  or a  LinRange , which are defined to be evenly sampled. Methods requiring evenly sampled data These methods are faster, but some are error-prone. :periodogram  or  :pg : Use the fast Fourier transform to compute a  periodogram (power-spectrum) of the given data.  Data must be evenly sampled. :multitaper  or  mt : The multitaper method reduces estimation bias by using multiple independent estimates from the same sample. Data tapers are then windowed and the power spectra are obtained.  Available keywords follow:  nw  is the time-bandwidth product, and  ntapers  is the number of tapers. If  window  is not specified, the signal is tapered with  ntapers  discrete prolate spheroidal sequences with time-bandwidth product  nw . Each sequence is equally weighted; adaptive multitaper is not (yet) supported. If  window  is specified, each column is applied as a taper. The sum of periodograms is normalized by the total sum of squares of  window . :autocorrelation  or  :ac : Use the autocorrelation function (AC). The value where the AC first comes back close to 1 is the period of the signal. The keyword  L = length(v)÷10  denotes the length of the AC (thus, given the default setting, this method will fail if there less than 10 periods in the signal). The keyword  ϵ = 0.2  ( \\epsilon ) means that  1-ϵ  counts as \"1\" for the AC. :yin : The YIN algorithm. An autocorrelation-based method to estimate the fundamental period of the signal. See the original paper  [CheveigneYIN2002]  or the implementation  yin . Sampling rate is taken as  sr = 1/mean(diff(t))  if not given. speech and music. The Journal of the Acoustical Society of America, 111(4), 1917-1930. Methods not requiring evenly sampled data These methods tend to be slow, but versatile and low-error. :lombscargle  or  :ls : Use the Lomb-Scargle algorithm to compute a periodogram.  The advantage of the Lomb-Scargle method is that it does not require an equally sampled dataset and performs well on undersampled datasets. Constraints have been set on the period, since Lomb-Scargle tends to have false peaks at very low frequencies.  That being said, it's a very flexible method.  It is extremely customizable, and the keyword arguments that can be passed to it are given  in the documentation . :zerocrossing  or  :zc : Find the zero crossings of the data, and use the average difference between zero crossings as the period.  This is a naïve implementation, with only linear interpolation; however, it's useful as a sanity check.  The keyword  line  controls where the \"crossing point\" is. It defaults to  mean(v) . For more information on the periodogram methods, see the documentation of DSP.jl and LombScargle.jl. source"},{"id":307,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.yin","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#ChaosTools.yin","content":" ChaosTools.yin  —  Function yin(sig::Vector, sr::Int; kwargs...) -> F0s, frame_times Estimate the fundamental frequency (F0) of the signal  sig  using the YIN algorithm  [1] . The signal  sig  is a vector of points uniformly sampled at a rate  sr . Keyword arguments w_len : size of the analysis window [samples == number of points] f_step : size of the lag between two consecutive frames [samples == number of points] f0_min : Minimum fundamental frequency that can be detected [linear frequency] f0_max : Maximum fundamental frequency that can be detected [linear frequency] harmonic_threshold : Threshold of detection. The algorithm returns the first minimum of the CMNDF function below this threshold. diffference_function : The difference function to be used (by default  ChaosTools.difference_function_original ). Description The YIN algorithm  [CheveigneYIN2002]  estimates the signal's fundamental frequency  F0  by basically looking for the period  τ0   which minimizes the signal's autocorrelation. This autocorrelation is calculated for signal segments (frames), composed of two windows of length  w_len . Each window is separated by a distance  τ , and the idea is that the distance which minimizes the pairwise difference between each window is considered to be the fundamental period  τ0  of that frame. More precisely, the algorithm first computes the cumulative mean normalized difference function (MNDF) between two windows of a frame for several candidate periods  τ  ranging from  τ_min=sr/f0_max  to  τ_max=sr/f0_min . The MNDF is defined as \\[d_t^\\prime(\\tau) = \\begin{cases}\n        1 & \\text{if} ~ \\tau=0 \\\\\n        d_t(\\tau)/\\left[{(\\frac 1 \\tau) \\sum_{j=1}^{\\tau} d_{t}(j)}\\right] & \\text{otherwise}\n        \\end{cases}\\] where  d_t  is the difference function: \\[d_t(\\tau) = \\sum_{j=1}^W (x_j - x_{j+\\tau})^2\\] It then refines the local minima of the MNDF using parabolic (quadratic) interpolation. This is done by taking each minima, along with their first neighbor points, and finding the minimum of the corresponding interpolated parabola. The MNDF minima are substituted by the interpolation minima. Finally, the algorithm chooses the minimum with the smallest period and with a corresponding MNDF below the  harmonic threshold . If this doesn't exist, it chooses the period corresponding to the global minimum. It repeats this for frames starting at the first signal point, and separated by a distance  f_step  (frames can overlap), and returns the vector of frequencies  F0=sr/τ0  for each frame, along with the start times of each frame. As a note, the physical unit of the frequency is 1/[time], where [time] is decided by the sampling rate  sr . If, for instance, the sampling rate is over seconds, then the frequency is in Hertz. speech and music. The Journal of the Acoustical Society of America, 111(4), 1917-1930. source"},{"id":308,"pagetitle":"Fixed points & Periodicity","title":"Example","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/periodicity/#Example","content":" Example Here we will use a modified FitzHugh-Nagumo system that results in periodic behavior, and then try to estimate its period. First, let's see the trajectory: using ChaosTools, CairoMakie\n\nfunction FHN(u, p, t)\n    e, b, g = p\n    v, w = u\n    dv = min(max(-2 - v, v), 2 - v) - w\n    dw = e*(v - g*w + b)\n    return SVector(dv, dw)\nend\n\ng, e, b  = 0.8, 0.04, 0.0\np0 = [e, b, g]\n\nfhn = CoupledODEs(FHN, SVector(-2, -0.6667), p0)\nT, Δt = 1000.0, 0.1\nX, t = trajectory(fhn, T; Δt)\nv = X[:, 1]\n\nlines(t, v) Examining the figure, one can see that the period of the system is around  91  time units. To estimate it numerically let's use some of the methods: estimate_period(v, :autocorrelation, t) 91.0 estimate_period(v, :periodogram, t) 91.62720091627202 estimate_period(v, :zerocrossing, t) 91.08000000000001 estimate_period(v, :yin, t; f0_min=0.01) 91.07348421171405 Schmelcher1997 P. Schmelcher & F. K. Diakonos, Phys. Rev. Lett.  78 , pp 4733 (1997) Pingel2000 D. Pingel  et al. , Phys. Rev. E  62 , pp 2119 (2000) Diakonos1998 F. K. Diakonos  et al. , Phys. Rev. Lett.  81 , pp 4349 (1998) CheveigneYIN2002 De Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency estimator for CheveigneYIN2002 De Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency estimator for"},{"id":311,"pagetitle":"Rare events","title":"Rare events","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/rareevents/#Rare-events","content":" Rare events"},{"id":312,"pagetitle":"Rare events","title":"Return time statistics","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/rareevents/#Return-time-statistics","content":" Return time statistics"},{"id":313,"pagetitle":"Rare events","title":"ChaosTools.mean_return_times","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/rareevents/#ChaosTools.mean_return_times","content":" ChaosTools.mean_return_times  —  Function mean_return_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → τ, c Return the mean return times  τ , as well as the amount of returns  c , for subsets of the state space of  ds  defined by  u₀, εs . The  ds  is evolved for a maximum of  T  time. This function is a convenience wrapper around calls to  exit_entry_times  and then to  transit_return  and then some averaging. Thus see  exit_entry_times  for the meaning of  u₀  and  εs  and further info. source"},{"id":314,"pagetitle":"Rare events","title":"ChaosTools.exit_entry_times","ref":"/DynamicalSystemsDocs.jl/chaostools/stable/rareevents/#ChaosTools.exit_entry_times","content":" ChaosTools.exit_entry_times  —  Function exit_entry_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → exits, entries Collect exit and entry times for balls or boxes centered at  u₀  with radii  εs , in the state space of the given dynamical system. Return the exit and (re-)entry return times to the set(s), where each of these is a vector containing all collected times for the respective  ε -radius set, for  ε ∈ εs . The dynamical system is evolved up to  T  total time. Use  transit_return_times(exits, entries)  to transform the output into transit and return times, and see also  mean_return_times . The keyword  show_progress  displays a progress bar. It is  false  for discrete and  true  for continuous systems by default. Description Transit and return time statistics are important for the transport properties of dynamical systems [Meiss1997]  and can be connected with fractal dimensions of chaotic sets [Boev2014] . The current algorithm collects exit and re-entry times to given sets in the state space, which are centered at the state  u₀ .  The system evolution always starts from  u₀  and the initial state of  ds  is irrelevant.  εs  is always a  Vector . Specification of sets to return to If each entry of  εs  is a real number, then sets around  u₀  are nested hyper-spheres of radius  ε ∈ εs . The sets can also be hyper-rectangles (boxes), if each entry of  εs  is a vector itself. Then, the  i -th box is defined by the space covered by  u0 .± εs[i]  (thus the actual box size is  2εs[i] !). In the future, state space sets will be specified more conveniently and a single argument  sets  will be given instead of  u₀, εs . The reason to input multiple  εs  at once is purely for performance optimization (much faster than doing each  ε  individually). Discrete time systems For discrete systems, exit time is recorded immediately after exiting of the set, and re-entry is recorded immediately on re-entry. This means that if an orbit needs 1 step to leave the set and then it re-enters immediately on the next step, the return time is 1. Continuous time systems For continuous systems, a steppable integrator supporting interpolation is used. The way to specify how to estimate exit and entry times is via the keyword  crossing_method  whose values can be: CrossingLinearIntersection() : Linear interpolation is used between integrator steps and the intersection between lines and spheres is used to find the crossing times. CrossingAccurateInterpolation(; abstol=1e-12, reltol=1e-6) : Extremely accurate high order interpolation is used between integrator steps. First, a minimization with Optim.jl finds the minimum distance of the trajectory to the set center. Then, Roots.jl is used to find the exact crossing point. The tolerances are given to both procedures. Clearly,  CrossingAccurateInterpolation  is much more accurate than  CrossingLinearIntersection , but also much slower. However, the smaller the steps the integrator takes (in case some very high accuracy solver is used), the closer the linear intersection gets to the accurate version. Benchmarks are advised for the individual specific case the algorithm is applied at, in order to choose the best method. The keyword  threshold_distance = Inf  provides a means to skip the interpolation check, if the current state of the integrator is too far from the set center. If the distance of the current state of the integrator is  threshold_distance  or more distance away from the set center, attempts to interpolate are skipped. By default  threshold_distance = Inf  and hence this never happens. Typically you'd want this to be 10-100 times the distance the trajectory covers at an average integrator step. source Meiss1997 Meiss, J. D.  Average exit time for volume-preserving maps ,  Chaos (1997) Boev2014 Boev, Vadivasova, & Anishchenko,  Poincaré recurrence statistics as an indicator of chaos synchronization ,  Chaos (2014)"},{"id":319,"pagetitle":"Attractors.jl","title":"Attractors.jl","ref":"/DynamicalSystemsDocs.jl/attractors/stable/#Attractors.jl","content":" Attractors.jl"},{"id":320,"pagetitle":"Attractors.jl","title":"Attractors","ref":"/DynamicalSystemsDocs.jl/attractors/stable/#Attractors","content":" Attractors  —  Module Attractors.jl A Julia module for finding attractors of arbitrary dynamical systems finding their basins of attraction or the state space fractions of the basins analyzing global stability of attractors (also called non-local stability or  resilience) \"continuing\" the attractors and their basins over a parameter range finding the basin boundaries and analyzing their fractal properties tipping points related functionality for systems with known dynamic rule and more! It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"Attractors\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, Attractors.jl was part of ChaosTools.jl source"},{"id":321,"pagetitle":"Attractors.jl","title":"Latest news","ref":"/DynamicalSystemsDocs.jl/attractors/stable/#Latest-news","content":" Latest news New functions  test_wada_merge  and  convergence_and_basins_fractions !"},{"id":322,"pagetitle":"Attractors.jl","title":"Outline of Attractors.jl","ref":"/DynamicalSystemsDocs.jl/attractors/stable/#Outline-of-Attractors.jl","content":" Outline of Attractors.jl First be sure that you are aware of what is a  DynamicalSystem  in the context of the  DynamicalSystems.jl  library. The best way to learn this is by following the  main tutorial  of the library. This is the input to the whole infrastructure of Attractors.jl. The bulk of the work in Attractors.jl is done by the  AttractorMapper  type, that instructs how to find attractors and maps initial conditions to them. It can be used in functions like  basins_fractions . For grouping features, there is a sub-infrastructure for instructing how to group features, which is governed by  GroupingConfig . The infrastructure of finding attractors and their basins fractions is then integrated into a brand new way of doing bifurcation analysis in the  continuation  function. See  Examples for Attractors.jl  for several applications in real world cases."},{"id":325,"pagetitle":"Finding Attractors","title":"Finding Attractors","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Finding-Attractors","content":" Finding Attractors Attractors.jl defines a generic interface for finding attractors of dynamical systems. One first decides the instance of  DynamicalSystem  they need. Then, an instance of  AttractorMapper  is created from this dynamical system. This  mapper  instance can be used to compute e.g.,  basins_of_attraction , and the output can be further analyzed to get e.g., the  basin_entropy ."},{"id":326,"pagetitle":"Finding Attractors","title":"Attractors.AttractorMapper","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.AttractorMapper","content":" Attractors.AttractorMapper  —  Type AttractorMapper(ds::DynamicalSystem, args...; kwargs...) → mapper Subtypes of  AttractorMapper  are structures that map initial conditions of  ds  to attractors. Currently available mapping methods: AttractorsViaProximity AttractorsViaRecurrences AttractorsViaFeaturizing All  AttractorMapper  subtypes can be used with  basins_fractions  or  basins_of_attraction . In addition, some mappers can be called as a function of an initial condition: label = mapper(u0) and this will on the fly compute and return the label of the attractor  u0  converges at. The mappers that can do this are: AttractorsViaProximity AttractorsViaRecurrences AttractorsViaFeaturizing  with the  GroupViaHistogram  configuration. source"},{"id":327,"pagetitle":"Finding Attractors","title":"Recurrences","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Recurrences","content":" Recurrences"},{"id":328,"pagetitle":"Finding Attractors","title":"Attractors.AttractorsViaRecurrences","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.AttractorsViaRecurrences","content":" Attractors.AttractorsViaRecurrences  —  Type AttractorsViaRecurrences(ds::DynamicalSystem, grid; kwargs...) Map initial conditions of  ds  to attractors by identifying attractors on the fly based on recurrences in the state space, as outlined in ( Datseris and Wagemakers, 2022 ). However, the Description section below for has a more accurate (and simpler) exposition to the algorithm than the paper. grid  is instructions for partitioning the state space into finite-sized cells so that a finite state machine can operate on top of it. Possibilities are: A tuple of sorted  AbstractRange s for a regular grid. Example is  grid = (xg, yg)  where  xg = yg = range(-5, 5; length = 100)    for a two-dimensional system. A tuple of sorted  AbstractVector s for an irregular grid, for example grid = (xg, yg)  with  xg = range(0, 10.0^(1/2); length = 200).^2,   yg = range(-5, 5; length = 100) . An instance of the special grid type SubdivisionBasedGrid , which can be created either manually or by using    subdivision_based_grid .   This automatically analyzes and adapts grid discretization   levels in accordance with state space flow speed in different regions. The grid has to be the same dimensionality as the state space, use a  ProjectedDynamicalSystem  if you want to search for attractors in a lower dimensional subspace. Keyword arguments sparse = true : control the storage type of the state space grid. If true,  uses a sparse array, whose memory usage is in general more efficient than a regular  array obtained with  sparse=false . In practice, the sparse representation should  always be preferred when searching for  basins_fractions . Only for very low  dimensional systems and for computing the full  basins_of_attraction  the  non-sparse version should be used. Time evolution configuration Ttr = 0 : Skip a transient before the recurrence routine begins. Δt : Approximate integration time step (second argument of the  step!  function). The keyword  Dt  can also be used instead if  Δ  ( \\Delta ) is not accessible. It is  1  for discrete time systems. For continuous systems, an automatic value is calculated using  automatic_Δt_basins . For very fine grids, this can become very small, much smaller than the typical integrator internal step size in case of adaptive integrators. In such cases, use  force_non_adaptive = true . force_non_adaptive = false : Only used if the input dynamical system is  CoupledODEs . If  true  the additional keywords  adaptive = false, dt = Δt  are given as  diffeq  to the  CoupledODEs . This means that adaptive integration is turned off and  Δt  is used as the ODE integrator timestep. This is useful in (1) very fine grids, and (2) if some of the attractors are limit cycles. We have noticed that in this case the integrator timestep becomes commensurate with the limit cycle period, leading to incorrectly counting the limit cycle as more than one attractor. Finite state machine configuration consecutive_recurrences = 100 : Number of consecutive visits to previously visited unlabeled cells (i.e., recurrences) required before declaring we have converged to a new attractor. This number tunes the accuracy of converging to attractors and should generally be high (and even higher for chaotic systems). attractor_locate_steps = 1000 : Number of subsequent steps taken to locate accurately the new attractor after the convergence phase is over. Once  attractor_locate_steps  steps have been taken, the new attractor has been identified with sufficient accuracy and iteration stops. This number can be very high without much impact to overall performance. store_once_per_cell = true : Control if multiple points in state space that belong to the same cell are stored or not in the attractor, when a new attractor is found. If  true , each visited cell will only store a point once, which is desirable for fixed points and limit cycles. If  false  then  attractor_locate_steps  points are stored per attractor, leading to more densely stored attractors, which may be desirable for instance in chaotic attractors. consecutive_attractor_steps = 2 : Μaximum checks of consecutives hits of an existing attractor cell before declaring convergence to that existing attractor. consecutive_basin_steps = 10 : Number of consecutive visits of the same basin of attraction required before declaring convergence to an existing attractor. This is ignored if  sparse = true , as basins are not stored internally in that case. consecutive_lost_steps = 20 : Maximum check of iterations outside the defined grid before we declare the orbit lost outside and hence assign it label  -1 . horizon_limit = 1e6 : If the norm of the integrator state reaches this limit we declare that the orbit diverged to infinity. maximum_iterations = Int(1e6) : A safety counter that is always increasing for each initial condition. Once exceeded, the algorithm assigns  -1  and throws a warning. This clause exists to stop the algorithm never halting for inappropriate grids. It may happen when a newly found attractor orbit intersects in the same cell of a previously found attractor (which leads to infinite resetting of all counters). Description An initial condition given to an instance of  AttractorsViaRecurrences  is iterated based on the integrator corresponding to  ds . Enough recurrences in the state space (i.e., a trajectory visited a region it has visited before) means that the trajectory has converged to an attractor. This is the basis for finding attractors. A finite state machine (FSM) follows the trajectory in the state space, and constantly maps it to a cell in the given  grid . The grid cells store information: they are empty, visited, basins, or attractor cells. The state of the FSM is decided based on the cell type and the previous state of the FSM. Whenever the FSM recurs its state, its internal counter is increased, otherwise it is reset to 0. Once the internal counter reaches a threshold, the FSM terminates or changes its state. The possibilities for termination are the following: The trajectory hits  consecutive_recurrences  times in a row previously visited cells:  it is considered that an attractor is found and is labelled with a new ID. Then,  iteration continues for  attractor_locate_steps  steps. Each cell visited in this period stores  the \"attractor\" information. Then iteration terminates and the initial condition is  numbered with the attractor's ID. The trajectory hits an already identified attractor  consecutive_attractor_steps  consecutive times:  the initial condition is numbered with the attractor's basin ID. The trajectory hits a known basin  consecutive_basin_steps  times in a row: the initial condition  belongs to that basin and is numbered accordingly. Notice that basins are stored and  used only when  sparse = false  otherwise this clause is ignored. The trajectory spends  consecutive_lost_steps  steps outside the defined grid or the norm  of the dynamical system state becomes > than  horizon_limit : the initial  condition is labelled  -1 . If none of the above happens, the initial condition is labelled  -1  after   maximum_iterations  steps. There are some special internal optimizations and details that we do not describe here but can be found in comments in the source code. (E.g., a special timer exists for the \"lost\" state which does not interrupt the main timer of the FSM.) A video illustrating how the algorithm works can be found in the online Examples page. source"},{"id":329,"pagetitle":"Finding Attractors","title":"Attractors.automatic_Δt_basins","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.automatic_Δt_basins","content":" Attractors.automatic_Δt_basins  —  Function automatic_Δt_basins(ds::DynamicalSystem, grid; N = 5000) → Δt Calculate an optimal  Δt  value for  basins_of_attraction . This is done by evaluating the dynamic rule  f  (vector field) at  N  randomly chosen points within the bounding box of the grid. The average  f  is then compared with the average diagonal length of a grid cell and their ratio provides  Δt . Notice that  Δt  should not be too small which happens typically if the grid resolution is high. It is okay if the trajectory skips a few cells. Also,  Δt  that is smaller than the internal step size of the integrator will cause a performance drop. source"},{"id":330,"pagetitle":"Finding Attractors","title":"Attractors.SubdivisionBasedGrid","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.SubdivisionBasedGrid","content":" Attractors.SubdivisionBasedGrid  —  Type SubdivisionBasedGrid(grid::NTuple{D, <:AbstractRange}, lvl_array::Array{Int, D}) Given a coarse  grid  tesselating the state space, construct a  SubdivisionBasedGrid  based on the given level array  lvl_array  that should have the same dimension as  grid . The level array has non-negative integer values, with 0 meaning that the corresponding cell of the coarse  grid  should not be subdivided any further. Value  n > 0  means that the corresponding cell will be subdivided in total  2^n  times (along each dimension), resulting in finer cells within the original coarse cell. source"},{"id":331,"pagetitle":"Finding Attractors","title":"Attractors.subdivision_based_grid","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.subdivision_based_grid","content":" Attractors.subdivision_based_grid  —  Function subdivision_based_grid(ds::DynamicalSystem, grid; maxlevel = 4, q = 0.99) Construct a grid structure  SubdivisionBasedGrid  that can be directly passed as a grid to  AttractorsViaRecurrences . The input  grid  is an originally coarse grid (a tuple of  AbstractRange s). The state space speed is evaluate in all cells of the  grid . Cells with small speed (when compared to the \"max\" speed) resultin in this cell being subdivided more. To avoid problems with spikes in the speed, the  q -th quantile of the velocities is used as the \"max\" speed (use  q = 1  for true maximum). The subdivisions in the resulting grid are clamped to at most value  maxlevel . This approach is designed for  continuous time  systems in which different areas of the state space flow may have significantly different velocity. In case of originally coarse grids, this may lead  AttractorsViaRecurrences  being stuck in some state space regions with a small motion speed and false identification of attractors. source"},{"id":332,"pagetitle":"Finding Attractors","title":"Proximity","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Proximity","content":" Proximity"},{"id":333,"pagetitle":"Finding Attractors","title":"Attractors.AttractorsViaProximity","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.AttractorsViaProximity","content":" Attractors.AttractorsViaProximity  —  Type AttractorsViaProximity(ds::DynamicalSystem, attractors::Dict [, ε]; kwargs...) Map initial conditions to attractors based on whether the trajectory reaches  ε -distance close to any of the user-provided  attractors . They have to be in a form of a dictionary mapping attractor labels to  StateSpaceSet s containing the attractors. The system gets stepped, and at each step the minimum distance to all attractors is computed. If any of these distances is  < ε , then the label of the nearest attractor is returned. If an  ε::Real  is not provided by the user, a value is computed automatically as half of the minimum distance between all attractors. This operation can be expensive for large  StateSpaceSet s. If  length(attractors) == 1 , then  ε  becomes 1/10 of the diagonal of the box containing the attractor. If  length(attractors) == 1  and the attractor is a single point, an error is thrown. Keywords Ttr = 100 : Transient time to first evolve the system for before checking for proximity. Δt = 1 : Step time given to  step! . horizon_limit = 1e3 : If the maximum distance of the trajectory from any of the given attractors exceeds this limit, it is assumed that the trajectory diverged (gets labelled as  -1 ). consecutive_lost_steps = 1000 : If the integrator has been stepped this many times without coming  ε -near to any attractor,  it is assumed that the trajectory diverged (gets labelled as  -1 ). source"},{"id":334,"pagetitle":"Finding Attractors","title":"Featurizing","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Featurizing","content":" Featurizing"},{"id":335,"pagetitle":"Finding Attractors","title":"Attractors.AttractorsViaFeaturizing","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.AttractorsViaFeaturizing","content":" Attractors.AttractorsViaFeaturizing  —  Type AttractorsViaFeaturizing(\n    ds::DynamicalSystem, featurizer::Function,\n    grouping_config = GroupViaClustering(); kwargs...\n) Initialize a  mapper  that maps initial conditions to attractors using a featurizing and grouping approach. This is a supercase of the featurizing and clustering approach that is utilized by bSTAB ( Stender and Hoffmann, 2021 ) and MCBB ( Gelbrecht  et al. , 2020 ). See  AttractorMapper  for how to use the  mapper . This  mapper  also allows the syntax  mapper(u0)  but only if the  grouping_config  is  not GroupViaClustering . featurizer  is a function  f(A, t)  that takes as an input an integrated trajectory  A::StateSpaceSet  and the corresponding time vector  t  and returns a vector  v  of features describing the trajectory. For better performance, it is strongly recommended that  v isa SVector{<:Real} . grouping_config  is an instance of any subtype of  GroupingConfig  and decides how features will be grouped into attractors, see below. See also the intermediate functions  extract_features  and  group_features , which can be utilized when wanting to work directly with features. Keyword arguments T=100, Ttr=100, Δt=1 : Propagated to  trajectory . threaded = true : Whether to run the generation of features over threads by integrating trajectories in parallel. Description The trajectory  X  of an initial condition is transformed into features. Each feature is a number useful in  characterizing the attractor  the initial condition ends up at, and distinguishing it from other attractors. Example features are the mean or standard deviation of some the dimensions of the trajectory, the entropy of some of the dimensions, the fractal dimension of  X , or anything else you may fancy. All feature vectors (each initial condition = 1 vector) are then grouped using one of the sevaral available grouping configurations. Each group is assumed to be a unique attractor, and hence each initial condition is labelled according to the group it is part of. The method thus relies on the user having at least some basic idea about what attractors to expect in order to pick the right features, and the right way to group them, in contrast to  AttractorsViaRecurrences . source"},{"id":336,"pagetitle":"Finding Attractors","title":"Grouping configurations","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Grouping-configurations","content":" Grouping configurations"},{"id":337,"pagetitle":"Finding Attractors","title":"Grouping types","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Grouping-types","content":" Grouping types"},{"id":338,"pagetitle":"Finding Attractors","title":"Attractors.GroupingConfig","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.GroupingConfig","content":" Attractors.GroupingConfig  —  Type GroupingConfig Supertype for configuration structs on how to group features together. Used in several occasions such as  AttractorsViaFeaturizing  or  aggregate_attractor_fractions . Currently available grouping configurations are: GroupViaClustering GroupViaNearestFeature GroupViaHistogram GroupViaPairwiseComparison GroupingConfig  defines an extendable interface. The only thing necessary for a new grouping configuration is to: Make a new type and subtype  GroupingConfig . If the grouping allows for mapping individual initial conditions to IDs, then instead extend the  internal function feature_to_group(feature, config) . This will allow doing  id = mapper(u0)  with  AttractorsViaFeaturizing . Else, extend the function  group_features(features, config) . You could still extend  group_features  even if (2.) is satisfied, if there are any performance benefits. Include the new grouping file in the  grouping/all_grouping_configs.jl  and list it in this documentation string. source"},{"id":339,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaClustering","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.GroupViaClustering","content":" Attractors.GroupViaClustering  —  Type GroupViaClustering(; kwargs...) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaClustering  clusters features into groups using DBSCAN, similar to the original work by bSTAB ( Stender and Hoffmann, 2021 ) and MCBB ( Gelbrecht  et al. , 2020 ). Several options on clustering are available, see keywords below. The defaults are a significant improvement over existing literature, see Description. Keyword arguments clust_distance_metric = Euclidean() : A metric to be used in the clustering. It can be any function  f(a, b)  that returns the distance between real-valued vectors  a, b . All metrics from Distances.jl can be used here. rescale_features = true : if true, rescale each dimension of the extracted features separately into the range  [0,1] . This typically leads to more accurate clustering. min_neighbors = 10 : minimum number of neighbors (i.e. of similar features) each feature needs to have, including counting its own self, in order to be considered in a cluster (fewer than this, it is labeled as an outlier,  -1 ). use_mmap = false : whether to use an on-disk map for creating the distance matrix of the features. Useful when the features are so many where a matrix with side their length would not fit to memory. Keywords for optimal radius estimation optimal_radius_method::Union{Real, String} = \"silhouettes_optim\" : if a real number, it is the radius used to cluster features. Otherwise, it determines the method used to automatically determine that radius. Possible values are: \"silhouettes\" : Performs a linear (sequential) search for the radius that maximizes a   statistic of the silhouette values of clusters (typically the mean). This can be chosen   with  silhouette_statistic . The linear search may take some time to finish. To   increase speed, the number of radii iterated through can be reduced by decreasing    num_attempts_radius  (see its entry below). \"silhouettes_optim\" : Same as  \"silhouettes\"  but performs an optimized search via   Optim.jl. It's faster than  \"silhouettes\" , with typically the same accuracy (the   search here is not guaranteed to always find the global maximum, though it typically   gets close). \"knee\" : chooses the the radius according to the knee (a.k.a. elbow,   highest-derivative method) and is quicker, though generally leading to much worse   clustering. It requires that  min_neighbors  > 1. num_attempts_radius = 100 : number of radii that the  optimal_radius_method  will try out in its iterative procedure. Higher values increase the accuracy of clustering, though not necessarily much, while always reducing speed. silhouette_statistic::Function = mean : statistic (e.g. mean or minimum) of the silhouettes that is maximized in the \"optimal\" clustering. The original implementation in ( Stender and Hoffmann, 2021 ) used the  minimum  of the silhouettes, and typically performs less accurately than the  mean . max_used_features = 0 : if not  0 , it should be an  Int  denoting the max amount of features to be used when finding the optimal radius. Useful when clustering a very large number of features (e.g., high accuracy estimation of fractions of basins of attraction). Description The DBSCAN clustering algorithm is used to automatically identify clusters of similar features. Each feature vector is a point in a feature space. Each cluster then basically groups points that are closely packed together. Closely packed means that the points have at least  min_neighbors  inside a ball of radius  optimal_radius  centered on them. This method typically works well if the radius is chosen well, which is not necessarily an easy task. Currently, three methods are implemented to automatically estimate an \"optimal\" radius. Estimating the optimal radius The default method is the  silhouettes method , which includes keywords  silhouette  and  silhouette_optim . Both of them search for the radius that optimizes the clustering, meaning the one that maximizes a statistic  silhouette_statistic  (e.g. mean value) of a quantifier for the quality of each cluster. This quantifier is the silhouette value of each identified cluster. A silhouette value measures how similar a point is to the cluster it currently belongs to, compared to the other clusters, and ranges from -1 (worst matching) to +1 (ideal matching). If only one cluster is found, the assigned silhouette is zero. So for each attempted radius in the search the clusters are computed, their silhouettes calculated, and the statistic of these silhouettes computed. The algorithm then finds the radius that leads to the maximum such statistic. For  optimal_radius_method = \"silhouettes\" , the search is done linearly, from a minimum to a maximum candidate radius for  optimal_radius_method = \"silhouettes\" ;  optimal_radius_method = silhouettes_optim , it is done via an optimized search performed by Optim.jl which is typically faster and with similar accuracy. A third alternative is the \"elbow\"  method, which works by calculating the distance of each point to its k-nearest-neighbors (with  k=min_neighbors ) and finding the distance corresponding to the highest derivative in the curve of the distances, sorted in ascending order. This distance is chosen as the optimal radius. It is described in ( Ester  et al. , 1996 ) and ( Schubert  et al. , 2017 ). It typically performs considerably worse than the  \"silhouette\"  methods. source"},{"id":340,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaHistogram","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.GroupViaHistogram","content":" Attractors.GroupViaHistogram  —  Type GroupViaHistogram(binning::FixedRectangularBinning) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaHistogram  performs a histogram in feature space. Then, all features that are in the same histogram bin get the same label. The  binning  is an instance of  FixedRectangularBinning  from ComplexityMeasures.jl. (the reason to not allow  RectangularBinning  is because during continuation we need to ensure that bins remain identical). source"},{"id":341,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaNearestFeature","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.GroupViaNearestFeature","content":" Attractors.GroupViaNearestFeature  —  Type GroupViaNearestFeature(templates; kwargs...) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaNearestFeature  accepts a  template , which is a vector of features. Then, generated features from initial conditions in  AttractorsViaFeaturizing  are labelled according to the feature in  templates  that is closest (the label is the index of the closest template). templates  can be a vector or dictionary mapping keys to templates. Internally all templates are converted to  SVector  for performance. Hence, it is strongly recommended that both  templates  and the output of the  featurizer  function in  AttractorsViaFeaturizing  return  SVector  types. Keyword arguments metric = Euclidean() : metric to be used to quantify distances in the feature space. max_distance = Inf : Maximum allowed distance between a feature and its nearest template for it to be assigned to that template. By default,  Inf  guarantees that a feature is assigned to its nearest template regardless of the distance. Features that exceed  max_distance  to their nearest template get labelled  -1 . source"},{"id":342,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaPairwiseComparison","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.GroupViaPairwiseComparison","content":" Attractors.GroupViaPairwiseComparison  —  Type GroupViaPairwiseComparison(; threshold::Real, kwargs...) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaPairwiseComparison  groups features and identifies clusters by considering the pairwise distance between features. It can be used as an alternative to the clustering method in  GroupViaClustering , having the advantage that it is simpler, typically faster and uses less memory. Keyword arguments threshold  (mandatory): A real number defining the maximum distance two features can be to be considered in the same cluster - above the threshold, features are different. This value simply needs to be large enough to differentiate clusters. metric = Euclidean() : A function  metric(a, b)  that returns the distance between two features  a  and  b , outputs of  featurizer . Any  Metric  from Distances.jl can be used here. rescale_features = true : if true, rescale each dimension of the extracted features separately into the range  [0,1] . This typically leads to more accurate grouping. Description This algorithm assumes that the features are well-separated into distinct clouds, with the maximum radius of the cloud controlled by  threshold . Since the systems are deterministic, this is achievable with a good-enough  featurizer  function, by removing transients, and running the trajectories for sufficiently long. It then considers that features belong to the same attractor when their pairwise distance, computed using  metric , is smaller than or equal to  threshold , and that they belong to different attractors when the distance is bigger. Attractors correspond to each grouping of similar features. In this way, the key parameter  threshold  is basically the amount of variation permissible in the features belonging to the same attractor. If they are well-chosen, the value can be relatively small and does not need to be fine tuned. The  threshold  should achieve a balance: one one hand, it should be large enough to account for variations in the features from the same attractor - if it's not large enough, the algorithm will find duplicate attractors. On the other hand, it should be small enough to not group together features from distinct attractors. This requires some knowledge of how spread the features are. If it's too big, the algorithm will miss some attractors, as it groups 2+ distinct attractors together. Therefore, as a rule of thumb, one can repeat the procedure a few times, starting with a relatively large value and reducing it until no more attractors are found and no duplicates appear. The method uses relatively little memory, as it only stores vectors whose size is on order of the number of attractors of the system. source"},{"id":343,"pagetitle":"Finding Attractors","title":"Grouping utility functions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Grouping-utility-functions","content":" Grouping utility functions"},{"id":344,"pagetitle":"Finding Attractors","title":"Attractors.group_features","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.group_features","content":" Attractors.group_features  —  Function group_features(features, group_config::GroupingConfig) → labels Group the given vector of feature vectors according to the configuration and return the labels (vector of equal length as  features ). See  AttractorsViaFeaturizing  for possible configurations. source"},{"id":345,"pagetitle":"Finding Attractors","title":"Attractors.extract_features","ref":"/DynamicalSystemsDocs.jl/attractors/stable/attractors/#Attractors.extract_features","content":" Attractors.extract_features  —  Function extract_features(mapper, ics; N = 1000, show_progress = true) Return a vector of the features of each initial condition in  ics  (as in  basins_fractions ), using the configuration of  mapper::AttractorsViaFeaturizing . Keyword  N  is ignored if  ics isa StateSpaceSet . source"},{"id":348,"pagetitle":"Basins of Attraction","title":"Basins of Attraction","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Basins-of-Attraction","content":" Basins of Attraction This page provides several functions related to the basins of attraction and their boundaries. It requires you to have first understood the  Finding Attractors  page."},{"id":349,"pagetitle":"Basins of Attraction","title":"Basins of attraction","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Basins-of-attraction","content":" Basins of attraction Calculating basins of attraction, or their state space fractions, can be done with the functions: basins_fractions basins_of_attraction ."},{"id":350,"pagetitle":"Basins of Attraction","title":"Attractors.basins_fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.basins_fractions","content":" Attractors.basins_fractions  —  Function basins_fractions(\n    mapper::AttractorMapper,\n    ics::Union{StateSpaceSet, Function};\n    kwargs...\n) Approximate the state space fractions  fs  of the basins of attraction of a dynamical system by mapping initial conditions to attractors using  mapper  (which contains a reference to a  DynamicalSystem ). The fractions are simply the ratios of how many initial conditions ended up at each attractor. Initial conditions to use are defined by  ics . It can be: a  StateSpaceSet  of initial conditions, in which case all are used. a 0-argument function  ics()  that spits out random initial conditions. Then  N  random initial conditions are chosen. See  statespace_sampler  to generate such functions. Return The function will always return  fractions , which is a dictionary whose keys are the labels given to each attractor (always integers enumerating the different attractors), and whose values are the respective basins fractions. The label  -1  is given to any initial condition where  mapper  could not match to an attractor (this depends on the  mapper  type). If  ics  is a  StateSpaceSet  the function will also return  labels , which is a  vector , of equal length to  ics , that contains the label each initial condition was mapped to. See  AttractorMapper  for all possible  mapper  types, and use  extract_attractors  (after calling  basins_fractions ) to extract the stored attractors from the  mapper . See also  convergence_and_basins_fractions . Keyword arguments N = 1000 : Number of random initial conditions to generate in case  ics  is a function. show_progress = true : Display a progress bar of the process. source basins_fractions(basins::AbstractArray [,ids]) → fs::Dict Calculate the state space fraction of the basins of attraction encoded in  basins . The elements of  basins  are integers, enumerating the attractor that the entry of  basins  converges to (i.e., like the output of  basins_of_attraction ). Return a dictionary that maps attractor IDs to their relative fractions. Optionally you may give a vector of  ids  to calculate the fractions of only the chosen ids (by default  ids = unique(basins) ). In ( Menck  et al. , 2013 ) the authors use these fractions to quantify the stability of a basin of attraction, and specifically how it changes when a parameter is changed. For this, see  continuation . source"},{"id":351,"pagetitle":"Basins of Attraction","title":"Attractors.extract_attractors","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.extract_attractors","content":" Attractors.extract_attractors  —  Function extract_attractors(mapper::AttractorsMapper) → attractors Return a dictionary mapping label IDs to attractors found by the  mapper . This function should be called after calling  basins_fractions  with the given  mapper  so that the attractors have actually been found first. For  AttractorsViaFeaturizing , the attractors are only stored if the mapper was called with pre-defined initial conditions rather than a sampler (function returning initial conditions). source"},{"id":352,"pagetitle":"Basins of Attraction","title":"Attractors.basins_of_attraction","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.basins_of_attraction","content":" Attractors.basins_of_attraction  —  Function basins_of_attraction(mapper::AttractorMapper, grid::Tuple) → basins, attractors Compute the full basins of attraction as identified by the given  mapper , which includes a reference to a  DynamicalSystem  and return them along with (perhaps approximated) found attractors. grid  is a tuple of ranges defining the grid of initial conditions that partition the state space into boxes with size the step size of each range. For example,  grid = (xg, yg)  where  xg = yg = range(-5, 5; length = 100) . The grid has to be the same dimensionality as the state space expected by the integrator/system used in  mapper . E.g., a  ProjectedDynamicalSystem  could be used for lower dimensional projections, etc. A special case here is a  PoincareMap  with  plane  being  Tuple{Int, <: Real} . In this special scenario the grid can be one dimension smaller than the state space, in which case the partitioning happens directly on the hyperplane the Poincaré map operates on. basins_of_attraction  function is a convenience 5-lines-of-code wrapper which uses the  labels  returned by  basins_fractions  and simply assigns them to a full array corresponding to the state space partitioning indicated by  grid . See also  convergence_and_basins_of_attraction . source basins_of_attraction(mapper::AttractorsViaRecurrences; show_progress = true) This is a special method of  basins_of_attraction  that using recurrences does  exactly  what is described in the paper by Datseris & Wagemakers ( Datseris and Wagemakers, 2022 ). By enforcing that the internal grid of  mapper  is the same as the grid of initial conditions to map to attractors, the method can further utilize found exit and attraction basins, making the computation faster as the grid is processed more and more. source"},{"id":353,"pagetitle":"Basins of Attraction","title":"StateSpaceSets.statespace_sampler","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#StateSpaceSets.statespace_sampler","content":" StateSpaceSets.statespace_sampler  —  Function statespace_sampler(region [, seed = 42]) → sampler, isinside A function that facilitates sampling points randomly and uniformly in a state space  region . It generates two functions: sampler  is a 0-argument function that when called generates a random point inside a state space  region . The point is always a  Vector  for type stability irrespectively of dimension. Generally, the generated point should be  copied  if it needs to be stored. (i.e., calling  sampler()  utilizes a shared vector)  sampler  is a thread-safe function. isinside  is a 1-argument function that returns  true  if the given state space point is inside the  region . The  region  can be an instance of any of the following types (input arguments if not specified are vectors of length  D , with  D  the state space dimension): HSphere(radius::Real, center) : points  inside  the hypersphere (boundary excluded). Convenience method  HSphere(radius::Real, D::Int)  makes the center a  D -long vector of zeros. HSphereSurface(radius, center) : points on the hypersphere surface. Same convenience method as above is possible. HRectangle(mins, maxs) : points in [min, max) for the bounds along each dimension. The random number generator is always  Xoshiro  with the given  seed . source statespace_sampler(grid::NTuple{N, AbstractRange} [, seed]) If given a  grid  that is a tuple of  AbstractVector s, the minimum and maximum of the vectors are used to make an  HRectangle  region. source"},{"id":354,"pagetitle":"Basins of Attraction","title":"Convergence times","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Convergence-times","content":" Convergence times"},{"id":355,"pagetitle":"Basins of Attraction","title":"Attractors.convergence_and_basins_fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.convergence_and_basins_fractions","content":" Attractors.convergence_and_basins_fractions  —  Function convergence_and_basins_fractions(mapper::AttractorMapper, ics::StateSpaceSet) An extension of  basins_fractions . Return  fs, labels, convergence . The first two are as in  basins_fractions , and  convergence  is a vector containing the time each initial condition took to converge to its attractor. Only usable with mappers that support  id = mapper(u0) . See also  convergence_time . Keyword arguments show_progress = true : show progress bar. source"},{"id":356,"pagetitle":"Basins of Attraction","title":"Attractors.convergence_and_basins_of_attraction","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.convergence_and_basins_of_attraction","content":" Attractors.convergence_and_basins_of_attraction  —  Function convergence_and_basins_of_attraction(mapper::AttractorMapper, grid) An extension of  basins_of_attraction . Return  basins, attractors, convergence , with  basins, attractors  as in  basins_of_attraction , and  convergence  being an array with same shape as  basins . It contains the time each initial condition took to converge to its attractor. It is useful to give to  shaded_basins_heatmap . See also  convergence_time . Keyword arguments show_progress = true : show progress bar. source"},{"id":357,"pagetitle":"Basins of Attraction","title":"Attractors.convergence_time","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.convergence_time","content":" Attractors.convergence_time  —  Function convergence_time(mapper::AttractorMapper) → t Return the approximate time the  mapper  took to converge to an attractor. This function should be called just right after  mapper(u0)  was called with  u0  the initial condition of interest. Hence it is only valid with  AttractorMapper  subtypes that support this syntax. Obtaining the convergence time is computationally free, so that  convergence_and_basins_fractions  can always be used instead of  basins_fractions . source"},{"id":358,"pagetitle":"Basins of Attraction","title":"Final state sensitivity / fractal boundaries","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Final-state-sensitivity-/-fractal-boundaries","content":" Final state sensitivity / fractal boundaries Several functions are provided related with analyzing the fractality of the boundaries of the basins of attraction: basins_fractal_dimension basin_entropy basins_fractal_test uncertainty_exponent test_wada_merge"},{"id":359,"pagetitle":"Basins of Attraction","title":"Attractors.basins_fractal_dimension","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.basins_fractal_dimension","content":" Attractors.basins_fractal_dimension  —  Function basins_fractal_dimension(basins; kwargs...) -> V_ε, N_ε, d Estimate the fractal dimension  d  of the boundary between basins of attraction using a box-counting algorithm for the boxes that contain at least two different basin IDs. Keyword arguments range_ε = 2:maximum(size(basins))÷20  is the range of sizes of the box to test (in pixels). Description The output  N_ε  is a vector with the number of the balls of radius  ε  (in pixels) that contain at least two initial conditions that lead to different attractors.  V_ε  is a vector with the corresponding size of the balls. The output  d  is the estimation of the box-counting dimension of the boundary by fitting a line in the  log.(N_ε)  vs  log.(1/V_ε)  curve. However it is recommended to analyze the curve directly for more accuracy. It is the implementation of the popular algorithm of the estimation of the box-counting dimension. The algorithm search for a covering the boundary with  N_ε  boxes of size  ε  in pixels. source"},{"id":360,"pagetitle":"Basins of Attraction","title":"Attractors.basin_entropy","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.basin_entropy","content":" Attractors.basin_entropy  —  Function basin_entropy(basins::Array, ε = 20) -> Sb, Sbb Compute the basin entropy ( Daza  et al. , 2016 )  Sb  and basin boundary entropy  Sbb  of the given  basins  of attraction by considering  ε  boxes along each dimension. Description First, the input  basins  is divided regularly into n-dimensional boxes of side  ε  (along all dimensions). Then  Sb  is simply the average of the Gibbs entropy computed over these boxes. The function returns the basin entropy  Sb  as well as the boundary basin entropy  Sbb . The later is the average of the entropy only for boxes that contains at least two different basins, that is, for the boxes on the boundaries. The basin entropy is a measure of the uncertainty on the initial conditions of the basins. It is maximum at the value  log(n_att)  being  n_att  the number of attractors. In this case the boundary is intermingled: for a given initial condition we can find another initial condition that lead to another basin arbitrarily close. It provides also a simple criterion for fractality: if the boundary basin entropy  Sbb  is above  log(2)  then we have a fractal boundary. It doesn't mean that basins with values below cannot have a fractal boundary, for a more precise test see  basins_fractal_test . An important feature of the basin entropy is that it allows comparisons between different basins using the same box size  ε . source"},{"id":361,"pagetitle":"Basins of Attraction","title":"Attractors.basins_fractal_test","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.basins_fractal_test","content":" Attractors.basins_fractal_test  —  Function basins_fractal_test(basins; ε = 20, Ntotal = 1000) -> test_res, Sbb Perform an automated test to decide if the boundary of the basins has fractal structures based on the method of Puy et al. ( Puy  et al. , 2021 ). Return  test_res  ( :fractal  or  :smooth ) and the mean basin boundary entropy. Keyword arguments ε = 20 : size of the box to compute the basin boundary entropy. Ntotal = 1000 : number of balls to test in the boundary for the computation of  Sbb Description The test \"looks\" at the basins with a magnifier of size  ε  at random. If what we see in the magnifier looks like a smooth boundary (onn average) we decide that the boundary is smooth. If it is not smooth we can say that at the scale  ε  we have structures, i.e., it is fractal. In practice the algorithm computes the boundary basin entropy  Sbb basin_entropy  for  Ntotal  random boxes of radius  ε . If the computed value is equal to theoretical value of a smooth boundary (taking into account statistical errors and biases) then we decide that we have a smooth boundary. Notice that the response  test_res  may depend on the chosen ball radius  ε . For larger size, we may observe structures for smooth boundary and we obtain a  different  answer. The output  test_res  is a symbol describing the nature of the basin and the output  Sbb  is the estimated value of the boundary basin entropy with the sampling method. source"},{"id":362,"pagetitle":"Basins of Attraction","title":"Attractors.uncertainty_exponent","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.uncertainty_exponent","content":" Attractors.uncertainty_exponent  —  Function uncertainty_exponent(basins; kwargs...) -> ε, N_ε, α Estimate the uncertainty exponent( Grebogi  et al. , 1983 ) of the basins of attraction. This exponent is related to the final state sensitivity of the trajectories in the phase space. An exponent close to  1  means basins with smooth boundaries whereas an exponent close to  0  represent completely fractalized basins, also called riddled basins. The output  N_ε  is a vector with the number of the balls of radius  ε  (in pixels) that contain at least two initial conditions that lead to different attractors. The output  α  is the estimation of the uncertainty exponent using the box-counting dimension of the boundary by fitting a line in the  log.(N_ε)  vs  log.(1/ε)  curve. However it is recommended to analyze the curve directly for more accuracy. Keyword arguments range_ε = 2:maximum(size(basins))÷20  is the range of sizes of the ball to test (in pixels). Description A phase space with a fractal boundary may cause a uncertainty on the final state of the dynamical system for a given initial condition. A measure of this final state sensitivity is the uncertainty exponent. The algorithm probes the basin of attraction with balls of size  ε  at random. If there are a least two initial conditions that lead to different attractors, a ball is tagged \"uncertain\".  f_ε  is the fraction of \"uncertain balls\" to the total number of tries in the basin. In analogy to the fractal dimension, there is a scaling law between,  f_ε ~ ε^α . The number that characterizes this scaling is called the uncertainty exponent  α . Notice that the uncertainty exponent and the box counting dimension of the boundary are related. We have  Δ₀ = D - α  where  Δ₀  is the box counting dimension computed with  basins_fractal_dimension  and  D  is the dimension of the phase space. The algorithm first estimates the box counting dimension of the boundary and returns the uncertainty exponent. source"},{"id":363,"pagetitle":"Basins of Attraction","title":"Attractors.test_wada_merge","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.test_wada_merge","content":" Attractors.test_wada_merge  —  Function test_wada_merge(basins, r) -> p Test if the 2D array  basins  has the  Wada property  using the merging technique of ( Daza  et al. , 2018 ). Description The technique consists in computing the generalized basins of each attractor. These new basins are formed with on of the basins and the union of the other basins. A new boundary is defined by these two objects. The algorithm then computes the distance between each boundaries of these basins pairwise. If all the boundaries are within some distance  r , there is a unique boundary separating the basins and we have the wada property. The algorithm returns the maximum proportion of pixels of a boundary with distance strictly greater than  r  from another boundary. If  p == 0 ,  we have the Wada property for this value of  r . If  p > 0 , the criteria to decide if the basins are Wada is left to the user. Numerical inaccuracies may be responsible for a small percentage of points with distance larger than  r source"},{"id":364,"pagetitle":"Basins of Attraction","title":"Edge tracking and edge states","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Edge-tracking-and-edge-states","content":" Edge tracking and edge states The edge tracking algorithm allows to locate and construct so-called edge states (also referred to as  Melancholia states ) embedded in the basin boundary separating different basins of attraction. These could be saddle points, unstable periodic orbits or chaotic saddles. The general idea is that these sets can be found because they act as attractors when restricting to the basin boundary."},{"id":365,"pagetitle":"Basins of Attraction","title":"Attractors.edgetracking","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.edgetracking","content":" Attractors.edgetracking  —  Function edgetracking(ds::DynamicalSystem, attractors::Dict; kwargs...) Track along a basin boundary in a dynamical system  ds  with two or more attractors in order to find an  edge state . Results are returned in the form of  EdgeTrackingResults , which contains the pseudo-trajectory  edge  representing the track on the basin boundary, along with additional output (see below). The system's  attractors  are specified as a  Dict  of  StateSpaceSet s, as in  AttractorsViaProximity  or the output of  extract_attractors . By default, the algorithm is initialized from the first and second attractor in  attractors . Alternatively, the initial states can be set via keyword arguments  u1 ,  u2  (see below). Note that the two initial states must belong to different basins of attraction. Keyword arguments bisect_thresh = 1e-7 : distance threshold for bisection diverge_thresh = 1e-6 : distance threshold for parallel integration u1 : first initial state (defaults to first point in first entry of  attractors ) u2 : second initial state (defaults to first point in second entry of  attractors ) maxiter = 100 : maximum number of iterations before the algorithm stops abstol = 0.0 : distance threshold for convergence of the updated edge state T_transient = 0.0 : transient time before the algorithm starts saving the edge track tmax = Inf : maximum integration time of parallel trajectories until re-bisection  Δt = 0.01 : time step passed to  step!  when evolving the two trajectories ϵ_mapper = nothing :  ϵ  parameter in  AttractorsViaProximity show_progress = true : if true, shows progress bar and information while running verbose = true : if false, silences print output and warnings while running kwargs... : additional keyword arguments to be passed to  AttractorsViaProximity Description The edge tracking algorithm is a numerical method to find an  edge state  or (possibly chaotic) saddle on the boundary between two basins of attraction. Introduced by ( Battelino  et al. , 1988 ) and further described by ( Skufca  et al. , 2006 ), the algorithm has been applied to, e.g., the laminar-turbulent boundary in plane Couette flow ( Schneider  et al. , 2008 ), Wada basins ( Wagemakers  et al. , 2020 ), as well as Melancholia states in conceptual ( Mehling  et al. , 2023 ) and intermediate-complexity ( Lucarini and Bódai, 2017 )  climate models.  Relying only on forward integration of the system, it works even in high-dimensional systems with complicated fractal basin boundary structures. The algorithm consists of two main steps: bisection and tracking. First, it iteratively  bisects along a straight line in state space between the intial states  u1  and  u2  to find the separating basin boundary. The bisection stops when the two updated states are less than  bisect_thresh  (Euclidean distance in state space) apart from each other. Next, a  ParallelDynamicalSystem  is initialized from these two updated states and integrated forward until the two trajectories diverge from each other by more than  diverge_thresh  (Euclidean distance). The two final states of the parallel integration are then used as new states  u1  and  u2  for a new bisection, and  so on, until a stopping criterion is fulfilled.  Two stopping criteria are implemented via the keyword arguments  maxiter  and  abstol . Either the algorithm stops when the number of iterations reaches  maxiter , or when the state space position of the updated edge point changes by less than  abstol  (in Euclidean distance) compared to the previous iteration. Convergence below  abstol  happens after sufficient iterations if the edge state is a saddle point. However, the edge state may also be an unstable limit cycle or a chaotic saddle. In these cases, the algorithm will never actually converge to a point but (after a transient period) continue populating the set constituting the edge state by tracking along it. A central idea behind this algorithm is that basin boundaries are typically the stable manifolds of unstable sets, namely edge states or saddles. The flow along the basin boundary  will thus lead to these sets, and the iterative bisection neutralizes the unstable direction of the flow away from the basin boundary. If the system possesses multiple edge  states, the algorithm will find one of them depending on where the initial bisection locates the boundary. Output Returns a data type  EdgeTrackingResults  containing the results. Sometimes, the AttractorMapper used in the algorithm may erroneously identify both states  u1  and  u2  with the same basin of attraction due to being very close to the basin boundary. If this happens, a warning is raised and  EdgeTrackingResults.success = false . source edgetracking(pds::ParallelDynamicalSystem, mapper::AttractorMapper; kwargs...) Low-level function for running the edge tracking algorithm, see  edgetracking  for a description, keyword arguments and output type. pds  is a  ParallelDynamicalSystem  with two states. The  mapper  must be an  AttractorMapper  of subtype  AttractorsViaProximity  or  AttractorsViaRecurrences . source"},{"id":366,"pagetitle":"Basins of Attraction","title":"Attractors.EdgeTrackingResults","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.EdgeTrackingResults","content":" Attractors.EdgeTrackingResults  —  Type EdgeTrackingResults(edge, track1, track2, time, bisect_idx) Data type that stores output of the  edgetracking  algorithm. Fields edge::StateSpaceSet : the pseudo-trajectory representing the tracked edge segment (given by the average in state space between  track1  and  track2 ) track1::StateSpaceSet : the pseudo-trajectory tracking the edge within basin 1 track2::StateSpaceSet : the pseudo-trajectory tracking the edge within basin 2 time::Vector : time points of the above  StateSpaceSet s bisect_idx::Vector : indices of  time  at which a re-bisection occurred success::Bool : indicates whether the edge tracking has been successful or not source"},{"id":367,"pagetitle":"Basins of Attraction","title":"Attractors.bisect_to_edge","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.bisect_to_edge","content":" Attractors.bisect_to_edge  —  Function bisect_to_edge(pds::ParallelDynamicalSystem, mapper::AttractorMapper; kwargs...) -> u1, u2 Finds the basin boundary between two states  u1, u2 = current_states(pds)  by bisecting along a straight line in phase space. The states  u1  and  u2  must belong to different basins. Returns a triple  u1, u2, success , where  u1, u2  are two new states located on either side of the basin boundary that lie less than  bisect_thresh  (Euclidean distance in state space) apart from each other, and  success  is a Bool indicating whether the bisection was successful (it may fail if the  mapper  maps both states to the same basin of attraction, in which case a warning is raised). Keyword arguments bisect_thresh = 1e-7 : The maximum (Euclidean) distance between the two returned states. Description pds  is a  ParallelDynamicalSystem  with two states. The  mapper  must be an  AttractorMapper  of subtype  AttractorsViaProximity  or  AttractorsViaRecurrences . Info If the straight line between  u1  and  u2  intersects the basin boundary multiple times, the method will find one of these intersection points. If more than two attractors exist, one of the two returned states may belong to a different basin than the initial conditions  u1  and  u2 . A warning is raised if the bisection involves a third basin. source"},{"id":368,"pagetitle":"Basins of Attraction","title":"Tipping points","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Tipping-points","content":" Tipping points This page discusses functionality related with tipping points in dynamical systems with known rule. If instead you are interested in identifying tipping points in measured timeseries, have a look at  TransitionIndicators.jl ."},{"id":369,"pagetitle":"Basins of Attraction","title":"Attractors.tipping_probabilities","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.tipping_probabilities","content":" Attractors.tipping_probabilities  —  Function tipping_probabilities(basins_before, basins_after) → P Return the tipping probabilities of the computed basins before and after a change in the system parameters (or time forcing), according to the definition of ( Kaszás  et al. , 2019 ). The input  basins  are integer-valued arrays, where the integers enumerate the attractor, e.g. the output of  basins_of_attraction . Description Let  $\\mathcal{B}_i(p)$  denote the basin of attraction of attractor  $A_i$  at parameter(s)  $p$ . Kaszás et al ( Kaszás  et al. , 2019 ) define the tipping probability from  $A_i$  to  $A_j$ , given a parameter change in the system of  $p_- \\to p_+$ , as \\[P(A_i \\to A_j | p_- \\to p_+) =\n\\frac{|\\mathcal{B}_j(p_+) \\cap \\mathcal{B}_i(p_-)|}{|\\mathcal{B}_i(p_-)|}\\] where  $|\\cdot|$  is simply the volume of the enclosed set. The value of  $P(A_i \\to A_j | p_- \\to p_+)$  is  P[i, j] . The equation describes something quite simple: what is the overlap of the basin of attraction of  $A_i$  at  $p_-$  with that of the attractor  $A_j$  at  $p_+$ . If  basins_before, basins_after  contain values of  -1 , corresponding to trajectories that diverge, this is considered as the last attractor of the system in  P . source"},{"id":370,"pagetitle":"Basins of Attraction","title":"Minimal Fatal Shock","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Minimal-Fatal-Shock","content":" Minimal Fatal Shock The algorithm to find minimal perturbation for arbitrary initial condition  u0  which will kick the system into different from the current basin."},{"id":371,"pagetitle":"Basins of Attraction","title":"Attractors.minimal_fatal_shock","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.minimal_fatal_shock","content":" Attractors.minimal_fatal_shock  —  Function minimal_fatal_shock(mapper::AttractorMapper, u0, search_area, algorithm; kw...) Return the  minimal fatal shock mfs  (also known as  excitability threshold ) for the initial point  u0  according to the specified  algorithm  given a  mapper  that satisfies the  id = mapper(u0)  interface (see  AttractorMapper  if you are not sure which mappers do that). The  mapper  contains a reference to a  DynamicalSystem . The options for  algorithm  are:  MFSBruteForce  or  MFSBlackBoxOptim . For high dimensional systems  MFSBlackBoxOptim  is likely more accurate. The  search_area  dictates the state space range for the search of the  mfs . It can be a 2-tuple of (min, max) values, in which case the same values are used for each dimension of the system in  mapper . Otherwise, it can be a vector of 2-tuples, each for each dimension of the system. The search area is defined w.r.t. to  u0  (i.e., it is the search area for perturbations of  u0 ). An alias to  minimal_fata_shock  is  excitability_threshold . Keyword arguments metric = LinearAlgebra.norm : a metric function that gives the norm of a perturbation vector. This keyword is ignored for the  MFSBruteForce  algorithm. target_id = nothing : when not  nothing , it should be an integer or a vector of integers corresponding to target attractor label(s). Then, the MFS is estimated based only on perturbations that lead to the target attractor(s). Description The minimal fatal shock is defined as the smallest-norm perturbation of the initial point  u0  that will lead it a different basin of attraction. It is inspired by the paper \"Minimal fatal shocks in multistable complex networks\" ( Halekotte and Feudel, 2020 ), however the implementation here is generic: it works for  any  dynamical system. The  excitability threshold  is a concept nearly identical, however, instead of looking for a perturbation that simply brings us out of the basin, we look for the smallest perturbation that brings us into specified basin(s). This is enabled via the keyword  target_id . source"},{"id":372,"pagetitle":"Basins of Attraction","title":"Attractors.MFSBlackBoxOptim","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.MFSBlackBoxOptim","content":" Attractors.MFSBlackBoxOptim  —  Type MFSBlackBoxOptim(; kwargs...) The black box derivative-free optimization algorithm used in  minimal_fatal_shock . Keyword arguments guess = nothing : a initial guess for the minimal fatal shock given to the optimization algorithm. If not  nothing ,  random_algo  below is ignored. max_steps = 10000 : maximum number of steps for the optimization algorithm. penalty = 1000.0 : penalty value for the objective function for perturbations that do not lead to a different basin of attraction. This value is added to the norm of the perturbation and its value should be much larger than the typical sizes of the basins of attraction. print_info : boolean value, if true, the optimization algorithm will print information on the evaluation steps of objective function,  default = false . random_algo = MFSBruteForce(100, 100, 0.99) : an instance of  MFSBruteForce  that can be used to provide an initial guess. bbkwargs = NamedTuple() : additional keyword arguments propagated to  BlackBoxOptim.bboptimize  for selecting solver, accuracy, and more. Description The algorithm uses BlackBoxOptim.jl and a penalized objective function to minimize. y function used as a constraint function. So, if we hit another basin during the search we encourage the algorithm otherwise we punish it with some penalty. The function to minimize is (besides some details): function mfs_objective(perturbation, u0, mapper, penalty)\n    dist = norm(perturbation)\n    if mapper(u0 + perturbation) == mapper(u0)\n        # penalize if we stay in the same basin:\n        return dist + penalty\n    else\n        return dist\n    end\nend Using an initial guess can be beneficial to both performance and accuracy, which is why the output of a crude  MFSBruteForce  is used to provide a guess. This can be disabled by either passing a  guess  vector explicitly or by giving  nothing  as  random_algo . source"},{"id":373,"pagetitle":"Basins of Attraction","title":"Attractors.MFSBruteForce","ref":"/DynamicalSystemsDocs.jl/attractors/stable/basins/#Attractors.MFSBruteForce","content":" Attractors.MFSBruteForce  —  Type MFSBruteForce(; kwargs...) The brute force randomized search algorithm used in  minimal_fatal_shock . It consists of two steps: random initialization and sphere radius reduction. On the first step, the algorithm generates random perturbations within the search area and records the perturbation that leads to a different basin but with the smallest magnitude. With this obtained perturbation it proceeds to the second step. On the second step, the algorithm generates random perturbations on the surface of the hypersphere with radius equal to the norm of the perturbation found in the first step. It reduces the radius of the hypersphere and continues searching for the better result with a smaller radius. Each time a better result is found, the radius is reduced further. The algorithm records the perturbation with smallest radius that leads to a different basin. Keyword arguments initial_iterations = 10000 : number of random perturbations to try in the first step of the algorithm. sphere_iterations = 10000 : number of steps while initializing random points on hypersphere and decreasing its radius. sphere_decrease_factor = 0.999  factor by which the radius of the hypersphere is decreased (at each step the radius is multiplied by this number). Number closer to 1 means more refined accuracy source"},{"id":376,"pagetitle":"Attractor & Basins Continuation","title":"Attractor & Basins Continuation","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractor-and-Basins-Continuation","content":" Attractor & Basins Continuation"},{"id":377,"pagetitle":"Attractor & Basins Continuation","title":"A new kind of continuation","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#A-new-kind-of-continuation","content":" A new kind of continuation If you have heard before the word \"continuation\", then you are likely aware of the  traditional continuation-based bifurcation analysis (CBA)  offered by many software, such as AUTO, MatCont, and in Julia  BifurcationKit.jl . Here we offer a completely different kind of continuation called  attractors & basins continuation . A direct comparison of the two approaches is not truly possible, because they do different things. The traditional linearized continuation analysis continues the curves of individual fixed points across the joint state-parameter space. The attractor and basins continuation first finds all attractors at all parameter values and then  matches  appropriately similar attractors across different parameters, giving the illusion of continuing them individually. Additionally, the curves of stable fixed points in the joint parameter space is only a small by-product of the attractor basins continuation, and the main information is the basin fractions and how these change in the parameter space. A more detailed comparison between these two fundamentally different approaches in is given in high detail in our  paper ."},{"id":378,"pagetitle":"Attractor & Basins Continuation","title":"Continuation API","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Continuation-API","content":" Continuation API"},{"id":379,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.continuation","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.continuation","content":" Attractors.continuation  —  Function continuation(abc::AttractorsBasinsContinuation, prange, pidx, ics; kwargs...) Find and continue attractors (or feature-based representations of attractors) and the fractions of their basins of attraction across a parameter range.  continuation  is the central function of the framework for global stability analysis illustrated in ( Datseris  et al. , 2023 ). The continuation type  abc  is a subtype of  AttractorsBasinsContinuation  and contains an  AttractorMapper . The mapper contains information on how to find the attractors and basins of a dynamical system. Additional arguments and keyword arguments given when creating  abc  further tune the continuation and how attractors are matched across different parameter values. The basin fractions and the attractors (or some representation of them) are continued across the parameter range  prange , for the parameter of the system with index  pidx  (any index valid in  set_parameter!  can be used). ics  is a 0-argument function generating initial conditions for the dynamical system (as in  basins_fractions ). Possible subtypes of  AttractorsBasinsContinuation  are: RecurrencesFindAndMatch FeaturizeGroupAcrossParameter Return fractions_curves::Vector{Dict{Int, Float64}} . The fractions of basins of attraction.  fractions_curves[i]  is a dictionary mapping attractor IDs to their basin fraction at the  i -th parameter. attractors_info::Vector{Dict{Int, <:Any}} . Information about the attractors.  attractors_info[i]  is a dictionary mapping attractor ID to information about the attractor at the  i -th parameter. The type of information stored depends on the chosen continuation type. Keyword arguments show_progress = true : display a progress bar of the computation. samples_per_parameter = 100 : amount of initial conditions sampled at each parameter from  ics . source"},{"id":380,"pagetitle":"Attractor & Basins Continuation","title":"Recurrences continuation (best)","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Recurrences-continuation-(best)","content":" Recurrences continuation (best)"},{"id":381,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.RecurrencesFindAndMatch","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.RecurrencesFindAndMatch","content":" Attractors.RecurrencesFindAndMatch  —  Type RecurrencesFindAndMatch <: AttractorsBasinsContinuation\nRecurrencesFindAndMatch(mapper::AttractorsViaRecurrences; kwargs...) A method for  continuation  as in ( Datseris  et al. , 2023 ) that is based on the recurrences algorithm for finding attractors ( AttractorsViaRecurrences ) and the \"matching attractors\" functionality offered by  match_continuation! . You can use  RAFM  as an alias. Keyword arguments distance = Centroid(), threshold = Inf, use_vanished = !isinf(threshold) : propagated to  match_continuation! . info_extraction = identity : A function that takes as an input an attractor ( StateSpaceSet ) and outputs whatever information should be stored. It is used to return the  attractors_info  in  continuation . Note that the same attractors that are stored in  attractors_info  are also used to perform the matching in  match_continuation! , hence this keyword should be altered with care. seeds_from_attractor : A function that takes as an input an attractor and returns an iterator of initial conditions to be seeded from the attractor for the next parameter slice. By default, we sample only the first stored point on the attractor. Description At the first parameter slice of the continuation process, attractors and their fractions are found as described in the  AttractorsViaRecurrences  mapper using recurrences in state space. At each subsequent parameter slice, new attractors are found by seeding initial conditions from the previously found attractors and then running these initial conditions through the recurrences algorithm of the  mapper . Seeding initial conditions close to previous attractors accelerates the main bottleneck of  AttractorsViaRecurrences , which is finding the attractors. After the special initial conditions are mapped to attractors, attractor basin fractions are computed by sampling random initial conditions using the provided  sampler  in  continuation ) and mapping them to attractors using the  AttractorsViaRecurrences  mapper. I.e., exactly as in  basins_fractions . Naturally, during this step new attractors may be found, besides those found using the \"seeding from previous attractors\". Once the basins fractions are computed, the parameter is incremented again and we perform the steps as before. This process continues for all parameter values. After all parameters are exhausted, the found attractors (and their fractions) are \"matched\" to the previous ones. I.e., their  IDs are changed , so that attractors that are \"similar\" to those at a previous parameter get assigned the same ID. Matching is done by the  match_continuation!  function and is an  orthogonal  step. This means, that if you don't like the initial outcome of the matching process, you may call  match_continuation!  again on the outcome with different matching-related keywords. You do not need to compute attractors and basins again! Matching is a very sophisticated process that can be understood in detail by reading the docstrings of  match_statespacesets!  first and then  match_continuation! . Here is a short summary: attractors from previous and current parameter are matched based on their \"distance\". By default this is distance in state space, but any measure of \"distance\" may be used, such as the distance between Lyapunov spectra. Matching prioritizes new->old pairs with smallest distance: once these are matched the next available new->old pair with smallest distance is matched, until all new/old attractors have been matched. The  threshold  keyword establishes that attractors with distance >  threshold  do not get matched. Additionally, use  use_vanished = true  if you want to include as matching candidates attractors that have vanished during the continuation process. source"},{"id":382,"pagetitle":"Attractor & Basins Continuation","title":"Matching attractors","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Matching-attractors","content":" Matching attractors"},{"id":383,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.match_statespacesets!","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.match_statespacesets!","content":" Attractors.match_statespacesets!  —  Function match_statespacesets!(a₊::AbstractDict, a₋; distance = Centroid(), threshold = Inf) Given dictionaries  a₊, a₋  mapping IDs to  StateSpaceSet  instances, match the IDs in dictionary  a₊  so that its sets that are the closest to those in dictionary  a₋  get assigned the same key as in  a₋ . Typically the +,- mean after and before some change of parameter of a system. Return the replacement map, a dictionary mapping old keys of  a₊  to the new ones that they were mapped to. You can obtain this map, without modifying the dictionaries, by calling the  replacement_map  function directly. Keyword arguments distance = Centroid() : given to  setsofsets_distances . threshold = Inf : attractors with distance larger than the  threshold  are guaranteed to not be mapped to each other. Description The distance between all possible pairs of sets between the \"old\" and \"new\" containers is computed using  setsofsets_distances  with the keyword  distance .  distance  can be whatever that function accepts, i.e., one of  Centroid, Hausdorff, StrictlyMinimumDistance  or any arbitrary user- provided function that given two sets it returns a positive number (their distance). State space sets are then matched according to this distance. First, all possible pairs (old, new, distance) are sorted according to their distance. The pair with smallest distance is matched. Sets in matched pairs are removed from the matching pool to ensure a unique mapping. Then, the next pair with least remaining distance is matched, and the process repeats until all pairs are exhausted. Additionally, you can provide a  threshold  value. If the distance between two attractors is larger than this  threshold , then it is guaranteed that the attractors will get assigned different key in the dictionary  a₊  (which is the next available integer). source"},{"id":384,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.Centroid","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#StateSpaceSets.Centroid","content":" StateSpaceSets.Centroid  —  Type Centroid(metric = Euclidean()) A distance that can be used in  set_distance . The  Centroid  method returns the distance (according to  metric ) between the  centroids  (a.k.a. centers of mass) of the sets. metric  can be any function that takes in two static vectors are returns a positive definite number to use as a distance (and typically is a  Metric  from Distances.jl). source"},{"id":385,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.Hausdorff","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#StateSpaceSets.Hausdorff","content":" StateSpaceSets.Hausdorff  —  Type Hausdorff(metric = Euclidean()) A distance that can be used in  set_distance . The  Hausdorff distance  is the greatest of all the distances from a point in one set to the closest point in the other set. The distance is calculated with the metric given to  Hausdorff  which defaults to Euclidean. Hausdorff  is 2x slower than  StrictlyMinimumDistance , however it is a proper metric in the space of sets of state space sets. source"},{"id":386,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.StrictlyMinimumDistance","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#StateSpaceSets.StrictlyMinimumDistance","content":" StateSpaceSets.StrictlyMinimumDistance  —  Type StrictlyMinimumDistance([brute = false,] [metric = Euclidean(),]) A distance that can be used in  set_distance . The  StrictlyMinimumDistance  returns the minimum distance of all the distances from a point in one set to the closest point in the other set. The distance is calculated with the given metric. The  brute::Bool  argument switches the computation between a KDTree-based version, or brute force (i.e., calculation of all distances and picking the smallest one). Brute force performs better for datasets that are either large dimensional or have a small amount of points. Deciding a cutting point is not trivial, and is recommended to simply benchmark the  set_distance  function to make a decision. source"},{"id":387,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.replacement_map","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.replacement_map","content":" Attractors.replacement_map  —  Function replacement_map(a₊, a₋; distance = Centroid(), threshold = Inf) → rmap Return a dictionary mapping keys in  a₊  to new keys in  a₋ , as explained in  match_statespacesets! . source"},{"id":388,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.set_distance","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#StateSpaceSets.set_distance","content":" StateSpaceSets.set_distance  —  Function set_distance(ssset1, ssset2 [, distance]) Calculate a distance between two  StateSpaceSet s, i.e., a distance defined between sets of points, as dictated by  distance . Possible  distance  types are: Centroid , which is the default, and 100s of times faster than the rest Hausdorff StrictlyMinimumDistance source"},{"id":389,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.setsofsets_distances","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#StateSpaceSets.setsofsets_distances","content":" StateSpaceSets.setsofsets_distances  —  Function setsofsets_distances(a₊, a₋ [, distance]) → distances Calculate distances between sets of  StateSpaceSet s. Here   a₊, a₋  are containers of  StateSpaceSet s, and the returned distances are dictionaries of distances. Specifically,  distances[i][j]  is the distance of the set in the  i  key of  a₊  to the  j  key of  a₋ . Notice that distances from  a₋  to  a₊  are not computed at all (assumming symmetry in the distance function). The  distance  can be as in  set_distance , or it can be an arbitrary function that takes as input two state space sets and returns any positive-definite number as their \"distance\". source"},{"id":390,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.match_continuation!","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.match_continuation!","content":" Attractors.match_continuation!  —  Function match_continuation!(fractions_curves::Vector{<:Dict}, attractors_info::Vector{<:Dict}; kwargs...) Loop over all entries in the given arguments (which are typically the direct outputs of  continuation ), and match the attractor IDs in both the attractors container and the basins fractions container. This means that we loop over each entry of the vectors (skipping the first), and in each entry we attempt to match the current dictionary keys to the keys of the previous dictionary using  match_statespacesets! . The keywords  distance, threshold  are propagated to  match_statespacesets! . However, there are two unique keywords for  match_continuation! : use_vanished::Bool retract_keys::Bool If  use_vanised = true , then attractors that existed before but have vanished are kept in \"memory\" when it comes to matching: the new attractors are compared to the latest instance of all attractors that have ever existed, and get matched to their closest ones as per  match_statespacesets! . If  false , vanished attractors are ignored. Note that in this case new attractors that cannot be matched to any previous attractors will get an appropriately incremented ID. E.g., if we started with three attractors, and attractor 3 vanished, and at some later parameter value we again have three attractors, the new third attractor will  not  have ID 3, but 4 (i.e., the next available ID). By default  use_vanished = !isinf(threshold)  and since the default value for  threshold  is  Inf ,  use_vanished  is  false . The last keyword is  retract_keys = true  which will \"retract\" keys (i.e., make the integers smaller integers) so that all unique IDs are the 1-incremented positive integers. E.g., if the IDs where 1, 6, 8, they will become 1, 2, 3. The special id -1 is unaffected by this. match_continuation!(attractors_info::Vector{<:Dict}; kwargs...) This is a convenience method that only uses and modifies the state space set dictionary container without the need for a basins fractions container. source"},{"id":391,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.match_basins_ids!","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.match_basins_ids!","content":" Attractors.match_basins_ids!  —  Function match_basins_ids!(b₊::AbstractArray, b₋; threshold = Inf) Similar to  match_statespacesets!  but operate on basin arrays instead (the arrays typically returned by  basins_of_attraction ). This method matches IDs of attractors whose basins of attraction before and after  b₋,b₊  have the most overlap (in pixels). This overlap is normalized in 0-1 (with 1 meaning 100% overlap of pixels). The  threshold  in this case is compared to the inverse of the overlap (so, for  threshold = 2  attractors that have less than 50% overlap get different IDs guaranteed). source"},{"id":392,"pagetitle":"Attractor & Basins Continuation","title":"Aggregating attractors and fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Aggregating-attractors-and-fractions","content":" Aggregating attractors and fractions"},{"id":393,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.aggregate_attractor_fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.aggregate_attractor_fractions","content":" Attractors.aggregate_attractor_fractions  —  Function aggregate_attractor_fractions(\n    fractions_curves, attractors_info, featurizer, group_config [, info_extraction]\n) Aggregate the already-estimated curves of fractions of basins of attraction of similar attractors using the same pipeline used by  GroupingConfig . The most typical application of this function is to transform the output of  RecurrencesFindAndMatch  so that similar attractors, even across parameter space, are grouped into one \"attractor\". Thus, the fractions of their basins are aggregated. You could also use this function to aggregate attractors and their fractions even in a single parameter configuration, i.e., using the output of  basins_fractions . This function is useful in cases where you want the accuracy and performance of  AttractorsViaRecurrences , but you also want the convenience of \"grouping\" similar attractrors like in  AttractorsViaFeaturizing  for presentation or analysis purposes. For example, a high dimensional model of competition dynamics across multispecies may have extreme multistability. After finding this multistability however, one may care about aggregating all attractors into two groups: where a given species is extinct or not. This is the example highlighted in our documentation, in  Extinction of a species in a multistable competition model . Input fractions_curves : a vector of dictionaries mapping labels to basin fractions. attractors_info : a vector of dictionaries mapping labels to attractors. 1st and 2nd argument are exactly like the return values of  continuation  with  RecurrencesFindAndMatch  (or, they can be the return of  basins_fractions ). featurizer : a 1-argument function to map an attractor into an appropriate feature to be grouped later. Features expected by  GroupingConfig  are  SVector . group_config : a subtype of  GroupingConfig . info_extraction : a function accepting a vector of features and returning a description of the features. I.e., exactly as in  FeaturizeGroupAcrossParameter . The 5th argument is optional and defaults to the centroid of the features. Return aggregated_fractions : same as  fractions_curves  but now contains the fractions of the aggregated attractors. aggregated_info : dictionary mapping the new labels of  aggregated_fractions  to the extracted information using  info_extraction . Clustering attractors directly (this is rather advanced) You may also use the DBSCAN clustering approach here to group attractors based on their state space distance (the  set_distance ) by making a distance matrix as expected by the DBSCAN implementation. For this, use  identity  as  featurizer , and choose  GroupViaClustering  as the  group_config  with  clust_distance_metric = set_distance  and provide a numerical value for  optimal_radius_method  when initializing the  GroupViaClustering , and also, for the  info_extraction  argument, you now need to provide a function that expects a  vector of  StateSpaceSet s  and outputs a descriptor. E.g.,  info_extraction = vector -> mean(mean(x) for x in vector) . source"},{"id":394,"pagetitle":"Attractor & Basins Continuation","title":"Grouping continuation","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Grouping-continuation","content":" Grouping continuation"},{"id":395,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.FeaturizeGroupAcrossParameter","ref":"/DynamicalSystemsDocs.jl/attractors/stable/continuation/#Attractors.FeaturizeGroupAcrossParameter","content":" Attractors.FeaturizeGroupAcrossParameter  —  Type FeaturizeGroupAcrossParameter <: AttractorsBasinsContinuation\nFeaturizeGroupAcrossParameter(mapper::AttractorsViaFeaturizing; kwargs...) A method for  continuation . It uses the featurizing approach discussed in  AttractorsViaFeaturizing  and hence requires an instance of that mapper as an input. When used in  continuation , features are extracted and then grouped across a parameter range. Said differently, all features of all initial conditions across all parameter values are put into the same \"pool\" and then grouped as dictated by the  group_config  of the mapper. After the grouping is finished the feature label fractions are distributed to each parameter value they came from. Keyword arguments info_extraction::Function  a function that takes as an input a vector of feature-vectors (corresponding to a cluster) and returns a description of the cluster. By default, the centroid of the cluster is used. par_weight = 0 : See below the section on MCBB. MCBB special version If the chosen grouping method is  GroupViaClustering , the additional keyword  par_weight::Real  can be used. If it is ≠ 0, the distance matrix between features obtains an extra weight that is proportional to the distance  par_weight*|p[i] - p[j]|  between the parameters used when extracting features. The range of parameters is normalized to 0-1 such that the largest distance in the parameter space is 1. The normalization is done because the feature space is also (by default) normalized to 0-1. This version of the algorithm is the original \"MCBB\" continuation method described in ( Gelbrecht  et al. , 2020 ), besides the improvements of clustering accuracy and performance done by the developer team of Attractors.jl. source"},{"id":398,"pagetitle":"Examples for Attractors.jl","title":"Examples for Attractors.jl","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Examples-for-Attractors.jl","content":" Examples for Attractors.jl Note that the examples utilize some convenience plotting functions offered by Attractors.jl which come into scope when using  Makie  (or any of its backends such as  CairoMakie ), see the  visualization utilities  for more."},{"id":399,"pagetitle":"Examples for Attractors.jl","title":"Newton's fractal (basins of 2D map)","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Newton's-fractal-(basins-of-2D-map)","content":" Newton's fractal (basins of 2D map) using Attractors\nfunction newton_map(z, p, n)\n    z1 = z[1] + im*z[2]\n    dz1 = newton_f(z1, p[1])/newton_df(z1, p[1])\n    z1 = z1 - dz1\n    return SVector(real(z1), imag(z1))\nend\nnewton_f(x, p) = x^p - 1\nnewton_df(x, p)= p*x^(p-1)\n\nds = DiscreteDynamicalSystem(newton_map, [0.1, 0.2], [3.0])\nxg = yg = range(-1.5, 1.5; length = 400)\ngrid = (xg, yg)\n# Use non-sparse for using `basins_of_attraction`\nmapper_newton = AttractorsViaRecurrences(ds, grid;\n    sparse = false, consecutive_lost_steps = 1000\n)\nbasins, attractors = basins_of_attraction(mapper_newton; show_progress = false)\nbasins 400×400 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3  …  3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3 attractors Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points Now let's plot this as a heatmap, and on top of the heatmap, let's scatter plot the attractors. We do this in one step by utilizing one of the pre-defined plotting functions offered by Attractors.jl using CairoMakie\nfig = heatmap_basins_attractors(grid, basins, attractors) Instead of computing the full basins, we could get only the fractions of the basins of attractions using  basins_fractions , which is typically the more useful thing to do in a high dimensional system. In such cases it is also typically more useful to define a sampler that generates initial conditions on the fly instead of pre-defining some initial conditions (as is done in  basins_of_attraction . This is simple to do: sampler, = statespace_sampler(grid)\n\nbasins = basins_fractions(mapper_newton, sampler) Dict{Int64, Float64} with 2 entries:\n  0 => 0.336\n  1 => 0.664 in this case, to also get the attractors we simply extract them from the underlying storage of the mapper: attractors = extract_attractors(mapper_newton) Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points"},{"id":400,"pagetitle":"Examples for Attractors.jl","title":"Shading basins according to convergence time","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Shading-basins-according-to-convergence-time","content":" Shading basins according to convergence time Continuing from above, we can utilize the  convergence_and_basins_of_attraction  function, and the  shaded_basins_heatmap  plotting utility function, to shade the basins of attraction based on the convergence time, with lighter colors indicating faster convergence to the attractor. mapper_newton = AttractorsViaRecurrences(ds, grid;\n    sparse = false, consecutive_lost_steps = 1000\n)\n\nbasins, attractors, iterations = convergence_and_basins_of_attraction(\n    mapper_newton, grid; show_progress = false\n)\n\nshaded_basins_heatmap(grid, basins, attractors, iterations)"},{"id":401,"pagetitle":"Examples for Attractors.jl","title":"Minimal Fatal Shock","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Minimal-Fatal-Shock","content":" Minimal Fatal Shock Here we find the Minimal Fatal Shock (MFS, see  minimal_fatal_shock ) for the attractors (i.e., fixed points) of Newton's fractal shocks = Dict()\nalgo_bb = Attractors.MFSBlackBoxOptim()\nfor atr in values(attractors)\n    u0 = atr[1]\n    shocks[u0] = minimal_fatal_shock(mapper_newton, u0, (-1.5,1.5), algo_bb)\nend\nshocks Dict{Any, Any} with 3 entries:\n  [-0.5, -0.866025] => [0.592005, 0.190975]\n  [1.0, 0.0]        => [-0.464867, -0.413587]\n  [-0.5, 0.866025]  => [0.592005, -0.190975] To visualize results we can make use of previously defined heatmap ax =  content(fig[1,1])\nfor (atr, shock) in shocks\n    lines!(ax, [atr, atr + shock]; color = :orange, linewidth = 3)\nend\nfig"},{"id":402,"pagetitle":"Examples for Attractors.jl","title":"Fractality of 2D basins of the (4D) magnetic pendulum","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Fractality-of-2D-basins-of-the-(4D)-magnetic-pendulum","content":" Fractality of 2D basins of the (4D) magnetic pendulum In this section we will calculate the basins of attraction of the four-dimensional magnetic pendulum. We know that the attractors of this system are all individual fixed points on the (x, y) plane so we will only compute the basins there. We can also use this opportunity to highlight a different method, the  AttractorsViaProximity  which works when we already know where the attractors are. Furthermore we will also use a  ProjectedDynamicalSystem  to project the 4D system onto a 2D plane, saving a lot of computational time!"},{"id":403,"pagetitle":"Examples for Attractors.jl","title":"Computing the basins","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Computing-the-basins","content":" Computing the basins First we need to load in the magnetic pendulum from the predefined dynamical systems library using Attractors, CairoMakie\nusing PredefinedDynamicalSystems\nds = PredefinedDynamicalSystems.magnetic_pendulum(d=0.2, α=0.2, ω=0.8, N=3) 4-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  MagneticPendulum\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    PredefinedDynamicalSystems.MagneticPendulumParams([1.0, 1.0, 1.0], 0.2, 0.2, 0.8)\n time:          0.0\n state:         [0.7094575840693688, 0.704748136859158, 0.0, 0.0]\n Then, we create a projected system on the x-y plane psys = ProjectedDynamicalSystem(ds, [1, 2], [0.0, 0.0]) 2-dimensional ProjectedDynamicalSystem\n deterministic:  true\n discrete time:  false\n in-place:       false\n dynamic rule:   MagneticPendulum\n projection:     [1, 2]\n complete state: [0.0, 0.0]\n parameters:     PredefinedDynamicalSystems.MagneticPendulumParams([1.0, 1.0, 1.0], 0.2, 0.2, 0.8)\n time:           0.0\n state:          [0.7094575840693688, 0.704748136859158]\n For this systems we know the attractors are close to the magnet positions. The positions can be obtained from the equations of the system, provided that one has seen the source code (not displayed here), like so: attractors = Dict(i => StateSpaceSet([dynamic_rule(ds).magnets[i]]) for i in 1:3) Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points and then create a mapper = AttractorsViaProximity(psys, attractors) AttractorsViaProximity\n system:      ProjectedDynamicalSystem\n ε:           0.8660254037844386\n Δt:          1\n Ttr:         100\n attractors:  Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n                2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n                3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n                1 => 2-dimensional StateSpaceSet{Float64} with 1 points\n and as before, get the basins of attraction xg = yg = range(-4, 4; length = 201)\ngrid = (xg, yg)\nbasins, = basins_of_attraction(mapper, grid; show_progress = false)\n\nheatmap_basins_attractors(grid, basins, attractors)"},{"id":404,"pagetitle":"Examples for Attractors.jl","title":"Computing the uncertainty exponent","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Computing-the-uncertainty-exponent","content":" Computing the uncertainty exponent Let's now calculate the  uncertainty_exponent  for this system as well. The calculation is straightforward: using CairoMakie\nε, f_ε, α = uncertainty_exponent(basins)\nfig, ax = lines(log.(ε), log.(f_ε))\nax.title = \"α = $(round(α; digits=3))\"\nfig The actual uncertainty exponent is the slope of the curve (α) and indeed we get an exponent near 0 as we know a-priory the basins have fractal boundaries for the magnetic pendulum."},{"id":405,"pagetitle":"Examples for Attractors.jl","title":"Computing the tipping probabilities","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Computing-the-tipping-probabilities","content":" Computing the tipping probabilities We will compute the tipping probabilities using the magnetic pendulum's example as the \"before\" state. For the \"after\" state we will change the  γ  parameter of the third magnet to be so small, its basin of attraction will virtually disappear. As we don't know  when  the basin of the third magnet will disappear, we switch the attractor finding algorithm back to  AttractorsViaRecurrences . set_parameter!(psys, :γs, [1.0, 1.0, 0.1])\nmapper = AttractorsViaRecurrences(psys, (xg, yg); Δt = 1)\nbasins_after, attractors_after = basins_of_attraction(\n    mapper, (xg, yg); show_progress = false\n)\n# matching attractors is important!\nrmap = match_statespacesets!(attractors_after, attractors)\n# Don't forget to update the labels of the basins as well!\nreplace!(basins_after, rmap...)\n\n# now plot\nheatmap_basins_attractors(grid, basins_after, attractors_after) And let's compute the tipping \"probabilities\": P = tipping_probabilities(basins, basins_after) 3×2 Matrix{Float64}:\n 0.503287  0.496713\n 0.448887  0.551113\n 0.551102  0.448898 As you can see  P  has size 3×2, as after the change only 2 attractors have been identified in the system (3 still exist but our state space discretization isn't fine enough to find the 3rd because it has such a small basin). Also, the first row of  P  is 50% probability to each other magnet, as it should be due to the system's symmetry."},{"id":406,"pagetitle":"Examples for Attractors.jl","title":"3D basins via recurrences","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#3D-basins-via-recurrences","content":" 3D basins via recurrences To showcase the true power of  AttractorsViaRecurrences  we need to use a system whose attractors span higher-dimensional space. An example is using Attractors\nusing PredefinedDynamicalSystems\nds = PredefinedDynamicalSystems.thomas_cyclical(b = 0.1665) 3-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  thomas_rule\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    [0.1665]\n time:          0.0\n state:         [1.0, 0.0, 0.0]\n which, for this parameter, contains 3 coexisting attractors which are entangled periodic orbits that span across all three dimensions. To compute the basins we define a three-dimensional grid and call on it  basins_of_attraction . # This computation takes about an hour\nxg = yg = zg = range(-6.0, 6.0; length = 251)\nmapper = AttractorsViaRecurrences(ds, (xg, yg, zg); sparse = false)\nbasins, attractors = basins_of_attraction(mapper)\nattractors Dict{Int16, StateSpaceSet{3, Float64}} with 5 entries:\n  5 => 3-dimensional StateSpaceSet{Float64} with 1 points\n  4 => 3-dimensional StateSpaceSet{Float64} with 379 points\n  6 => 3-dimensional StateSpaceSet{Float64} with 1 points\n  2 => 3-dimensional StateSpaceSet{Float64} with 538 points\n  3 => 3-dimensional StateSpaceSet{Float64} with 537 points\n  1 => 3-dimensional StateSpaceSet{Float64} with 1 points Note: the reason we have 6 attractors here is because the algorithm also finds 3 unstable fixed points and labels them as attractors. This happens because we have provided initial conditions on the grid  xg, yg, zg  that start exactly on the unstable fixed points, and hence stay there forever, and hence are perceived as attractors by the recurrence algorithm. As you will see in the video below, they don't have any basin fractions The basins of attraction are very complicated. We can try to visualize them by animating the 2D slices at each z value, to obtain: Then, we visualize the attractors to obtain: In the animation above, the scattered points are the attractor values the function  AttractorsViaRecurrences  found by itself. Of course, for the periodic orbits these points are incomplete. Once the function's logic understood we are on an attractor, it stops computing. However, we also simulated lines, by evolving initial conditions colored appropriately with the basins output. The animation was produced with the code: using GLMakie\nfig = Figure()\ndisplay(fig)\nax = fig[1,1] = Axis3(fig; title = \"found attractors\")\ncmap = cgrad(:dense, 6; categorical = true)\n\nfor i in keys(attractors)\n    tr = attractors[i]\n    markersize = length(attractors[i]) > 10 ? 2000 : 6000\n    marker = length(attractors[i]) > 10 ? :circle : :rect\n    scatter!(ax, columns(tr)...; markersize, marker, transparency = true, color = cmap[i])\n    j = findfirst(isequal(i), bsn)\n    x = xg[j[1]]\n    y = yg[j[2]]\n    z = zg[j[3]]\n    tr = trajectory(ds, 100, SVector(x,y,z); Ttr = 100)\n    lines!(ax, columns(tr)...; linewidth = 1.0, color = cmap[i])\nend\n\na = range(0, 2π; length = 200) .+ π/4\n\nrecord(fig, \"cyclical_attractors.mp4\", 1:length(a)) do i\n    ax.azimuth = a[i]\nend"},{"id":407,"pagetitle":"Examples for Attractors.jl","title":"Basins of attraction of a Poincaré map","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Basins-of-attraction-of-a-Poincaré-map","content":" Basins of attraction of a Poincaré map PoincareMap  is just another discrete time dynamical system within the DynamicalSystems.jl ecosystem. With respect to Attractors.jl functionality, there is nothing special about Poincaré maps. You simply initialize one use it like any other type of system. Let's continue from the above example  of the Thomas cyclical system using Attractors\nusing PredefinedDynamicalSystems\nds = PredefinedDynamicalSystems.thomas_cyclical(b = 0.1665); 3-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  thomas_rule\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    [0.1665]\n time:          0.0\n state:         [1.0, 0.0, 0.0]\n The three limit cycles attractors we have above become fixed points in the Poincaré map (for appropriately chosen hyperplanes). Since we already know the 3D structure of the basins, we can see that an appropriately chosen hyperplane is just the plane  z = 0 . Hence, we define a Poincaré map on this plane: plane = (3, 0.0)\npmap = PoincareMap(ds, plane) 3-dimensional PoincareMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  thomas_rule\n hyperplane:    (3, 0.0)\n crossing time: 0.0\n parameters:    [0.1665]\n time:          0\n state:         [1.0, 0.0, 0.0]\n We define the same grid as before, but now only we only use the x-y coordinates. This is because we can utilize the special  reinit!  method of the  PoincareMap , that allows us to initialize a new state directly on the hyperplane (and then the remaining variable of the dynamical system takes its value from the hyperplane itself). xg = yg = range(-6.0, 6.0; length = 250)\ngrid = (xg, yg)\nmapper = AttractorsViaRecurrences(pmap, grid; sparse = false) AttractorsViaRecurrences\n system:      PoincareMap\n grid:        (-6.0:0.04819277108433735:6.0, -6.0:0.04819277108433735:6.0)\n attractors:  Dict{Int64, StateSpaceSet{2, Float64}}()\n All that is left to do is to call  basins_of_attraction : basins, attractors = basins_of_attraction(mapper; show_progress = false); ([1 1 … 2 2; 1 1 … 2 2; … ; 2 2 … 1 1; 2 2 … 1 1], Dict{Int64, StateSpaceSet{2, Float64}}(2 => 2-dimensional StateSpaceSet{Float64} with 1 points, 3 => 2-dimensional StateSpaceSet{Float64} with 5 points, 1 => 2-dimensional StateSpaceSet{Float64} with 1 points)) heatmap_basins_attractors(grid, basins, attractors) just like in the example above, there is a fourth attractor with 0 basin fraction. This is an unstable fixed point, and exists exactly because we provided a grid with the unstable fixed point exactly on this grid"},{"id":408,"pagetitle":"Examples for Attractors.jl","title":"Irregular grid for  AttractorsViaRecurrences","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Irregular-grid-for-AttractorsViaRecurrences","content":" Irregular grid for  AttractorsViaRecurrences It is possible to provide an irregularly spaced grid to  AttractorsViaRecurrences . This can make algorithm performance better for continuous time systems where the state space flow has significantly different speed in some state space regions versus others. In the following example the dynamical system has only one attractor: a limit cycle. However, near the origin (0, 0) the timescale of the dynamics becomes very slow. As the trajectory is stuck there for quite a while, the recurrences algorithm may identify this region as an \"attractor\" (incorrectly). The solutions vary and can be to increase drastically the max time checks for finding attractors, or making the grid much more fine. Alternatively, one can provide a grid that is only more fine near the origin and not fine elsewhere. The example below highlights that for rather coarse settings of grid and convergence thresholds, using a grid that is finer near (0, 0) gives correct results: using Attractors, CairoMakie\n\nfunction predator_prey_fastslow(u, p, t)\n    α, γ, ϵ, ν, h, K, m = p\n    N, P = u\n    du1 = α*N*(1 - N/K) - γ*N*P / (N+h)\n    du2 = ϵ*(ν*γ*N*P/(N+h) - m*P)\n    return SVector(du1, du2)\nend\nγ = 2.5\nh = 1\nν = 0.5\nm = 0.4\nϵ = 1.0\nα = 0.8\nK = 15\nu0 = rand(2)\np0 = [α, γ, ϵ, ν, h, K, m]\nds = CoupledODEs(predator_prey_fastslow, u0, p0)\n\nfig = Figure()\nax = Axis(fig[1,1])\n\n# when pow > 1, the grid is finer close to zero\nfor pow in (1, 2)\n    xg = yg = range(0, 18.0^(1/pow); length = 200).^pow\n    mapper = AttractorsViaRecurrences(ds, (xg, yg);\n        Dt = 0.1, sparse = true,\n        consecutive_recurrences = 10, attractor_locate_steps = 10,\n        maximum_iterations = 1000,\n    )\n\n    # Find attractor and its fraction (fraction is always 1 here)\n    sampler, _ = statespace_sampler(HRectangle(zeros(2), fill(18.0, 2)), 42)\n    fractions = basins_fractions(mapper, sampler; N = 100, show_progress = false)\n    attractors = extract_attractors(mapper)\n    scatter!(ax, vec(attractors[1]); markersize = 16/pow, label = \"pow = $(pow)\")\nend\n\naxislegend(ax)\n\nfig"},{"id":409,"pagetitle":"Examples for Attractors.jl","title":"Subdivision Based Grid for  AttractorsViaRecurrences","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Subdivision-Based-Grid-for-AttractorsViaRecurrences","content":" Subdivision Based Grid for  AttractorsViaRecurrences To achieve even better results for this kind of problematic systems than with previuosly introduced  Irregular Grids   we provide a functionality to construct  Subdivision Based Grids  in which one can obtain more coarse or dense structure not only along some axis but for a specific regions where the state space flow has significantly different speed.  subdivision_based_grid  enables automatic evaluation of velocity vectors for regions of originally user specified grid to further treat those areas as having more dense or coarse structure than others. using Attractors, CairoMakie\n\nfunction predator_prey_fastslow(u, p, t)\n    α, γ, ϵ, ν, h, K, m = p\n    N, P = u\n    du1 = α*N*(1 - N/K) - γ*N*P / (N+h)\n    du2 = ϵ*(ν*γ*N*P/(N+h) - m*P)\nreturn SVector(du1, du2)\nend\nγ = 2.5\nh = 1\nν = 0.5\nm = 0.4\nϵ = 1.0\nα = 0.8\nK = 15\nu0 = rand(2)\np0 = [α, γ, ϵ, ν, h, K, m]\nds = CoupledODEs(predator_prey_fastslow, u0, p0)\n\nxg = yg = range(0, 18, length = 30)\n# Construct `Subdivision Based Grid`\ngrid = subdivision_based_grid(ds, (xg, yg))\ngrid.lvl_array 30×30 Matrix{Int64}:\n 4  4  4  4  4  4  4  4  4  4  4  4  4  …  3  3  3  3  3  3  3  3  3  3  3  3\n 4  4  4  4  4  4  4  4  3  3  3  3  3     2  2  2  2  2  2  2  2  2  2  1  1\n 4  4  4  4  4  4  3  3  3  3  2  2  2     2  1  1  1  1  1  1  1  1  1  1  1\n 4  4  4  4  4  3  3  3  3  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  1\n 4  4  4  4  4  3  3  3  3  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  0\n 4  4  4  4  4  3  3  3  2  2  2  2  2  …  1  1  1  1  1  1  1  1  1  1  0  0\n 4  4  4  4  4  3  3  3  2  2  2  2  2     1  1  1  1  1  1  1  1  1  0  0  0\n 4  4  4  4  4  3  3  3  2  2  2  2  2     1  1  1  1  1  1  1  1  0  0  0  0\n 4  4  4  4  4  3  3  3  2  2  2  2  2     1  1  1  1  1  1  1  1  0  0  0  0\n 4  4  4  4  4  3  3  3  2  2  2  2  2     1  1  1  1  1  1  1  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n 4  4  4  4  3  3  3  2  2  2  2  2  1     1  1  1  1  0  0  0  0  0  0  0  0\n 4  4  4  4  3  3  2  2  2  2  2  1  1     1  1  1  1  0  0  0  0  0  0  0  0\n 4  4  4  3  3  3  2  2  2  2  2  1  1     1  1  1  0  0  0  0  0  0  0  0  0\n 4  4  4  3  3  3  2  2  2  2  2  1  1     1  1  1  0  0  0  0  0  0  0  0  0\n 4  4  4  3  3  2  2  2  2  2  1  1  1  …  1  1  1  0  0  0  0  0  0  0  0  0\n 4  4  3  3  3  2  2  2  2  2  1  1  1     1  1  0  0  0  0  0  0  0  0  0  0\n 4  4  3  3  3  2  2  2  2  2  1  1  1     1  1  0  0  0  0  0  0  0  0  0  0\n 4  4  3  3  2  2  2  2  2  1  1  1  1     1  0  0  0  0  0  0  0  0  0  0  0\n 4  3  3  3  2  2  2  2  2  1  1  1  1     1  0  0  0  0  0  0  0  0  0  0  0 The constructed array corresponds to levels of discretization for specific regions of the grid as a powers of 2, meaning that if area index is assigned to be  3 , for example, the algorithm will treat the region as one being  2^3 = 8  times more dense than originally user provided grid  (xg, yg) . Now upon the construction of this structure, one can simply pass it into mapper function as usual. fig = Figure()\nax = Axis(fig[1,1])\n# passing SubdivisionBasedGrid into mapper\nmapper = AttractorsViaRecurrences(ds, grid;\n        Dt = 0.1, sparse = true,\n        consecutive_recurrences = 10, attractor_locate_steps = 10,\n        maximum_iterations = 1000,\n    )\n\n# Find attractor and its fraction (fraction is always 1 here)\nsampler, _ = statespace_sampler(HRectangle(zeros(2), fill(18.0, 2)), 42)\nfractions = basins_fractions(mapper, sampler; N = 100, show_progress = false)\nattractors_SBD = extract_attractors(mapper)\nscatter!(ax, vec(attractors_SBD[1]); label = \"SubdivisionBasedGrid\")\n\n\n# to compare the results we also construct RegularGrid of same length here\nxg = yg = range(0, 18, length = 30)\nmapper = AttractorsViaRecurrences(ds, (xg, yg);\n        Dt = 0.1, sparse = true,\n        consecutive_recurrences = 10, attractor_locate_steps = 10,\n        maximum_iterations = 1000,\n    )\n\nsampler, _ = statespace_sampler(HRectangle(zeros(2), fill(18.0, 2)), 42)\nfractions = basins_fractions(mapper, sampler; N = 100, show_progress = false)\nattractors_reg = extract_attractors(mapper)\nscatter!(ax, vec(attractors_reg[1]); label = \"RegularGrid\")\n\naxislegend(ax)\nfig"},{"id":410,"pagetitle":"Examples for Attractors.jl","title":"Basin fractions continuation in the magnetic pendulum","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Basin-fractions-continuation-in-the-magnetic-pendulum","content":" Basin fractions continuation in the magnetic pendulum Perhaps the simplest application of  continuation  is to produce a plot of how the fractions of attractors change as we continuously change the parameter we changed above to calculate tipping probabilities."},{"id":411,"pagetitle":"Examples for Attractors.jl","title":"Computing the fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Computing-the-fractions","content":" Computing the fractions This is what the following code does: # initialize projected magnetic pendulum\nusing Attractors, PredefinedDynamicalSystems\nusing Random: Xoshiro\nds = Systems.magnetic_pendulum(; d = 0.3, α = 0.2, ω = 0.5)\nxg = yg = range(-3, 3; length = 101)\nds = ProjectedDynamicalSystem(ds, 1:2, [0.0, 0.0])\n# Choose a mapper via recurrences\nmapper = AttractorsViaRecurrences(ds, (xg, yg); Δt = 1.0)\n# What parameter to change, over what range\nγγ = range(1, 0; length = 101)\nprange = [[1, 1, γ] for γ in γγ]\npidx = :γs\n# important to make a sampler that respects the symmetry of the system\nregion = HSphere(3.0, 2)\nsampler, = statespace_sampler(region, 1234)\n# continue attractors and basins:\n# `Inf` threshold fits here, as attractors move smoothly in parameter space\nrsc = RecurrencesFindAndMatch(mapper; threshold = Inf)\nfractions_curves, attractors_info = continuation(\n    rsc, prange, pidx, sampler;\n    show_progress = false, samples_per_parameter = 100\n)\n# Show some characteristic fractions:\nfractions_curves[[1, 50, 101]] 3-element Vector{Dict{Int64, Float64}}:\n Dict(2 => 0.32, 3 => 0.3, 1 => 0.38)\n Dict(2 => 0.47572815533980584, 3 => 0.4174757281553398, 1 => 0.10679611650485436)\n Dict(2 => 0.39215686274509803, 3 => 0.6078431372549019)"},{"id":412,"pagetitle":"Examples for Attractors.jl","title":"Plotting the fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Plotting-the-fractions","content":" Plotting the fractions We visualize them using a predefined function that you can find in  docs/basins_plotting.jl # careful; `prange` isn't a vector of reals!\nplot_basins_curves(fractions_curves, γγ)"},{"id":413,"pagetitle":"Examples for Attractors.jl","title":"Fixed point curves","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Fixed-point-curves","content":" Fixed point curves A by-product of the analysis is that we can obtain the curves of the position of fixed points for free. However, only the stable branches can be obtained! using CairoMakie\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"\\gamma_3\", ylabel = \"fixed point\")\n# choose how to go from attractor to real number representation\nfunction real_number_repr(attractor)\n    p = attractor[1]\n    return (p[1] + p[2])/2\nend\n\nfor (i, γ) in enumerate(γγ)\n    for (k, attractor) in attractors_info[i]\n        scatter!(ax, γ, real_number_repr(attractor); color = Cycled(k))\n    end\nend\nfig as you can see, two of the three fixed points, and their stability, do not depend at all on the parameter value, since this parameter value tunes the magnetic strength of only the third magnet. Nevertheless, the  fractions of basin of attraction  of all attractors depend strongly on the parameter. This is a simple example that highlights excellently how this new approach we propose here should be used even if one has already done a standard linearized bifurcation analysis."},{"id":414,"pagetitle":"Examples for Attractors.jl","title":"Extinction of a species in a multistable competition model","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Extinction-of-a-species-in-a-multistable-competition-model","content":" Extinction of a species in a multistable competition model In this advanced example we utilize both  RecurrencesFindAndMatch  and  aggregate_attractor_fractions  in analyzing species extinction in a dynamical model of competition between multiple species. The final goal is to show the percentage of how much of the state space leads to the extinction or not of a pre-determined species, as we vary a parameter. The model however displays extreme multistability, a feature we want to measure and preserve before aggregating information into \"extinct or not\". To measure and preserve this we will apply  RecurrencesFindAndMatch  as-is first. Then we can aggregate information. First we have using Attractors, OrdinaryDiffEq\nusing PredefinedDynamicalSystems\nusing Random: Xoshiro\n# arguments to algorithms\nsamples_per_parameter = 1000\ntotal_parameter_values = 101\ndiffeq = (alg = Vern9(), reltol = 1e-9, abstol = 1e-9, maxiters = Inf)\nrecurrences_kwargs = (; Δt= 1.0, consecutive_recurrences=9, diffeq);\n# initialize dynamical system and sampler\nds = PredefinedDynamicalSystems.multispecies_competition() # 8-dimensional\nds = CoupledODEs(ODEProblem(ds), diffeq)\n# define grid in state space\nxg = range(0, 60; length = 300)\ngrid = ntuple(x -> xg, 8)\nprange = range(0.2, 0.3; length = total_parameter_values)\npidx = :D\nsampler, = statespace_sampler(grid, 1234)\n# initialize mapper\nmapper = AttractorsViaRecurrences(ds, grid; recurrences_kwargs...)\n# perform continuation of attractors and their basins\ncontinuation = RecurrencesFindAndMatch(mapper; threshold = Inf)\nfractions_curves, attractors_info = continuation(\n    continuation, prange, pidx, sampler;\n    show_progress = true, samples_per_parameter\n);\nplot_basins_curves(fractions_curves, prange; separatorwidth = 1) this example is not actually run when building the docs, because it takes about 60 minutes to complete depending on the computer; we load precomputed results instead As you can see, the system has extreme multistability with 64 unique attractors (according to the default matching behavior in  RecurrencesFindAndMatch ; a stricter matching with less than  Inf  threshold would generate more \"distinct\" attractors). One could also isolate a specific parameter slice, and do the same as what we do in the  Fractality of 2D basins of the (4D) magnetic pendulum  example, to prove that the basin boundaries are fractal, thereby indeed confirming the paper title \"Fundamental Unpredictability\". Regardless, we now want to continue our analysis to provide a figure similar to the above but only with two colors: fractions of attractors where a species is extinct or not. Here's how: species = 3 # species we care about its existence\n\nfeaturizer = (A) -> begin\n    i = isextinct(A, species)\n    return SVector(Int32(i))\nend\nisextinct(A, idx = unitidxs) = all(a -> a <= 1e-2, A[:, idx])\n\n# `minneighbors = 1` is crucial for grouping single attractors\ngroupingconfig = GroupViaClustering(; min_neighbors=1, optimal_radius_method=0.5)\n\naggregated_fractions, aggregated_info = aggregate_attractor_fractions(\n    fractions_curves, attractors_info, featurizer, groupingconfig\n)\n\nplot_basins_curves(aggregated_fractions, prange;\n    separatorwidth = 1, colors = [\"green\", \"black\"],\n    labels = Dict(1 => \"extinct\", 2 => \"alive\"),\n) (in hindsight, the labels are reversed; attractor 1 is the alive one, but oh well)"},{"id":415,"pagetitle":"Examples for Attractors.jl","title":"Trivial featurizing and grouping for basins fractions","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Trivial-featurizing-and-grouping-for-basins-fractions","content":" Trivial featurizing and grouping for basins fractions This is a rather trivial example showcasing the usage of  AttractorsViaFeaturizing . Let us use once again the magnetic pendulum example. For it, we have a really good idea of what features will uniquely describe each attractor: the last points of a trajectory (which should be very close to the magnetic the trajectory converged to). To provide this information to the  AttractorsViaFeaturizing  we just create a julia function that returns this last point using Attractors\nusing PredefinedDynamicalSystems\n\nds = Systems.magnetic_pendulum(d=0.2, α=0.2, ω=0.8, N=3)\npsys = ProjectedDynamicalSystem(ds, [1, 2], [0.0, 0.0])\n\nfunction featurizer(X, t)\n    return X[end]\nend\n\nmapper = AttractorsViaFeaturizing(psys, featurizer; Ttr = 200, T = 1)\n\nxg = yg = range(-4, 4; length = 101)\n\nregion = HRectangle([-4, 4], [4, 4])\nsampler, = statespace_sampler(region)\n\nfs = basins_fractions(mapper, sampler; show_progress = false) Dict{Int64, Float64} with 3 entries:\n  2 => 0.393\n  3 => 0.248\n  1 => 0.359 As expected, the fractions are each about 1/3 due to the system symmetry."},{"id":416,"pagetitle":"Examples for Attractors.jl","title":"Featurizing and grouping across parameters (MCBB)","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Featurizing-and-grouping-across-parameters-(MCBB)","content":" Featurizing and grouping across parameters (MCBB) Here we showcase the example of the Monte Carlo Basin Bifurcation publication. For this, we will use  FeaturizeGroupAcrossParameter  while also providing a  par_weight = 1  keyword. However, we will not use a network of 2nd order Kuramoto oscillators (as done in the paper by Gelbrecht et al.) because it is too costly to run on CI. Instead, we will use \"dummy\" system which we know analytically the attractors and how they behave versus a parameter. the Henon map and try to group attractors into period 1 (fixed point), period 3, and divergence to infinity. We will also use a pre-determined optimal radius for clustering, as we know a-priory the expected distances of features in feature space (due to the contrived form of the  featurizer  function below). using Attractors, Random\n\nfunction dumb_map(dz, z, p, n)\n    x, y = z\n    r = p[1]\n    if r < 0.5\n        dz[1] = dz[2] = 0.0\n    else\n        if x > 0\n            dz[1] = r\n            dz[2] = r\n        else\n            dz[1] = -r\n            dz[2] = -r\n        end\n    end\n    return\nend\n\nr = 3.833\nds = DiscreteDynamicalSystem(dumb_map, [0., 0.], [r]) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      true\n dynamic rule:  dumb_map\n parameters:    [3.833]\n time:          0\n state:         [0.0, 0.0]\n sampler, = statespace_sampler(HRectangle([-3.0, -3.0], [3.0, 3.0]), 1234)\n\nrrange = range(0, 2; length = 21)\nridx = 1\n\nfeaturizer(a, t) = a[end]\nclusterspecs = GroupViaClustering(optimal_radius_method = \"silhouettes\", max_used_features = 200)\nmapper = AttractorsViaFeaturizing(ds, featurizer, clusterspecs; T = 20, threaded = true)\ngap = FeaturizeGroupAcrossParameter(mapper; par_weight = 1.0)\nfractions_curves, clusters_info = continuation(\n    gap, rrange, ridx, sampler; show_progress = false\n)\nfractions_curves 21-element Vector{Dict{Int64, Float64}}:\n Dict(1 => 1.0)\n Dict(2 => 1.0)\n Dict(3 => 1.0)\n Dict(4 => 1.0)\n Dict(5 => 1.0)\n Dict(6 => 0.47, 7 => 0.53)\n Dict(9 => 0.52, 8 => 0.48)\n Dict(11 => 0.44, 10 => 0.56)\n Dict(13 => 0.56, 12 => 0.44)\n Dict(15 => 0.46, 14 => 0.54)\n ⋮\n Dict(20 => 0.48, 21 => 0.52)\n Dict(22 => 0.54, 23 => 0.46)\n Dict(25 => 0.47, 24 => 0.53)\n Dict(27 => 0.6, 26 => 0.4)\n Dict(29 => 0.51, 28 => 0.49)\n Dict(31 => 0.49, 30 => 0.51)\n Dict(32 => 0.46, 33 => 0.54)\n Dict(34 => 0.45, 35 => 0.55)\n Dict(36 => 0.47, 37 => 0.53) Looking at the information of the \"attractors\" (here the clusters of the grouping procedure) does not make it clear which label corresponds to which kind of attractor, but we can look at the: clusters_info 21-element Vector{Dict{Int64, Vector{Float64}}}:\n Dict(1 => [0.0, 0.0])\n Dict(2 => [0.0, 0.0])\n Dict(3 => [0.0, 0.0])\n Dict(4 => [0.0, 0.0])\n Dict(5 => [0.0, 0.0])\n Dict(6 => [0.5, 0.5], 7 => [-0.5, -0.5])\n Dict(9 => [-0.6000000000000006, -0.6000000000000006], 8 => [0.6000000000000005, 0.6000000000000005])\n Dict(11 => [0.6999999999999995, 0.6999999999999995], 10 => [-0.7000000000000002, -0.7000000000000002])\n Dict(13 => [0.7999999999999995, 0.7999999999999995], 12 => [-0.8, -0.8])\n Dict(15 => [0.8999999999999992, 0.8999999999999992], 14 => [-0.8999999999999991, -0.8999999999999991])\n ⋮\n Dict(20 => [-1.200000000000001, -1.200000000000001], 21 => [1.2000000000000013, 1.2000000000000013])\n Dict(22 => [-1.2999999999999987, -1.2999999999999987], 23 => [1.299999999999999, 1.299999999999999])\n Dict(25 => [1.3999999999999992, 1.3999999999999992], 24 => [-1.4000000000000001, -1.4000000000000001])\n Dict(27 => [-1.5, -1.5], 26 => [1.5, 1.5])\n Dict(29 => [1.5999999999999994, 1.5999999999999994], 28 => [-1.5999999999999996, -1.5999999999999996])\n Dict(31 => [-1.7000000000000013, -1.7000000000000013], 30 => [1.7000000000000015, 1.7000000000000015])\n Dict(32 => [-1.7999999999999985, -1.7999999999999985], 33 => [1.7999999999999983, 1.7999999999999983])\n Dict(34 => [-1.9000000000000006, -1.9000000000000006], 35 => [1.9000000000000015, 1.9000000000000015])\n Dict(36 => [-2.0, -2.0], 37 => [2.0, 2.0])"},{"id":417,"pagetitle":"Examples for Attractors.jl","title":"Using histograms and histogram distances as features","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Using-histograms-and-histogram-distances-as-features","content":" Using histograms and histogram distances as features One of the aspects discussed in the original MCBB paper and implementation was the usage of histograms of the means of the variables of a dynamical system as the feature vector. This is useful in very high dimensional systems, such as oscillator networks, where the histogram of the means is significantly different in synchronized or unsychronized states. This is possible to do with current interface without any modifications, by using two more packages: ComplexityMeasures.jl to compute histograms, and Distances.jl for the Kullback-Leibler divergence (or any other measure of distance in the space of probability distributions you fancy). The only code we need to write to achieve this feature is a custom featurizer and providing an alternative distance to  GroupViaClustering . The code would look like this: using Distances: KLDivergence\nusing ComplexityMeasures: ValueHistogram, FixedRectangularBinning, probabilities\n\n# you decide the binning for the histogram, but for a valid estimation of\n# distances, all histograms must have exactly the same bins, and hence be\n# computed with fixed ranges, i.e., using the `FixedRectangularBinning`\nconst binning = FixedRectangularBinning(range(-5, 5; length = 11))\n\nfunction histogram_featurizer(A, t)\n    ms = mean.(columns(A)) # vector of mean of each variable\n    p = probabilities(ValueHistogram(binning), ms) # this is the histogram\n    return vec(p) # because Distances.jl doesn't know `Probabilities`\nend\n\ngconfig = GroupViaClustering(;\n    clust_distance_metric = KLDivergence(), # or any other PDF distance\n) You can then pass the  histogram_featurizer  and  gconfig  to an  AttractorsViaFeaturizing  and use the rest of the library as usual."},{"id":418,"pagetitle":"Examples for Attractors.jl","title":"Animation illustrating  AttractorsViaRecurrences","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Animation-illustrating-AttractorsViaRecurrences","content":" Animation illustrating  AttractorsViaRecurrences The following Julia script inputs a 2D continuous time dynamical system and animates its time evolution while illustrating how  AttractorsViaRecurrences  works. using Attractors, CairoMakie\nusing PredefinedDynamicalSystems\nusing OrdinaryDiffEq\n\n# Set up dynamical system: bi-stable predator pray\nfunction predator_prey_rule(u, p, t)\n    r, c, μ, ν, α, β, χ, δ = p\n    N, P = u\n    common = α*N*P/(β+N)\n    dN = r*N*(1 - (c/r)*N)*((N-μ)/(N+ν)) - common\n    dP = χ*common - δ*P\n    return SVector(dN, dP)\nend\n\nu0 = SVector(8.0, 0.01)\nr = 2.0\n# r, c, μ, ν, α, β, χ, δ = p\np = [r, 0.19, 0.03, 0.003, 800, 1.5, 0.004, 2.2]\n\ndiffeq = (alg = Rodas5P(), abstol = 1e-9, rtol = 1e-9)\nds = CoupledODEs(predator_prey_rule, u0, p; diffeq)\n\nu0s = [ # animation will start from these initial conditions\n    [10, 0.012],\n    [15, 0.02],\n    [12, 0.01],\n    [13, 0.015],\n    [5, 0.02],\n]\n\ndensity = 31\nxg = range(-0.1, 20; length = density)\nyg = range(-0.001, 0.03; length = density)\nΔt = 0.1\ngrid = (xg, yg)\nmapper = AttractorsViaRecurrences(ds, grid;\n    Δt, consecutive_attractor_steps = 10, consecutive_basin_steps = 10, sparse = false,\n    consecutive_recurrences = 100, attractor_locate_steps = 100,\n)\n\n##########################################################################\n\nfunction animate_attractors_via_recurrences(\n        mapper::AttractorsViaRecurrences, u0s;\n        colors = [\"#FFFFFF\", \"#7143E0\",\"#0A9A84\",\"#AF9327\",\"#791457\", \"#6C768C\", \"#4287f5\",],\n        filename = \"recurrence_algorithm.mp4\",\n    )\n\n    grid_nfo = mapper.bsn_nfo.grid_nfo\n\n    fig = Figure()\n    ax = Axis(fig[1,1])\n\n    # Populate the grid with poly! rectangle plots. However! The rectangles\n    # correspond to the same \"cells\" of the grid. Additionally, all\n    # rectangles are colored with an _observable_, that can be accessed\n    # later using the `basin_cell_index` function. The observable\n    # holds the face color of the rectangle!\n\n    # Only 6 colors; need 3 for base, and extra 2 for each attractor.\n    # will choose initial conditions that are only in the first 2 attractors\n    COLORS = map(c -> Makie.RGBA(Makie.RGB(to_color(c)), 0.9), colors)\n\n    function initialize_cells2!(ax, grid; kwargs...)\n        # These are all possible outputs of the `basin_cell_index` function\n        idxs = all_cartesian_idxs(grid)\n        color_obs = Matrix{Any}(undef, size(idxs)...)\n        # We now need to reverse-engineer\n        for i in idxs\n            rect = cell_index_to_rect(i, grid)\n            color = Observable(COLORS[1])\n            color_obs[i] = color\n            poly!(ax, rect; color = color, strokecolor = :black, strokewidth = 0.5)\n        end\n        # Set the axis limits better\n        mini, maxi = Attractors.minmax_grid_extent(grid)\n        xlims!(ax, mini[1], maxi[1])\n        ylims!(ax, mini[2], maxi[2])\n        return color_obs\n    end\n\n    all_cartesian_idxs(grid::Attractors.RegularGrid) = CartesianIndices(length.(grid.grid))\n\n    # Given a cartesian index, the output of `basin_cell_index`, create\n    # a `Rect` object that corresponds to that grid cell!\n    function cell_index_to_rect(n::CartesianIndex, grid::Attractors.RegularGrid)\n        x = grid.grid[1][n[1]]\n        y = grid.grid[2][n[2]]\n        dx = grid.grid_steps[1]\n        dy = grid.grid_steps[2]\n        rect = Rect(x - dx/2, y - dy/2, dx, dy)\n        return rect\n    end\n\n    color_obs = initialize_cells2!(ax, grid_nfo)\n\n    # plot the trajectory\n    state2marker = Dict(\n        :att_search => :circle,\n        :att_found => :dtriangle,\n        :att_hit => :rect,\n        :lost => :star5,\n        :bas_hit => :xcross,\n    )\n\n    # This function gives correct color to search, recurrence, and\n    # the individual attractors. Ignores the lost state.\n    function update_current_cell_color!(cellcolor, bsn_nfo)\n        # We only alter the cell color at specific situations\n        state = bsn_nfo.state\n        if state == :att_search\n            if cellcolor[] == COLORS[1] # empty\n                cellcolor[] = COLORS[2] # visited\n            elseif cellcolor[] == COLORS[2] # visited\n                cellcolor[] = COLORS[3] # recurrence\n            end\n        elseif state == :att_found\n            attidx = (bsn_nfo.current_att_label ÷ 2)\n            attlabel = (attidx - 1)*2 + 1\n            cellcolor[] = COLORS[3+attlabel]\n        end\n        return\n    end\n\n    # Iteration and labelling\n    ds = mapper.ds\n    bsn_nfo = mapper.bsn_nfo\n    u0 = current_state(ds)\n\n    traj = Observable(SVector{2, Float64}[u0])\n    point = Observable([u0])\n\n    marker = Observable(:circle)\n    lines!(ax, traj; color = :black, linewidth = 1)\n    scatter!(ax, point; color = (:black, 0.5), markersize = 20, marker, strokewidth = 1.0, strokecolor = :black)\n\n    stateobs = Observable(:att_search)\n    consecutiveobs = Observable(0)\n    labeltext = @lift(\"state: $($(stateobs))\\nconsecutive: $($(consecutiveobs))\")\n\n    Label(fig[0, 1][1,1], labeltext; justification = :left, halign = :left, tellwidth = false)\n    # add text  with options\n    kwargstext = prod(\"$(p[1])=$(p[2])\\n\" for p in mapper.kwargs)\n    Label(fig[0, 1][1, 2], kwargstext; justification = :right, halign = :right, tellwidth = false)\n\n\n    # make legend\n    entries = [PolyElement(color = c) for c in COLORS[2:end]]\n    labels = [\"visited\", \"recurrence\", \"attr. 1\", \"basin 1\", \"attr. 2\", \"basin 2\"]\n\n    Legend(fig[:, 2][1, 1], entries, labels)\n\n\n    # %% loop\n    # The following code is similar to the source code of `recurrences_map_to_label!`\n\n    cell_label = 0\n    record(fig, filename) do io\n        for u0 in u0s\n            reinit!(ds, copy(u0))\n            traj[] = [copy(u0)]\n\n            while cell_label == 0\n                step!(ds, bsn_nfo.Δt)\n                u = current_state(ds)\n\n                # update FSM\n                n = Attractors.basin_cell_index(u, bsn_nfo.grid_nfo)\n                cell_label = Attractors.finite_state_machine!(bsn_nfo, n, u; mapper.kwargs...)\n\n                state = bsn_nfo.state\n\n                if cell_label ≠ 0 # FSM terminated; we assume no lost/divergence in the system\n                    stateobs[] = :terminated\n\n                    # color-code initial condition if we converged to attractor\n                    # or to basin (even or odd cell label)\n                    u0n = Attractors.basin_cell_index(u0, bsn_nfo.grid_nfo)\n\n                    basidx = (cell_label - 1)\n                    color_obs[u0n][] = COLORS[3+basidx]\n\n                    # Clean up: all \"visited\" cells become white again\n                    visited_idxs = findall(v -> (v[] == COLORS[2] || v[] == COLORS[3]), color_obs)\n                    for n in visited_idxs\n                        color_obs[n][] = COLORS[1] # empty\n                    end\n                    # clean up trajectory line\n                    traj[] = []\n\n                    for i in 1:15; recordframe!(io); end\n                    cell_label = 0\n                    break\n                end\n\n                # update visuals:\n                point[] = [u]\n                push!(traj[], u)\n                notify(traj)\n                marker[] = state2marker[state]\n                stateobs[] = state\n                consecutiveobs[] = bsn_nfo.consecutive_match\n\n                update_current_cell_color!(color_obs[n], bsn_nfo)\n\n                recordframe!(io)\n            end\n        end\n    end\nend\n\nanimate_attractors_via_recurrences(mapper, u0s)"},{"id":419,"pagetitle":"Examples for Attractors.jl","title":"Edge tracking","ref":"/DynamicalSystemsDocs.jl/attractors/stable/examples/#Edge-tracking","content":" Edge tracking To showcase how to run the  edgetracking  algorithm, let us use it to find the saddle point of the bistable FitzHugh-Nagumo (FHN) model, a two-dimensional ODE system originally conceived to represent a spiking neuron. We define the system in the following form: using OrdinaryDiffEq: Vern9\n\nfunction fitzhugh_nagumo(u,p,t)\n    x, y = u\n    eps, beta = p\n    dx = (x - x^3 - y)/eps\n    dy = -beta*y + x\n    return SVector{2}([dx, dy])\nend\n\nparams = [0.1, 3.0]\nds = CoupledODEs(fitzhugh_nagumo, ones(2), params, diffeq=(;alg = Vern9(), reltol=1e-11)) 2-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  fitzhugh_nagumo\n ODE solver:    Vern9\n ODE kwargs:    (reltol = 1.0e-11,)\n parameters:    [0.1, 3.0]\n time:          0.0\n state:         [1.0, 1.0]\n Now, we can use Attractors.jl to compute the fixed points and basins of attraction of the FHN model. xg = yg = range(-1.5, 1.5; length = 201)\ngrid = (xg, yg)\nmapper = AttractorsViaRecurrences(ds, grid; sparse=false)\nbasins, attractors = basins_of_attraction(mapper)\nattractors Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points The  basins_of_attraction  function found three fixed points: the two stable nodes of the system (labelled A and B) and the saddle point at the origin. The saddle is an unstable equilibrium and typically will not be found by  basins_of_attraction . Coincidentally here we initialized an initial condition exactly on the saddle, and hence it was found. We can always find saddles with the  edgetracking  function. For illustration, let us initialize the algorithm from two initial conditions  init1  and  init2  (which must belong to different basins of attraction, see figure below). attractors_AB = Dict(1 => attractors[1], 2 => attractors[2])\ninit1, init2 = [-1.0, -1.0], [-1.0, 0.2] ([-1.0, -1.0], [-1.0, 0.2]) Now, we run the edge tracking algorithm: et = edgetracking(ds, attractors_AB; u1=init1, u2=init2,\n    bisect_thresh = 1e-3, diverge_thresh = 2e-3, Δt = 1e-5, abstol = 1e-3\n)\n\net.edge[end] 2-element SVector{2, Float64} with indices SOneTo(2):\n -0.0008236331423976793\n -0.0014714498606449838 The algorithm has converged to the origin (up to the specified accuracy) where the saddle is located. The figure below shows how the algorithm has iteratively tracked along the basin boundary from the two initial conditions (red points) to the saddle (green square). Points of the edge track (orange) at which a re-bisection occured are marked with a white border. The figure also depicts two trajectories (blue) intialized on either side of the basin boundary at the first bisection point. We see that these trajectories follow the basin boundary for a while but then relax to either attractor before reaching the saddle. By counteracting the instability of the saddle, the edge tracking algorithm instead allows to track the basin boundary all the way to the saddle, or edge state. traj1 = trajectory(ds, 2, et.track1[et.bisect_idx[1]], Δt=1e-5)\ntraj2 = trajectory(ds, 2, et.track2[et.bisect_idx[1]], Δt=1e-5)\n\nfig = Figure()\nax = Axis(fig[1,1], xlabel=\"x\", ylabel=\"y\")\nheatmap_basins_attractors!(ax, grid, basins, attractors, add_legend=false, labels=Dict(1=>\"Attractor A\", 2=>\"Attractor B\", 3=>\"Saddle\"))\nlines!(ax, traj1[1][:,1], traj1[1][:,2], color=:dodgerblue, linewidth=2, label=\"Trajectories\")\nlines!(ax, traj2[1][:,1], traj2[1][:,2], color=:dodgerblue, linewidth=2)\nlines!(ax, et.edge[:,1], et.edge[:,2], color=:orange, linestyle=:dash)\nscatter!(ax, et.edge[et.bisect_idx,1], et.edge[et.bisect_idx,2], color=:white, markersize=15, marker=:circle, zorder=10)\nscatter!(ax, et.edge[:,1], et.edge[:,2], color=:orange, markersize=11, marker=:circle, zorder=10, label=\"Edge track\")\nscatter!(ax, [-1.0,-1.0], [-1.0, 0.2], color=:red, markersize=15, label=\"Initial conditions\")\nxlims!(ax, -1.2, 1.1); ylims!(ax, -1.3, 0.8)\naxislegend(ax, position=:rb)\nfig In this simple two-dimensional model, we could of course have found the saddle directly by computing the zeroes of the ODE system. However, the edge tracking algorithm allows finding edge states also in high-dimensional and chaotic systems where a simple computation of unstable equilibria becomes infeasible."},{"id":422,"pagetitle":"References","title":"References","ref":"/DynamicalSystemsDocs.jl/attractors/stable/references/#References","content":" References Battelino, P. M.; Grebogi, C.; Ott, E.; Yorke, J. A. and Yorke, E. D. (1988).  Multiple coexisting attractors, basin boundaries and basic sets . Physica D: Nonlinear Phenomena  32 , 296–305. Datseris, G.; Rossi, K. L. and Wagemakers, A. (2023).  Framework for global stability analysis of dynamical systems .  Chaos: An Interdisciplinary Journal of Nonlinear Science  33 . Datseris, G. and Wagemakers, A. (2022).  Effortless estimation of basins of attraction .  Chaos: An Interdisciplinary Journal of Nonlinear Science  32 , 023104 . Daza, A.; Wagemakers, A.; Georgeot, B.; Guéry-Odelin, D. and Sanjuán, M. A. (2016).  Basin entropy: a new tool to analyze uncertainty in dynamical systems .  Scientific Reports  6 . Daza, A.; Wagemakers, A. and Sanjuán, M. A. (2018).  Ascertaining when a basin is Wada: the merging method . Scientific Reports  8 , 9954. Ester, M.; Kriegel, H.-P.; Sander, J. and Xu, X. (1996).  A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise . In:  Proceedings of the Second International Conference on Knowledge Discovery and Data Mining ,  KDD'96  (AAAI Press); pp. 226–231. Gelbrecht, M.; Kurths, J. and Hellmann, F. (2020).  Monte Carlo basin bifurcation analysis .  New Journal of Physics  22 , 033032 . Grebogi, C.; McDonald, S. W.; Ott, E. and Yorke, J. A. (1983).  Final state sensitivity: An obstruction to predictability .  Physics Letters A  99 , 415–418 . Halekotte, L. and Feudel, U. (2020).  Minimal fatal shocks in multistable complex networks .  Scientific Reports  10 . Kaszás, B.; Feudel, U. and Tél, T. (2019).  Tipping phenomena in typical dynamical systems subjected to parameter drift .  Scientific Reports  9 . Lucarini, V. and Bódai, T. (2017).  Edge states in the climate system: exploring global instabilities and critical transitions . Nonlinearity  30 , R32. Mehling, O.; Börner, R. and Lucarini, V. (2023).  Limits to predictability of the asymptotic state of the Atlantic Meridional Overturning Circulation in a conceptual climate model , arXiv preprint arXiv:2308.16251. Menck, P. J.; Heitzig, J.; Marwan, N. and Kurths, J. (2013).  How basin stability complements the linear-stability paradigm .  Nature Physics  9 , 89–92 . Puy, A.; Daza, A.; Wagemakers, A. and Sanjuán, M. A. (2021).  A test for fractal boundaries based on the basin entropy .  Communications in Nonlinear Science and Numerical Simulation  95 , 105588 . Schneider, T. M.; Gibson, J. F.; Lagha, M.; De Lillo, F. and Eckhardt, B. (2008).  Laminar-turbulent boundary in plane Couette flow . Physical Review E  78 , 037301. Schubert, E.; Sander, J.; Ester, M.; Kriegel, H. P. and Xu, X. (2017).  DBSCAN Revisited,  Revisited .  ACM Transactions on Database Systems  42 , 1–21 . Skufca, J. D.; Yorke, J. A. and Eckhardt, B. (2006).  Edge of chaos in a parallel shear flow . Physical review letters  96 , 174101. Stender, M. and Hoffmann, N. (2021),  bSTAB: an open-source software for computing the basin stability of multi-stable dynamical systems .  Nonlinear Dynamics  107 , 1451–1468 . Wagemakers, A.; Daza, A. and Sanjuán, M. A. (2020).  The saddle-straddle method to test for Wada basins . Communications in Nonlinear Science and Numerical Simulation  84 , 105167."},{"id":425,"pagetitle":"Visualization utilities","title":"Visualization utilities","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Visualization-utilities","content":" Visualization utilities In this page we document several plotting utility functions that have been created to make the visualization of the output of Attractors.jl seamless. See the examples page for usage of all these plotting functions. Note that most functions have an out-of-place and an in-place form, the in-place form always taking as a first input a pre-initialized  Axis  to plot in while the out-of-place creates and returns a new figure object. E.g., heatmap_basins_attractors(grid, basins, attractors; kwargs...)\nheatmap_basins_attractors!(ax, grid, basins, attractors; kwargs...)"},{"id":426,"pagetitle":"Visualization utilities","title":"Common plotting keywords","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#common_plot_kwargs","content":" Common plotting keywords Common keywords for plotting functions in Attractors.jl are: ukeys : the basin ids (unique keys, vector of integers) to use. By default all existing keys are used. access = [1, 2] : indices of which dimensions of an attractor to select and visualize in a two-dimensional plot. Only these ids will be visualized. By default all are used. colors : a dictionary mapping basin ids (i.e., including the  -1  key) to a color. By default the JuliaDynamics colorscheme is used if less than 7 ids are present, otherwise random colors from the  :darktest  colormap. markers : dictionary mapping attractor ids to markers they should be plotted as labels = Dict(ukeys .=> ukeys) : how to label each attractor. add_legend = length(ukeys) < 7 : whether to add a legend mapping colors to labels."},{"id":427,"pagetitle":"Visualization utilities","title":"Basins related","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Basins-related","content":" Basins related"},{"id":428,"pagetitle":"Visualization utilities","title":"Attractors.heatmap_basins_attractors","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Attractors.heatmap_basins_attractors","content":" Attractors.heatmap_basins_attractors  —  Function heatmap_basins_attractors(grid, basins, attractors; kwargs...) Plot a heatmap of found (2-dimensional)  basins  of attraction and corresponding  attractors , i.e., the output of  basins_of_attraction . Keyword arguments All the  common plotting keywords . source"},{"id":429,"pagetitle":"Visualization utilities","title":"Attractors.shaded_basins_heatmap","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Attractors.shaded_basins_heatmap","content":" Attractors.shaded_basins_heatmap  —  Function shaded_basins_heatmap(grid, basins, attractors, iterations; kwargs...) Plot a heatmap of found (2-dimensional)  basins  of attraction and corresponding  attractors . A matrix  iterations  with the same size of  basins  must be provided to shade the color according to the value of this matrix. A small value corresponds to a light color and a large value to a darker tone. This is useful to represent the number of iterations taken for each initial condition to converge. See also  convergence_time  to store this iteration number. Keyword arguments show_attractors = true : shows the attractor on plot maxit = maximum(iterations) : clip the values of  iterations  to the value  maxit . Useful when there are some very long iterations and keep the range constrained to a given interval. All the  common plotting keywords . source"},{"id":430,"pagetitle":"Visualization utilities","title":"Continuation related","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Continuation-related","content":" Continuation related"},{"id":431,"pagetitle":"Visualization utilities","title":"Attractors.plot_basins_curves","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Attractors.plot_basins_curves","content":" Attractors.plot_basins_curves  —  Function plot_basins_curves(fractions_curves, prange = 1:length(); kwargs...) Plot the fractions of basins of attraction versus a parameter range, i.e., visualize the output of  continuation . See also  plot_basins_attractors_curves . Keyword arguments style = :band : how to visualize the basin fractions. Choices are  :band  for a band plot with cumulative sum = 1 or  :lines  for a lines plot of each basin fraction separatorwidth = 1, separatorcolor = \"white\" : adds a line separating the fractions if the style is  :band axislegend_kwargs = (position = :lt,) : propagated to  axislegend  if a legend is added series_kwargs = NamedTuple() : propagated to the band or scatterline plot Also all  common plotting keywords . source"},{"id":432,"pagetitle":"Visualization utilities","title":"Attractors.plot_attractors_curves","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Attractors.plot_attractors_curves","content":" Attractors.plot_attractors_curves  —  Function plot_attractors_curves(attractors_info, attractor_to_real, prange = 1:length(); kwargs...) Same as in  plot_basins_curves  but visualizes the attractor dependence on the parameter instead of their fraction. The function  attractor_to_real  takes as input a  StateSpaceSet  (attractor) and returns a real number so that it can be plotted versus the parameter axis. See also  plot_basins_attractors_curves . Same keywords as  plot_basins_curves . source"},{"id":433,"pagetitle":"Visualization utilities","title":"Attractors.plot_basins_attractors_curves","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Attractors.plot_basins_attractors_curves","content":" Attractors.plot_basins_attractors_curves  —  Function plot_basins_attractors_curves(\n    fractions_curves, attractors_info, a2rs [, prange]\n    kwargs...\n) Convenience combination of  plot_basins_curves  and  plot_attractors_curves  in a multi-panel plot that shares legend, colors, markers, etc. This function allows  a2rs  to be a  Vector  of functions, each mapping attractors into real numbers. Below the basins fractions plot, one additional panel is created for each entry in  a2rs .  a2rs  can also be a single function, in which case only one panel is made. source"},{"id":434,"pagetitle":"Visualization utilities","title":"Video output","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Video-output","content":" Video output"},{"id":435,"pagetitle":"Visualization utilities","title":"Attractors.animate_attractors_continuation","ref":"/DynamicalSystemsDocs.jl/attractors/stable/visualization/#Attractors.animate_attractors_continuation","content":" Attractors.animate_attractors_continuation  —  Function animate_attractors_continuation(\n    ds::DynamicalSystem, attractors_info, fractions_curves, prange, pidx;\n    kwargs...\n) Animate how the found system attractors and their corresponding basin fractions change as the system parameter is increased. This function combines the input and output of the  continuation  function into a video output. The input dynamical system  ds  is used to evolve initial conditions sampled from the found attractors, so that the attractors are better visualized.  attractors_info, fractions_curves  are the output of  continuation  while  ds, prange, pidx  are the input to  continuation . Keyword arguments savename = \"attracont.mp4\" : name of video output file. framerate = 4 : framerate of video output. Δt, T : propagated to  trajectory  for evolving an initial condition sampled from an attractor. Also all  common plotting keywords . source"},{"id":438,"pagetitle":"DelayEmbeddings.jl","title":"DelayEmbeddings.jl","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/#DelayEmbeddings.jl","content":" DelayEmbeddings.jl"},{"id":439,"pagetitle":"DelayEmbeddings.jl","title":"DelayEmbeddings","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/#DelayEmbeddings","content":" DelayEmbeddings  —  Module DelayEmbeddings.jl A Julia package that provides a generic interface for performing delay coordinate embeddings, as well as cutting edge algorithms for creating optimal embeddings given some data. It can be used as a standalone package, or as part of DynamicalSystems.jl. To install it, run  import Pkg; Pkg.add(\"DelayEmbeddings\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. source"},{"id":440,"pagetitle":"DelayEmbeddings.jl","title":"Overview","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/#Overview","content":" Overview Note The documentation and the code of this package is parallelizing Chapter 6 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. The package provides an interface to perform delay coordinates embeddings, as explained in  homonymous page . There are two approaches for estimating optimal parameters to do delay embeddings: Separated , where one tries to find the best value for a delay time  τ  and then an optimal embedding dimension  d . Unified , where at the same time an optimal combination of  τ, d  is found. The separated approach is something \"old school\", while recent scientific research has shifted almost exclusively to unified approaches. This page describes algorithms belonging to the separated approach, which is mainly done by the function  optimal_separated_de . The unified approach is discussed in the  Unified optimal embedding  page."},{"id":443,"pagetitle":"Delay coordinates embedding","title":"Delay coordinates embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#embedding","content":" Delay coordinates embedding A timeseries recorded in some manner from a dynamical system can be used to gain information about the dynamics of the entire state space of the system. This can be done by constructing a new state space from the timeseries. One method that can do this is what is known as  delay coordinates embedding  or delay coordinates reconstruction. The main functions to use for embedding some input data are  embed  or  genembed . Both functions return a  StateSpaceSet ."},{"id":444,"pagetitle":"Delay coordinates embedding","title":"Timeseries embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#Timeseries-embedding","content":" Timeseries embedding"},{"id":445,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.embed","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#DelayEmbeddings.embed","content":" DelayEmbeddings.embed  —  Function embed(s, d, τ [, h]) Embed  s  using delay coordinates with embedding dimension  d  and delay time  τ  and return the result as a  StateSpaceSet . Optionally use weight  h , see below. Here  τ > 0 , use  genembed  for a generalized version. Description If  τ  is an integer, then the  $n$ -th entry of the embedded space is \\[(s(n), s(n+\\tau), s(n+2\\tau), \\dots, s(n+(d-1)\\tau))\\] If instead  τ  is a vector of integers, so that  length(τ) == d-1 , then the  $n$ -th entry is \\[(s(n), s(n+\\tau[1]), s(n+\\tau[2]), \\dots, s(n+\\tau[d-1]))\\] The resulting set can have same invariant quantities (like e.g. Lyapunov exponents) with the original system that the timeseries were recorded from, for proper  d  and  τ . This is known as the Takens embedding theorem  [Takens1981] [Sauer1991] . The case of different delay times allows embedding systems with many time scales, see [Judd1998] . If provided,  h  can be weights to multiply the entries of the embedded space. If  h isa Real  then the embedding is \\[(s(n), h \\cdot s(n+\\tau), h^2 \\cdot s(n+2\\tau), \\dots,h^{d-1} \\cdot s(n+γ\\tau))\\] Otherwise  h  can be a vector of length  d-1 , which the decides the weights of each entry directly. References [Takens1981]  : F. Takens,  Detecting Strange Attractors in Turbulence — Dynamical Systems and Turbulence , Lecture Notes in Mathematics  366 , Springer (1981) [Sauer1991]  : T. Sauer  et al. , J. Stat. Phys.  65 , pp 579 (1991) source Embedding discretized data values If the data values are very strongly discretized (e.g., integers or floating-point numbers with very small bits), this can result to distances between points in the embedded space being 0. This is problematic for several library functions. Best practice here is to add noise to your original timeseries  before  embedding, e.g.,  s = s .+ 1e-15randn(length(s)) . Here are some examples of embedding a 3D continuous chaotic system: using DelayEmbeddings\n\nx = cos.(0:0.1:1) 11-element Vector{Float64}:\n 1.0\n 0.9950041652780258\n 0.9800665778412416\n 0.955336489125606\n 0.9210609940028851\n 0.8775825618903728\n 0.8253356149096783\n 0.7648421872844885\n 0.6967067093471654\n 0.6216099682706644\n 0.5403023058681398 embed(x, 3, 1) 3-dimensional StateSpaceSet{Float64} with 9 points\n 1.0       0.995004  0.980067\n 0.995004  0.980067  0.955336\n 0.980067  0.955336  0.921061\n 0.955336  0.921061  0.877583\n 0.921061  0.877583  0.825336\n 0.877583  0.825336  0.764842\n 0.825336  0.764842  0.696707\n 0.764842  0.696707  0.62161\n 0.696707  0.62161   0.540302 `τ` and `Δt` Keep in mind that whether a value of  τ  is \"reasonable\" for continuous time systems depends on the sampling time  Δt ."},{"id":446,"pagetitle":"Delay coordinates embedding","title":"Embedding Structs","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#Embedding-Structs","content":" Embedding Structs The high level function  embed  utilizes a low-level interface for creating embedded vectors on-the-fly. The high level interface simply loops over the low level interface."},{"id":447,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.DelayEmbedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#DelayEmbeddings.DelayEmbedding","content":" DelayEmbeddings.DelayEmbedding  —  Type DelayEmbedding(γ, τ, h = nothing) → `embedding` Return a delay coordinates embedding structure to be used as a function-like-object, given a timeseries and some index. Calling embedding(s, n) will create the  n -th delay vector of the embedded space, which has  γ  temporal neighbors with delay(s)  τ .  γ  is the embedding dimension minus 1,  τ  is the delay time(s) while  h  are extra weights, as in  embed  for more. Be very careful when choosing  n , because  @inbounds  is used internally.  Use  τrange ! source"},{"id":448,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.τrange","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#DelayEmbeddings.τrange","content":" DelayEmbeddings.τrange  —  Function τrange(s, de::AbstractEmbedding) Return the range  r  of valid indices  n  to create delay vectors out of  s  using  de . source"},{"id":449,"pagetitle":"Delay coordinates embedding","title":"Generalized embeddings","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#Generalized-embeddings","content":" Generalized embeddings"},{"id":450,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.genembed","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#DelayEmbeddings.genembed","content":" DelayEmbeddings.genembed  —  Function genembed(s, τs, js = ones(...); ws = nothing) → ssset Create a generalized embedding of  s  which can be a timeseries or arbitrary  StateSpaceSet , and return the result as a new  StateSpaceSet . The generalized embedding works as follows: τs  denotes what delay times will be used for each of the entries of the delay vector. It is recommended that  τs[1] = 0 .  τs  is allowed to have  negative entries  as well. js  denotes which of the timeseries contained in  s  will be used for the entries of the delay vector.  js  can contain duplicate indices. ws  are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by  ws[i] ). If provided, it is recommended that  ws[1] == 1 . τs, js, ws  are tuples (or vectors) of length  D , which also coincides with the embedding dimension. For example, imagine input trajectory  $s = [x, y, z]$  where  $x, y, z$  are timeseries (the columns of the  StateSpaceSet ). If  js = (1, 3, 2)  and  τs = (0, 2, -7)  the created delay vector at each step  $n$  will be \\[(x(n), z(n+2), y(n-7))\\] Using  ws = (1, 0.5, 0.25)  as well would create \\[(x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))\\] js  can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if  s  is a timeseries instead of a  StateSpaceSet . See also  embed . Internally uses  GeneralizedEmbedding . source"},{"id":451,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.GeneralizedEmbedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#DelayEmbeddings.GeneralizedEmbedding","content":" DelayEmbeddings.GeneralizedEmbedding  —  Type GeneralizedEmbedding(τs, js = ones(length(τs)), ws = nothing) -> `embedding` Return a delay coordinates embedding structure to be used as a function. Given a timeseries  or  trajectory (i.e.  StateSpaceSet )  s  and calling embedding(s, n) will create the delay vector of the  n -th point of  s  in the embedded space using generalized embedding (see  genembed ). js  is ignored for timeseries input  s  (since all entries of  js  must be  1  in this case) and in addition  js  defaults to  (1, ..., 1)  for all  τ . Be very careful when choosing  n , because  @inbounds  is used internally.  Use  τrange ! source"},{"id":452,"pagetitle":"Delay coordinates embedding","title":"StateSpaceSet reference","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#StateSpaceSet-reference","content":" StateSpaceSet reference"},{"id":453,"pagetitle":"Delay coordinates embedding","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/embed/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. Judd1998 K. Judd & A. Mees,  Physica D  120 , pp 273 (1998) Farmer1988 Farmer & Sidorowich,  Exploiting Chaos to Predict the Future and Reduce Noise\""},{"id":458,"pagetitle":"Separated optimal embedding","title":"Separated optimal embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#Separated-optimal-embedding","content":" Separated optimal embedding This page discusses and provides algorithms for estimating optimal parameters to do Delay Coordinates Embedding (DCE) with using the separated approach."},{"id":459,"pagetitle":"Separated optimal embedding","title":"Automated function","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#Automated-function","content":" Automated function"},{"id":460,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.optimal_separated_de","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.optimal_separated_de","content":" DelayEmbeddings.optimal_separated_de  —  Function optimal_separated_de(s, method = \"afnn\", dmethod = \"mi_min\"; kwargs...) → 𝒟, τ, E Produce an optimal delay embedding  𝒟  of the given timeseries  s  by using the  separated  approach of first finding an optimal (and constant) delay time using  estimate_delay  with the given  dmethod , and then an optimal embedding dimension, by calculating an appropriate statistic for each dimension  d ∈ 1:dmax . Return the embedding  𝒟 , the optimal delay time  τ  (the optimal embedding dimension  d  is just  size(𝒟, 2) ) and the actual statistic  E  used to estimate optimal  d . Notice that  E  is a function of the embedding dimension, which ranges from 1 to  dmax . For calculating  E  to estimate the dimension we use the given  method  which can be: \"afnn\"  (default) is Cao's \"Averaged False Nearest Neighbors\" method [Cao1997] ,   which gives a ratio of distances between nearest neighbors. \"ifnn\"  is the \"Improved False Nearest Neighbors\" from Hegger & Kantz [Hegger1999] ,   which gives the fraction of false nearest neighbors. \"fnn\"  is Kennel's \"False Nearest Neighbors\" method [Kennel1992] , which gives   the number of points that cease to be \"nearest neighbors\" when the dimension   increases. \"f1nn\"  is Krakovská's \"False First Nearest Neighbors\" method [Krakovská2015] ,   which gives the ratio of pairs of points that cease to be \"nearest neighbors\"   when the dimension increases. For more details, see individual methods:  delay_afnn ,  delay_ifnn ,  delay_fnn ,  delay_f1nn . Careful in automated methods While this method is automated if you want to be  really sure  of the results, you should directly calculate the statistic and plot its values versus the dimensions. Keyword arguments The keywords τs = 1:100, dmax = 10 denote which delay times and embedding dimensions  ds ∈ 1:dmax  to consider when calculating optimal embedding. The keywords slope_thres = 0.05, stoch_thres = 0.1, fnn_thres = 0.05 are specific to this function, see Description below. All remaining keywords are propagated to the low level functions: w, rtol, atol, τs, metric, r Description We estimate the optimal embedding dimension based on the given delay time gained from  dmethod  as follows: For Cao's method the optimal dimension is reached, when the slope of the  E₁ -statistic (output from  \"afnn\" ) falls below the threshold  slope_thres  and the according stochastic test turns out to be false, i.e. if the  E₂ -statistic's first value is  < 1 - stoch_thres . For all the other methods we return the optimal embedding dimension when the corresponding FNN-statistic (output from  \"ifnn\" ,  \"fnn\"  or  \"f1nn\" ) falls below the fnn-threshold  fnn_thres  AND the slope of the statistic falls below the threshold  slope_thres . Note that with noise contaminated time series, one might need to adjust  fnn_thres  according to the noise level. See also the file  test/compare_different_dimension_estimations.jl  for a comparison. source"},{"id":461,"pagetitle":"Separated optimal embedding","title":"Optimal delay time","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#Optimal-delay-time","content":" Optimal delay time"},{"id":462,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.estimate_delay","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.estimate_delay","content":" DelayEmbeddings.estimate_delay  —  Function estimate_delay(s, method::String [, τs = 1:100]; kwargs...) -> τ Estimate an optimal delay to be used in  embed . The  method  can be one of the following: \"ac_zero\"  : first delay at which the auto-correlation function becomes <0. \"ac_min\"  : delay of first minimum of the auto-correlation function. \"mi_min\"  : delay of first minimum of mutual information of  s  with itself (shifted for various  τs ). Keywords  nbins, binwidth  are propagated into  selfmutualinfo . \"exp_decay\"  :  exponential_decay_fit  of the correlation function rounded  to an integer (uses least squares on  c(t) = exp(-t/τ)  to find  τ ). \"exp_extrema\"  : same as above but the exponential fit is done to the absolute value of the local extrema of the correlation function. Both the mutual information and correlation function ( autocor ) are computed  only  for delays  τs . This means that the  min  methods can never return the first value of  τs ! The method  mi_min  is significantly more accurate than the others and also returns good results for most timeseries. It is however the slowest method (but still quite fast!). source"},{"id":463,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.exponential_decay_fit","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.exponential_decay_fit","content":" DelayEmbeddings.exponential_decay_fit  —  Function exponential_decay_fit(x, y, weight = :equal) -> τ Perform a least square fit of the form  y = exp(-x/τ)  and return  τ . Taken from:  http://mathworld.wolfram.com/LeastSquaresFittingExponential.html. Assumes equal lengths of  x, y  and that  y ≥ 0 . To use the method that gives more weight to small values of  y , use  weight = :small . source"},{"id":464,"pagetitle":"Separated optimal embedding","title":"Self Mutual Information","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#Self-Mutual-Information","content":" Self Mutual Information"},{"id":465,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.selfmutualinfo","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.selfmutualinfo","content":" DelayEmbeddings.selfmutualinfo  —  Function selfmutualinfo(s, τs; kwargs...) → m Calculate the mutual information between the time series  s  and itself delayed by  τ  points for  τ  ∈  τs , using an  improvement  of the method outlined by Fraser & Swinney in [Fraser1986] . Description The joint space of  s  and its  τ -delayed image ( sτ ) is partitioned as a rectangular grid, and the mutual information is computed from the joint and marginal frequencies of  s  and  sτ  in the grid as defined in [1]. The mutual information values are returned in a vector  m  of the same length as  τs . If any of the optional keyword parameters is given, the grid will be a homogeneous partition of the space where  s  and  sτ  are defined. The margins of that partition will be divided in a number of bins equal to  nbins , such that the width of each bin will be  binwidth , and the range of nonzero values of  s  will be in the centre. If only of those two parameters is given, the other will be automatically calculated to adjust the size of the grid to the area where  s  and  sτ  are nonzero. If no parameter is given, the space will be partitioned by a recursive bisection algorithm based on the method given in [1]. Notice that the recursive method of [1] evaluates the joint frequencies of  s  and  sτ  in each cell resulting from a partition, and stops when the data points are uniformly distributed across the sub-partitions of the following levels. For performance and stability reasons, the automatic partition method implemented in this function is only used to divide the axes of the grid, using the marginal frequencies of  s . source Notice that mutual information between two  different  timeseries x, y exists in JuliaDynamics as well, but in the package  CausalityTools.jl . It is also trivial to define it yourself using  entropy  from  ComplexityMeasures ."},{"id":466,"pagetitle":"Separated optimal embedding","title":"Optimal embedding dimension","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#Optimal-embedding-dimension","content":" Optimal embedding dimension"},{"id":467,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_afnn","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.delay_afnn","content":" DelayEmbeddings.delay_afnn  —  Function delay_afnn(s::AbstractVector, τ:Int, ds = 2:6; metric=Euclidean(), w = 0) → E₁ Compute the parameter E₁ of Cao's \"averaged false nearest neighbors\" method for determining the minimum embedding dimension of the time series  s , with a sequence of  τ -delayed temporal neighbors. Description Given the scalar timeseries  s  and the embedding delay  τ  compute the values of  E₁  for each embedding dimension  d ∈ ds , according to Cao's Method (eq. 3 of [Cao1997] ). This quantity is a ratio of the averaged distances between the nearest neighbors of the reconstructed time series, which quantifies the increment of those distances when the embedding dimension changes from  d  to  d+1 . Return the vector of all computed  E₁ s. To estimate a good value for  d  from this, find  d  for which the value  E₁  saturates at some value around 1. Note: This method does not work for datasets with perfectly periodic signals. w  is the  Theiler window . See also:  optimal_separated_de  and  stochastic_indicator . source"},{"id":468,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_ifnn","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.delay_ifnn","content":" DelayEmbeddings.delay_ifnn  —  Function delay_ifnn(s::Vector, τ::Int, ds = 1:10; kwargs...) → `FNNs` Compute and return the  FNNs -statistic for the time series  s  and a uniform time delay  τ  and embedding dimensions  ds  after  [Hegger1999] . In this notation  γ ∈ γs = d-1 , if  d  is the embedding dimension. This fraction tends to 0 when the optimal embedding dimension with an appropriate lag is reached. Keywords * r = 2 : Obligatory threshold, which determines the maximum tolerable spreading     of trajectories in the reconstruction space. * metric = Euclidean : The norm used for distance computations. * w = 1  = The  Theiler window . See also:  optimal_separated_de . source"},{"id":469,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_fnn","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.delay_fnn","content":" DelayEmbeddings.delay_fnn  —  Function delay_fnn(s::AbstractVector, τ:Int, ds = 2:6; rtol=10.0, atol=2.0) → FNNs Calculate the number of \"false nearest neighbors\" (FNNs) of the datasets created from  s  with  embed(s, d, τ) for d ∈ ds . Description Given a dataset made by  embed(s, d, τ)  the \"false nearest neighbors\" (FNN) are the pairs of points that are nearest to each other at dimension  d , but are separated at dimension  d+1 . Kennel's criteria for detecting FNN are based on a threshold for the relative increment of the distance between the nearest neighbors ( rtol , eq. 4 in [Kennel1992] ), and another threshold for the ratio between the increased distance and the \"size of the attractor\" ( atol , eq. 5 in [Kennel1992] ). These thresholds are given as keyword arguments. The returned value is a vector with the number of FNN for each  γ ∈ γs . The optimal value for  γ  is found at the point where the number of FNN approaches zero. See also:  optimal_separated_de . source"},{"id":470,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_f1nn","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.delay_f1nn","content":" DelayEmbeddings.delay_f1nn  —  Function delay_f1nn(s::AbstractVector, τ::Int, ds = 2:6; metric = Euclidean()) Calculate the ratio of \"false first nearest neighbors\" (FFNN) of the datasets created from  s  with  embed(s, d, τ) for d ∈ ds . Description Given a dataset made by  embed(s, d, τ)  the \"false first nearest neighbors\" (FFNN) are the pairs of points that are nearest to each other at dimension  d  that cease to be nearest neighbors at dimension  d+1 . The returned value is a vector with the ratio between the number of FFNN and the number of points in the dataset for each  d ∈ ds . The optimal value for  d  is found at the point where this ratio approaches zero. See also:  optimal_separated_de . source"},{"id":471,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.stochastic_indicator","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#DelayEmbeddings.stochastic_indicator","content":" DelayEmbeddings.stochastic_indicator  —  Function stochastic_indicator(s::AbstractVector, τ:Int, ds = 2:5) -> E₂s Compute an estimator for apparent randomness in a delay embedding with  ds  dimensions. Description Given the scalar timeseries  s  and the embedding delay  τ  compute the values of  E₂  for each  d ∈ ds , according to Cao's Method (eq. 5 of  [Cao1997] ). Use this function to confirm that the input signal is not random and validate the results of  delay_afnn . In the case of random signals, it should be  E₂ ≈ 1 ∀ d . source"},{"id":472,"pagetitle":"Separated optimal embedding","title":"Example","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/separated/#Example","content":" Example using DelayEmbeddings, CairoMakie\nusing DynamicalSystemsBase\n\nfunction roessler_rule(u, p, t)\n    a, b, c = p\n    du1 = -u[2]-u[3]\n    du2 = u[1] + a*u[2]\n    du3 = b + u[3]*(u[1] - c)\n    return SVector(du1, du2, du3)\nend\nds = CoupledODEs(roessler_rule, [1, -2, 0.1], [0.2, 0.2, 5.7])\n\n# This trajectory is a chaotic attractor with fractal dim ≈ 2\n# therefore the set needs at least embedding dimension of 3\nX, tvec = trajectory(ds, 1000.0; Δt = 0.05)\nx = X[:, 1]\n\ndmax = 7\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"embedding dimension\", ylabel = \"estimator\")\nfor (i, method) in enumerate([\"afnn\", \"fnn\", \"f1nn\", \"ifnn\"])\n    # Plot statistic used to estimate optimal embedding\n    # as well as the automated output embedding\n    𝒟, τ, E = optimal_separated_de(x, method; dmax)\n    lines!(ax, 1:dmax, E; label = method, marker = :circle, color = Cycled(i))\n    optimal_d = size(𝒟, 2)\n    ## Scatter the optimal embedding dimension as a lager marker\n    scatter!(ax, [optimal_d], [E[optimal_d]];\n        color = Cycled(i), markersize = 30\n    )\nend\naxislegend(ax)\nfig Cao1997 Liangyue Cao,  Physica D, pp. 43-50 (1997) Kennel1992 M. Kennel  et al. ,  Phys. Review A  45 (6), (1992) . Krakovská2015 Anna Krakovská  et al. ,  J. Complex Sys. 932750 (2015) Hegger1999 Hegger & Kantz,  Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970 . Fraser1986 Fraser A.M. & Swinney H.L. \"Independent coordinates for strange attractors from mutual information\"  Phys. Rev. A 33 (2), 1986, 1134:1140."},{"id":475,"pagetitle":"Unified optimal embedding","title":"Unified optimal embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#Unified-optimal-embedding","content":" Unified optimal embedding Unified approaches try to create an optimal embedding by in parallel optimizing what combination of delay times and embedding dimensions suits best. In addition, the unified approaches are the only ones that can accommodate multi-variate inputs. This means that if you have multiple measured input timeseries, you should be able to take advantage of all of them for the best possible embedding of the dynamical system's set."},{"id":476,"pagetitle":"Unified optimal embedding","title":"An example","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#An-example","content":" An example"},{"id":477,"pagetitle":"Unified optimal embedding","title":"Univariate input","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#Univariate-input","content":" Univariate input In following we illustrate the most recent unified optimal embedding method, called PECUZAL, on three examples (see  pecuzal_embedding ). We start with a univariate case, i.e. we only feed in one time series, here the x-component of the Lorenz system. using DynamicalSystemsBase # to simulate Lorenz63\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector(du1, du2, du3)\nend\n\nlo = CoupledODEs(lorenz_rule, [1.0, 1.0, 50.0], [10, 28, 8/3])\ntr, tvec = trajectory(lo, 100; Δt = 0.01, Ttr = 10) (3-dimensional StateSpaceSet{Float64} with 10001 points, 10.0:0.01:110.0) using DelayEmbeddings\ns = vec(tr[:, 1]) # input timeseries = x component of Lorenz\ntheiler = estimate_delay(s, \"mi_min\") # estimate a Theiler window\nTmax = 100 # maximum possible delay\n\nY, τ_vals, ts_vals, Ls, εs = pecuzal_embedding(s; τs = 0:Tmax , w = theiler, econ = true)\n\nprintln(\"τ_vals = \", τ_vals)\nprintln(\"Ls = \", Ls)\nprintln(\"L_total_uni: $(sum(Ls))\") Initializing PECUZAL algorithm for univariate input...\nStarting 1-th embedding cycle...\nStarting 2-th embedding cycle...\nStarting 3-th embedding cycle...\nAlgorithm stopped due to increasing L-values. VALID embedding achieved ✓.\nτ_vals = [0, 18, 9]\nLs = [-0.813361817423154, -0.410653828820728]\nL_total_uni: -1.224015646243882 The output reveals that PECUZAL suggests a 3-dimensional embedding out of the un-lagged time series as the 1st component of the reconstruction, the time series lagged by 18 samples as the 2nd component and the time series lagged by 9 samples as the 3rd component. In the third embedding cycle there is no  ΔL<0  and the algorithm terminates. The result after two successful embedding cycles is the 3-dimensional embedding  Y  which is also returned. The total obtained decrease of  ΔL  throughout all encountered embedding cycles has been ~ -1.24. We can also look at  continuity statistic using CairoMakie\n\nfig = Figure()\nax = Axis(fig[1,1])\nlines!(εs[:,1], label=\"1st emb. cycle\")\nscatter!([τ_vals[2]], [εs[τ_vals[2],1]])\nlines!(εs[:,2], label=\"2nd emb. cycle\")\nscatter!([τ_vals[3]], [εs[τ_vals[3],2]])\nlines!(εs[:,3], label=\"3rd emb. cycle\")\nax.title = \"Continuity statistics PECUZAL Lorenz\"\nax.xlabel = \"delay τ\"\nax.ylabel = \"⟨ε⋆⟩\"\naxislegend(ax)\nfig The picked delay values are marked with filled circles. As already mentioned, the third embedding cycle did not contribute to the embedding, i.e. there has been no delay value chosen."},{"id":478,"pagetitle":"Unified optimal embedding","title":"Multivariate input","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#Multivariate-input","content":" Multivariate input Similar to the approach in the preceding example, we now highlight the capability of the PECUZAL embedding method for a multivariate input. The idea is now to feed in all three time series to the algorithm, even though this is a very far-from-reality example. We already have an adequate representation of the system we want to reconstruct, namely the three time series from the numerical integration. But let us see what PECUZAL suggests for a reconstruction. # compute Theiler window\nw1 = estimate_delay(tr[:,1], \"mi_min\")\nw2 = estimate_delay(tr[:,2], \"mi_min\")\nw3 = estimate_delay(tr[:,3], \"mi_min\")\nw = max(w1,w2,w3)\nY_m, τ_vals_m, ts_vals_m, = pecuzal_embedding(tr; τs = 0:Tmax , w = theiler, econ = true)\n\nprintln(τ_vals_m)\nprintln(ts_vals_m) [0, 12, 0, 84, 69, 56]\n[3, 1, 1, 1, 1, 1] PECUZAL returns a 6-dimensional embedding using the un-lagged  z - and  x -component as 1st and 3rd component of the reconstruction vectors, as well as the  x -component lagged by 12, 79, 64, and 53 samples. The total decrease of  ΔL  is ~-1.64, and thus, way smaller compared to the univariate case, as we would expect it. Nevertheless, the main contribution to this increase is made by the first two embedding cycles. For suppressing embedding cycles, which yield negligible - but negative -  ΔL -values one can use the keyword argument  L_threshold Y_mt, τ_vals_mt, ts_vals_mt, Ls_mt, εs_mt = pecuzal_embedding(tr;\n    τs = 0:Tmax, L_threshold = 0.2, w = theiler, econ = true\n)\n\nprintln(τ_vals_mt)\nprintln(ts_vals_mt) Initializing PECUZAL algorithm for multivariate input...\nStarting 1-th embedding cycle...\nStarting 2-th embedding cycle...\nStarting 3-th embedding cycle...\nAlgorithm stopped due to increasing L-values. VALID embedding achieved ✓.\n[0, 12, 0]\n[3, 1, 1] As you can see here the algorithm stopped already at 3-dimensional embedding. Let's plot these three components: ts_str = [\"x\", \"y\", \"z\"]\n\nfig = Figure(resolution = (1000,500) )\nax1 = Axis3(fig[1,1], title = \"PECUZAL reconstructed\")\nlines!(ax1, Y_mt[:,1], Y_mt[:,2], Y_mt[:,3]; linewidth = 1.0)\nax1.xlabel = \"$(ts_str[ts_vals_mt[1]])(t+$(τ_vals_mt[1]))\"\nax1.ylabel = \"$(ts_str[ts_vals_mt[2]])(t+$(τ_vals_mt[2]))\"\nax1.zlabel = \"$(ts_str[ts_vals_mt[3]])(t+$(τ_vals_mt[3]))\"\nax1.azimuth = 3π/2 + π/4\n\nax2 = Axis3(fig[1,2], title = \"original\")\nlines!(ax2, tr[:,1], tr[:,2], tr[:,3]; linewidth = 1.0, color = Cycled(2))\nax2.xlabel = \"x(t)\"\nax2.ylabel = \"y(t)\"\nax2.zlabel = \"z(t)\"\nax2.azimuth = π/2 + π/4\nfig Finally we show what PECUZAL does with a non-deterministic source: using Random\n\n# Dummy input\nRandom.seed!(1234)\nd1 = randn(1000)\nd2 = rand(1000)\nTmax = 100\ndummy_set = Dataset(d1,d2)\n\nw1 = estimate_delay(d1, \"mi_min\")\nw2 = estimate_delay(d2, \"mi_min\")\ntheiler = min(w1, w2)\n\nY_d, τ_vals_d, ts_vals_d, Ls_d , ε★_d = pecuzal_embedding(dummy_set; τs = 0:Tmax , w = theiler, econ = true)\n\nsize(Y_d) (1000,) So, no (proper) embedding is done."},{"id":479,"pagetitle":"Unified optimal embedding","title":"All unified algorithms","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#All-unified-algorithms","content":" All unified algorithms Several algorithms have been created to implement a unified approach to delay coordinates embedding. You can find some implementations below:"},{"id":480,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.pecora","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.pecora","content":" DelayEmbeddings.pecora  —  Function pecora(s, τs, js; kwargs...) → ⟨ε★⟩, ⟨Γ⟩ Compute the (average) continuity statistic  ⟨ε★⟩  and undersampling statistic  ⟨Γ⟩  according to Pecora et al. [Pecoral2007]  (A unified approach to attractor reconstruction), for a given input  s  (timeseries or  StateSpaceSet ) and input generalized embedding defined by  (τs, js) , according to  genembed . The continuity statistic represents functional independence between the components of the existing embedding and one additional timeseries. The returned results are  matrices  with size  T x J . Keyword arguments delays = 0:50 : Possible time delay values  delays  (in sampling time units). For each of the  τ 's in  delays  the continuity-statistic  ⟨ε★⟩  gets computed. If  undersampling = true  (see further down), also the undersampling statistic  ⟨Γ⟩  gets returned for all considered delay values. J = 1:dimension(s) : calculate for all timeseries indices in  J . If input  s  is a timeseries, this is always just 1. samplesize::Real = 0.1 : determine the fraction of all phase space points (= length(s) ) to be considered (fiducial points v) to average ε★ to produce  ⟨ε★⟩, ⟨Γ⟩ K::Int = 13 : the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic).  ⟨ε★⟩  is computed taking the minimum result over all  k ∈ K . metric = Chebyshev() : metrix with which to find nearest neigbhors in the input embedding (ℝᵈ space,  d = length(τs) ). w = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. undersampling = false  : whether to calculate the undersampling statistic or not (if not, zeros are returned for  ⟨Γ⟩ ). Calculating  ⟨Γ⟩  is thousands of times slower than  ⟨ε★⟩ . db::Int = 100 : Amount of bins used into calculating the histograms of each timeseries (for the undersampling statistic). α::Real = 0.05 : The significance level for obtaining the continuity statistic p::Real = 0.5 : The p-parameter for the binomial distribution used for the computation of the continuity statistic. Description Notice that the full algorithm is too large to discuss here, and is written in detail (several pages!) in the source code of  pecora . source"},{"id":481,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.uzal_cost","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.uzal_cost","content":" DelayEmbeddings.uzal_cost  —  Function uzal_cost(Y::StateSpaceSet; kwargs...) → L Compute the L-statistic  L  for input dataset  Y  according to Uzal et al. [Uzal2011] , based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. It serves as a cost function of a state space trajectory/embedding and therefore allows to estimate a \"goodness of a embedding\" and also to choose proper embedding parameters, while minimizing  L  over the parameter space. For receiving the local cost function  L_local  (for each point in state space - not averaged), use  uzal_cost_local(...) . Keyword arguments samplesize = 0.5 : Number of considered fiducial points v as a fraction of input state space trajectory  Y 's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce  L . K = 3 : the amount of nearest neighbors considered, in order to compute σ_k^2 (read algorithm description). If given a vector, minimum result over all  k ∈ K  is returned. metric = Euclidean() : metric used for finding nearest neigbhors in the input state space trajectory `Y. w = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. Tw = 40 : The time horizon (in sampling units) up to which E_k^2 gets computed and averaged over (read algorithm description). Description The  L -statistic is based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. Technically, it is the logarithm of the product of  σ -statistic and a normalization statistic  α : L = log10(σ*α) The  σ -statistic is computed as follows.  σ = √σ² = √(E²/ϵ²) .  E²  approximates the conditional variance at each point in state space and for a time horizon  T ∈ Tw , using  K  nearest neighbors. For each reference point of the state space trajectory, the neighborhood consists of the reference point itself and its  K+1  nearest neighbors.  E²  measures how strong a neighborhood expands during  T  time steps.  E²  is averaged over many time horizons  T = 1:Tw . Consequently,  ϵ²  is the size of the neighborhood at the reference point itself and is defined as the mean pairwise distance of the neighborhood. Finally,  σ²  gets averaged over a range of reference points on the attractor, which is controlled by  samplesize . This is just for performance reasons and the most accurate result will obviously be gained when setting  samplesize=1.0 The  α -statistic is a normalization factor, such that  σ 's from different embeddings can be compared.  α²  is defined as the inverse of the sum of the inverse of all  ϵ² 's for all considered reference points. source"},{"id":482,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.garcia_almeida_embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.garcia_almeida_embedding","content":" DelayEmbeddings.garcia_almeida_embedding  —  Function garcia_almeida_embedding(s; kwargs...) → Y, τ_vals, ts_vals, FNNs ,NS A unified approach to properly embed a time series ( Vector  type) or a set of time series ( StateSpaceSet  type) based on the papers of Garcia & Almeida  [Garcia2005a] , [Garcia2005b] . Keyword arguments τs= 0:50 : Possible delay values  τs  (in sampling time units). For each of the  τs 's the N-statistic gets computed. w::Int = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. r1 = 10 : The threshold, which defines the factor of tolerable stretching for the d_E1-statistic. r2 = 2 : The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension. fnn_thres= 0.05 : A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ≤ fnn_thres < 1). T::Int = 1 : The forward time step (in sampling units) in order to compute the  d_E2 -statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to  T=1 . metric = Euclidean() : metric used for finding nearest neigbhors in the input phase space trajectory  Y . max_num_of_cycles = 50 : The algorithm will stop after that many cycles no matter what. Description The method works iteratively and gradually builds the final embedding vectors  Y . Based on the  N -statistic the algorithm picks an optimal delay value  τ  for each embedding cycle as the first local minimum of  N . In case of multivariate embedding, i.e. when embedding a set of time series ( s::StateSpaceSet ), the optimal delay value  τ  is chosen as the first minimum from all minimum's of all considered  N -statistics for each embedding cycle. The range of considered delay values is determined in  τs  and for the nearest neighbor search we respect the Theiler window  w . After each embedding cycle the FNN-statistic  FNNs [Hegger1999] [Kennel1992]  is being checked and as soon as this statistic drops below the threshold  fnn_thres , the algorithm breaks. In order to increase the  practability of the method the algorithm also breaks, when the FNN-statistic  FNNs  increases . The final embedding vector is stored in  Y  ( StateSpaceSet ). The chosen delay values for each embedding cycle are stored in the  τ_vals  and the according time series number chosen for the according delay value in  τ_vals  is stored in  ts_vals . For univariate embedding ( s::Vector )  ts_vals  is a vector of ones of length  τ_vals , because there is simply just one time series to choose from. The function also returns the  N -statistic  NS  for each embedding cycle as an  Array  of  Vector s. Notice that we were  not  able to reproduce the figures from the papers with our implementation (which nevertheless we believe is the correct one). source"},{"id":483,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.mdop_embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.mdop_embedding","content":" DelayEmbeddings.mdop_embedding  —  Function mdop_embedding(s::Vector; kwargs...) → Y, τ_vals, ts_vals, FNNs, βS MDOP (for \"maximizing derivatives on projection\") is a unified approach to properly embed a timeseries or a set of timeseries ( StateSpaceSet ) based on the paper of Chetan Nichkawde  [Nichkawde2013] . Keyword arguments τs= 0:50 : Possible delay values  τs . For each of the  τs 's the β-statistic gets computed. w::Int = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. fnn_thres::Real= 0.05 : A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ≤ fnn_thres < 1). r::Real = 2 : The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension. max_num_of_cycles = 50 : The algorithm will stop after that many cycles no matter what. Description The method works iteratively and gradually builds the final embedding  Y . Based on the  beta_statistic  the algorithm picks an optimal delay value  τ  for each embedding cycle as the global maximum of  β . In case of multivariate embedding, i.e. when embedding a set of time series ( s::StateSpaceSet ), the optimal delay value  τ  is chosen as the maximum from all maxima's of all considered  β -statistics for each possible timeseries. The range of considered delay values is determined in  τs  and for the nearest neighbor search we respect the Theiler window  w . After each embedding cycle the FNN-statistic  FNNs [Hegger1999] [Kennel1992]  is being checked and as soon as this statistic drops below the threshold  fnn_thres , the algorithm terminates. In order to increase the practability of the method the algorithm also terminates when the FNN-statistic  FNNs  increases. The final embedding is returned as  Y . The chosen delay values for each embedding cycle are stored in the  τ_vals  and the according timeseries index chosen for the the respective according delay value in  τ_vals  is stored in  ts_vals .  βS, FNNs  are returned for clarity and double-checking, since they are computed anyway. In case of multivariate embedding,  βS  will store all  β -statistics for all available time series in each embedding cycle. To double-check the actual used  β -statistics in an embedding cycle 'k', simply  βS[k][:,ts_vals[k+1]] . source"},{"id":484,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.pecuzal_embedding","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.pecuzal_embedding","content":" DelayEmbeddings.pecuzal_embedding  —  Function pecuzal_embedding(s; kwargs...) → Y, τ_vals, ts_vals, ΔLs, ⟨ε★⟩ A unified approach to properly embed a timeseries or a set of timeseries ( StateSpaceSet ) based on the recent PECUZAL algorithm due to Kraemer et al. [Kraemer2021] . For more details, see the description below. Keyword arguments τs = 0:50 : Possible delay values  τs  (in sampling time units). For each of the  τs 's the continuity statistic ⟨ε★⟩ gets computed and further processed in order to find optimal delays  τᵢ  for each embedding cycle  i . w::Int = 0 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. samplesize::Real = 1.0 : Fraction of state space points to be considered (fiducial points v) to average ε★ over, in order to produce  ⟨ε★⟩ . Lower fraction value decreases accuracy as well as computation time. K::Int = 13 : the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic).  ⟨ε★⟩  is computed taking the minimum result over all  k ∈ K . KNN::Int = 3 : the amount of nearest neighbors considered, in order to compute σ k^2 (read algorithm description [`uzal cost ]@ref). If given a vector, the minimum result over all knn ∈ KNN` is returned. L_threshold::Real = 0 : The algorithm breaks, when this threshold is exceeded by  ΔL  in an embedding cycle (set as a positive number, i.e. an absolute value of  ΔL ). α::Real = 0.05 : The significance level for obtaining the continuity statistic p::Real = 0.5 : The p-parameter for the binomial distribution used for the computation of the continuity statistic ⟨ε★⟩. max_cycles = 50 : The algorithm will stop after that many cycles no matter what. econ::Bool = false : Economy-mode for L-statistic computation. Instead of computing L-statistics for time horizons  2:Tw , here we only compute them for  2:2:Tw , see description for further details. verbose = true : Print information about the process. Description The method works iteratively and gradually builds the final embedding vectors  Y . Based on the  ⟨ε★⟩ -statistic (of  pecora ) the algorithm picks an optimal delay value  τᵢ  for each embedding cycle  i . For achieving that, we take the inpute time series  s , denoted as the actual phase space trajectory  Y_actual  and compute the continuity statistic  ⟨ε★⟩ . Each local maxima in  ⟨ε★⟩  is used for constructing a candidate embedding trajectory  Y_trial  with a delay corresponding to that specific peak in  ⟨ε★⟩ . We then compute the  L -statistic (of  uzal_cost ) for  Y_trial  ( L-trial ) and  Y_actual  ( L_actual ) for increasing prediction time horizons (free parameter in the  L -statistic) and save the maximum difference  max(L-trial - L_actual)  as  ΔL  (Note that this is a negative number, since the  L -statistic decreases with better reconstructions). We pick the  τ -value, for which  ΔL  is minimal (=maximum decrease of the overall  L -value) and construct the actual embedding trajectory  Y_actual  (steps 1.-3. correspond to an embedding cycle). We repeat steps 1.-3. with  Y_actual  as input and stop the algorithm when  ΔL  is > 0, i.e. when and additional embedding component would not lead to a lower overall L-value.  Y_actual  ->  Y . In case of multivariate embedding, i.e. when embedding a set of M time series ( s::StateSpaceSet ), in each embedding cycle the continuity statistic  ⟨ε★⟩  gets computed for all M time series available. The optimal delay value  τ  in each embedding cycle is chosen as the peak/ τ -value for which  ΔL  is minimal under all available peaks and under all M  ⟨ε★⟩ 's. In the first embedding cycle there will be M² different  ⟨ε★⟩ 's to consider, since it is not clear a priori which time series of the input should consitute the first component of the embedding vector and form  Y_actual . The range of considered delay values is determined in  τs  and for the nearest neighbor search we respect the Theiler window  w . The final embedding vector is stored in  Y  ( StateSpaceSet ). The chosen delay values for each embedding cycle are stored in  τ_vals  and the according time series numbers chosen for each delay value in  τ_vals  are stored in  ts_vals . For univariate embedding ( s::Vector )  ts_vals  is a vector of ones of length  τ_vals , because there is simply just one timeseries to choose from. The function also returns the  ΔLs -values for each embedding cycle and the continuity statistic  ⟨ε★⟩  as an  Array  of  Vector s. For distance computations the Euclidean norm is used. source"},{"id":485,"pagetitle":"Unified optimal embedding","title":"Low-level functions of unified approach","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#Low-level-functions-of-unified-approach","content":" Low-level functions of unified approach"},{"id":486,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.n_statistic","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.n_statistic","content":" DelayEmbeddings.n_statistic  —  Function n_statistic(Y, s; kwargs...) → N, d_E1 Perform one embedding cycle according to the method proposed in  [Garcia2005a]  for a given phase space trajectory  Y  (of type  StateSpaceSet ) and a time series  s (of type Vector ). Return the proposed N-Statistic N and all nearest neighbor distances d_E1 for each point of the input phase space trajectory Y . Note that Y` is a single time series in case of the first embedding cycle. Keyword arguments τs= 0:50 : Considered delay values  τs  (in sampling time units). For each of the  τs 's the N-statistic gets computed. r = 10 : The threshold, which defines the factor of tolerable stretching for the d_E1-statistic (see algorithm description). T::Int = 1 : The forward time step (in sampling units) in order to compute the  d_E2 -statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to  T=1 . w::Int = 0 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. Note that in the paper this is not a free parameter and always  w=0 . metric = Euclidean() : metric used for finding nearest neigbhors in the input phase space trajectory  Y . Description For a range of possible delay values  τs  one constructs a temporary embedding matrix. That is, one concatenates the input phase space trajectory  Y  with the  τ -lagged input time series  s . For each point on the temporary trajectory one computes its nearest neighbor, which is denoted as the  d_E1 -statistic for a specific  τ . Now one considers the distance between the reference point and its nearest neighbor  T  sampling units ahead and calls this statistic  d_E2 .  [Garcia2005a]  strictly use  T=1 , so they forward each reference point and its corresponding nearest neighbor just by one (!) sampling unit. Here it is a free parameter. The  N -statistic is then the fraction of  d_E2 / d_E1 -pairs which exceed a threshold  r . Plotted vs. the considered  τs -values it is proposed to pick the  τ -value for this embedding cycle as the value, where  N  has its first local minimum. source"},{"id":487,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.beta_statistic","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.beta_statistic","content":" DelayEmbeddings.beta_statistic  —  Function beta_statistic(Y::StateSpaceSet, s::Vector) [, τs, w]) → β Compute the β-statistic  β  for input state space trajectory  Y  and a timeseries  s  according to Nichkawde  [Nichkawde2013] , based on estimating derivatives on a projected manifold. For a range of delay values  τs ,  β  gets computed and its maximum over all considered  τs  serves as the optimal delay considered in this embedding cycle. Arguments  τs, w  as in  mdop_embedding . Description The  β -statistic is based on the geometrical idea of maximal unfolding of the reconstructed attractor and is tightly related to the False Nearest Neighbor method ( [Kennel1992] ). In fact the method eliminates the maximum amount of false nearest neighbors in each embedding cycle. The idea is to estimate the absolute value of the directional derivative with respect to a possible new dimension in the reconstruction process, and with respect to the nearest neighbor, for all points of the state space trajectory: ϕ'(τ) = Δϕ d(τ) / Δx d Δx d is simply the Euclidean nearest neighbor distance for a reference point with respect to the given Theiler window  w . Δϕ d(τ) is the distance of the reference point to its nearest neighbor in the one dimensional time series  s , for the specific τ. Δϕ_d(τ) = |s(i+τ)-s(j+τ)|, with i being the index of the considered reference point and j the index of its nearest neighbor. Finally, β  = log β(τ) = ⟨log₁₀ ϕ'(τ)⟩ , with ⟨.⟩ being the mean over all reference points. When one chooses the maximum of  β  over all considered τ's, one obtains the optimal delay value for this embedding cycle. Note that in the first embedding cycle, the input state space trajectory  Y  can also be just a univariate time series. source"},{"id":488,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.mdop_maximum_delay","ref":"/DynamicalSystemsDocs.jl/delayembeddings/stable/unified/#DelayEmbeddings.mdop_maximum_delay","content":" DelayEmbeddings.mdop_maximum_delay  —  Function mdop_maximum_delay(s, tw = 1:50, samplesize = 1.0)) -> τ_max, L Compute an upper bound for the search of optimal delays, when using  mdop_embedding mdop_embedding  or  beta_statistic beta_statistic . Description The input time series  s  gets embedded with unit lag and increasing dimension, for dimensions (or time windows)  tw  ( RangeObject ). For each of such a time window the  L -statistic from Uzal et al.  [Uzal2011]  will be computed.  samplesize  determines the fraction of points to be considered in the computation of  L  (see  uzal_cost ). When this statistic reaches its global minimum the maximum delay value  τ_max  gets returned. When  s  is a multivariate  StateSpaceSet ,  τ_max  will becomputed for all timeseries of that StateSpaceSet and the maximum value will be returned. The returned  L -statistic has size  (length(tw), size(s,2)) . source Pecora2007 Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007).  A unified approach to attractor reconstruction. Chaos 17(1) . Uzal2011 Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011).  Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223 . Garcia2005a Garcia, S. P., Almeida, J. S. (2005).  Nearest neighbor embedding with different time delays. Physical Review E 71, 037204 . Garcia2005b Garcia, S. P., Almeida, J. S. (2005).  Multivariate phase space reconstruction by nearest neighbor embedding with different time delays. Physical Review E 72, 027205 . Nichkawde2013 Nichkawde, Chetan (2013).  Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905 . Hegger1999 Hegger, Rainer and Kantz, Holger (1999).  Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970 . Kennel1992 Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992).  Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403 . Kraemer2021 Kraemer, K.H., Datseris, G., Kurths, J., Kiss, I.Z., Ocampo-Espindola, Marwan, N. (2021)  A unified and automated approach to attractor reconstruction. New Journal of Physics 23(3), 033017 . Garcia2005a Garcia, S. P., Almeida, J. S. (2005).  Nearest neighbor embedding with different time delays. Physical Review E 71, 037204 . Nichkawde2013 Nichkawde, Chetan (2013).  Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905 . Kennel1992 Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992).  Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403 . Nichkawde2013 Nichkawde, Chetan (2013).  Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905 . Uzal2011 Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011).  Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223 ."},{"id":491,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.jl","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.jl","content":" FractalDimensions.jl"},{"id":492,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions","content":" FractalDimensions  —  Module FractalDimensions.jl A Julia package that estimates various definitions of fractal dimension from data. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"FractalDimensions\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was part of ChaosTools.jl. Publication FractalDimensions.jl is used in a review article comparing various estimators for fractal dimensions. The paper is likely a relevant read if you are interested in the package. And if you use the package, please cite the paper. @article{FractalDimensions.jl,\n  doi = {10.1063/5.0160394},\n  url = {https://doi.org/10.1063/5.0160394},\n  year = {2023},\n  month = oct,\n  publisher = {{AIP} Publishing},\n  volume = {33},\n  number = {10},\n  author = {George Datseris and Inga Kottlarz and Anton P. Braun and Ulrich Parlitz},\n  title = {Estimating fractal dimensions: A comparative review and open source implementations},\n  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science}\n} source"},{"id":493,"pagetitle":"FractalDimensions.jl","title":"Introduction","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Introduction","content":" Introduction Note This package is accompanying a review paper on estimating the fractal dimension:  https://arxiv.org/abs/2109.05937 . The paper is continuing the discussion of chapter 5 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. There are numerous methods that one can use to calculate a so-called \"dimension\" of a dataset which in the context of dynamical systems is called the  Fractal dimension . One way to do this is to estimate the  scaling behaviour of some quantity as a size/scale increases . In the  Fractal dimension example  below, one finds the scaling of the correlation sum versus a ball radius. In this case, it approximately holds $ \\log(C) \\approx \\Delta\\log(\\varepsilon) $ for radius  $\\varepsilon$ . The scaling of many other quantities can be estimated as well, such as the generalized entropy, the Higuchi length, or others provided here. To actually find  $\\Delta$ , one needs to find a linearly scaling region in the graph  $\\log(C)$  vs.  $\\log(\\varepsilon)$  and estimate its slope. Hence,  identifying a linear region is central to estimating a fractal dimension . That is why, the section  Linear scaling regions  is of central importance for this documentation."},{"id":494,"pagetitle":"FractalDimensions.jl","title":"Fractal dimension example","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Fractal-dimension-example","content":" Fractal dimension example In this simplest example we will calculate the fractal dimension of the  chaotic attractor of the Hénon map  (for default parameters). For this example, we will generate the data on the spot: using DynamicalSystemsBase # for simulating dynamical systems\nusing CairoMakie           # for plotting\n\nhenon_rule(x, p, n) = SVector(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nu0 = zeros(2)\np0 = [1.4, 0.3]\nhenon = DeterministicIteratedMap(henon_rule, u0, p0)\n\nX, t = trajectory(henon, 20_000; Ttr = 100)\nscatter(X[:, 1], X[:, 2]; color = (\"black\", 0.01), markersize = 4) instead of simulating the set  X  we could load it from disk, e.g., if there was a text file with two columns as x and y coordinates, we would load it as using DelimitedFiles\nfile = \"path/to/file.csv\"\nM = readdlm(file)    # here `M` is a metrix with two columns\nX = StateSpaceSet(M) # important to convert to a state space set After we have  X , we can start computing a fractal dimension and for this example we will use the  correlationsum . Our goal is to compute the correlation sum of  X  for many different sizes/radii  ε . This is as simple as using FractalDimensions\nες = 2 .^ (-15:0.5:5) # semi-random guess\nCs = correlationsum(X, ες; show_progress = false) 41-element Vector{Float64}:\n 1.8799060046997648e-6\n 2.884855757212139e-6\n 4.514774261286935e-6\n 7.204639768011599e-6\n 1.1174441277936103e-5\n 1.7944102794860256e-5\n 2.8333583320833955e-5\n 4.5037748112594364e-5\n 6.943652817359132e-5\n 0.0001071696415179241\n ⋮\n 0.9486205889705515\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999 For a fractal set  X  dynamical systems theory says that there should be an exponential relationship between the correlation sum and the sizes: xs = log2.(ες)\nys = log2.(Cs)\nscatterlines(xs, ys; axis = (ylabel = L\"\\log(C_2)\", xlabel = L\"\\log (\\epsilon)\")) The slope of the linear scaling region of the above plot is the fractal dimension (based on the correlation sum). Given that we  see  the plot, we can estimate where the linear scaling region starts and ends. This is generally done using  LargestLinearRegion  in  slopefit . But first, let's visualize what the method does, as it uses  linear_regions . lrs, slopes = linear_regions(xs, ys, tol = 0.25)\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = L\"\\log(C_2)\", xlabel = L\"\\log (\\epsilon)\")\nfor r in lrs\n    scatterlines!(ax, xs[r], ys[r])\nend\nfig The  LargestLinearRegion  method finds, and computes the slope of, the largest region: Δ = slopefit(xs, ys, LargestLinearRegion()) (1.2318376178087478, 1.2233720116518771, 1.2403032239656184) This result is an approximation of  a  fractal dimension. The whole above pipeline we went through is bundled in  grassberger_proccacia_dim . Similar work is done by  generalized_dim  and many other functions. Be wary when using `xxxxx_dim` As stated clearly by the documentation strings, all pre-made dimension estimating functions (ending in  _dim ) perform a lot of automated steps, each having its own heuristic choices for function default values. They are more like convenient bundles with on-average good defaults, rather than precise functions. You should be careful when considering the validity of the returned number!"},{"id":495,"pagetitle":"FractalDimensions.jl","title":"Linear scaling regions","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Linear-scaling-regions","content":" Linear scaling regions"},{"id":496,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.slopefit","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.slopefit","content":" FractalDimensions.slopefit  —  Function slopefit(x, y [, t::SLopeFit]; kw...) → s, s05, s95 Fit a linear scaling region in the curve of the two  AbstractVectors y  versus  x  using  t  as the estimation method. Return the estimated slope, as well as the confidence intervals for it. The methods  t  that can be used for the estimation are: LinearRegression LargestLinearRegion  (default) AllSlopesDistribution The keyword  ignore_saturation = true  ignores saturation that (sometimes) happens at the start and end of the curve  y(x) , where the curve flattens. The keyword  sat_threshold = 0.01  decides what saturation is: while  abs(y[i]-y[i+1]) < sat_threshold  we are in a saturation regime. Said differently, slopes with value  sat_threshold/dx  with  dx = x[i+1] - x[i]  are neglected. The keyword  ci = 0.95  specifies which quantile (and the 1 - quantile) the confidence interval values are returned at, and by defualt it is 95% (and hence also 5%). source"},{"id":497,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.LinearRegression","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.LinearRegression","content":" FractalDimensions.LinearRegression  —  Type LinearRegression <: SlopeFit\nLinearRegression() Standard linear regression fit to all available data. Estimation of the confidence intervals is based om the standard error of the slope following a T-distribution, see: https://stattrek.com/regression/slope-confidence-interval source"},{"id":498,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.LargestLinearRegion","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.LargestLinearRegion","content":" FractalDimensions.LargestLinearRegion  —  Type LargestLinearRegion <: SlopeFit\nLargestLinearRegion(; dxi::Int = 1, tol = 0.25) Identify regions where the curve  y(x)  is linear, by scanning the  x -axis every  dxi  indices sequentially (e.g. at  x[1]  to  x[5] ,  x[5]  to  x[10] ,  x[10]  to  x[15]  and so on if  dxi=5 ). If the slope (calculated via linear regression) of a region of width  dxi  is approximatelly equal to that of the previous region, within relative tolerance  tol  and absolute tolerance  0 , then these two regions belong to the same linear region. The largest such region is then used to estimate the slope via standard linear regression of all points belonging to the largest linear region. \"Largest\" here means the region that covers the more extent along the  x -axis. Use  linear_regions  if you wish to obtain the decomposition into linear regions. source"},{"id":499,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.linear_regions","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.linear_regions","content":" FractalDimensions.linear_regions  —  Function linear_regions(x, y; dxi, tol) → lrs, tangents Apply the algorithm described by  LargestLinearRegion , and return the indices of  x  that correspond to the linear regions,  lrs , and the  tangents  at each region (obtained via a second linear regression at each accumulated region).  lrs  is hence a vector of  UnitRange s. source"},{"id":500,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.AllSlopesDistribution","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.AllSlopesDistribution","content":" FractalDimensions.AllSlopesDistribution  —  Type AllSlopesDistribution <: SlopeFit\nAllSlopesDistribution() Estimate a slope by computing the distribution of all possible slopes that can be estimated from the curve  y(x) , according to the method by  (Deshmukh  et al. , 2021) . The returned slope is the distribution mean and the confidence intervals are simply the corresponding quantiles of the distribution. Not implemented yet, the method is here as a placeholder. source"},{"id":501,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_boxsizes","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.estimate_boxsizes","content":" FractalDimensions.estimate_boxsizes  —  Function estimate_boxsizes(X::AbstractStateSpaceSet; kwargs...) → εs Return  k  exponentially spaced values:  εs = base .^ range(lower + w, upper + z; length = k) , that are a good estimate for sizes ε that are used in calculating a  Fractal Dimension . It is strongly recommended to  standardize  input dataset before using this function. Let  d₋  be the minimum pair-wise distance in  X ,  d₋ = dminimum_pairwise_distance(X) . Let  d₊  be the average total length of  X ,  d₊ = mean(ma - mi)  with  mi, ma = minmaxima(X) . Then  lower = log(base, d₋)  and  upper = log(base, d₊) . Because by default  w=1, z=-1 , the returned sizes are an order of mangitude larger than the minimum distance, and an order of magnitude smaller than the maximum distance. Keywords w = 1, z = -1, k = 16  : as explained above. base = MathConstants.e  : the base used in the  log  function. warning = true : Print some warnings for bad estimates. autoexpand = true : If the final estimated range does not cover at least 2 orders of magnitude, it is automatically expanded by setting  w -= we  and  z -= ze . You can set different default values to the keywords  we = w, ze = z . source"},{"id":502,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.minimum_pairwise_distance","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.minimum_pairwise_distance","content":" FractalDimensions.minimum_pairwise_distance  —  Function minimum_pairwise_distance(X::StateSpaceSet, kdtree = dimension(X) < 10, metric = Euclidean()) Return  min_d, min_pair : the minimum pairwise distance of all points in the dataset, and the corresponding point pair. The third argument is a switch of whether to use KDTrees or a brute force search. source"},{"id":503,"pagetitle":"FractalDimensions.jl","title":"Generalized (entropy) dimension","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Generalized-(entropy)-dimension","content":" Generalized (entropy) dimension Based on the definition of the Generalized entropy ( genentropy ), one can calculate an appropriate dimension, called  generalized dimension :"},{"id":504,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.generalized_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.generalized_dim","content":" FractalDimensions.generalized_dim  —  Function generalized_dim(X::StateSpaceSet [, sizes]; q = 1, base = 2) -> Δ_q Return the  q  order generalized dimension of  X , by calculating  its histogram-based Rényi entropy for each  ε ∈ sizes . The case of  q = 0  is often called \"capacity\" or \"box-counting\" dimension, while  q = 1  is the \"information\" dimension. Description The returned dimension is approximated by the (inverse) power law exponent of the scaling of the Renyi entropy  $H_q$ , versus the box size  ε , where  ε ∈ sizes : \\[H_q \\approx -\\Delta_q\\log_{b}(\\varepsilon)\\] $H_q$  is calculated using  ComplexityMeasures: Renyi, ValueHistogram, entropy , i.e., by doing a histogram of the data with a given box size. Calling this function performs a lot of automated steps: A vector of box sizes is decided by calling  sizes = estimate_boxsizes(dataset) , if  sizes  is not given. For each element of  sizes  the appropriate entropy is calculated as H = [entropy(Renyi(; q, base), ValueHistogram(ε), data) for ε ∈ sizes] Let  x = -log.(sizes) . The curve  H(x)  is decomposed into linear regions, using  slopefit (x, h)[1] . The biggest linear region is chosen, and a fit for the slope of that region is performed using the function  linear_region , which does a simple linear regression fit using  linreg . This slope is the return value of  generalized_dim . By doing these steps one by one yourself, you can adjust the keyword arguments given to each of these function calls, refining the accuracy of the result. The source code of this function is only 3 lines of code. This approach to estimating the fractal dimension has been used (to our knowledge) for the first time in  (Russell  et al. , 1980) . source"},{"id":505,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.molteno_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.molteno_dim","content":" FractalDimensions.molteno_dim  —  Function molteno_dim(X::AbstractStateSpaceSet; k0::Int = 10, q = 1.0, base = 2) Return an estimate of the  generalized_dim  of  X  using the algorithm by  (Molteno, 1993) . This function is a simple utilization of the probabilities estimated by  molteno_boxing  so see that function for more details. Here the entropy of the probabilities is computed at each size, and a line is fitted in the entropy vs log(size) graph, just like in  generalized_dim . source"},{"id":506,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.molteno_boxing","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.molteno_boxing","content":" FractalDimensions.molteno_boxing  —  Function molteno_boxing(X::AbstractStateSpaceSet; k0::Int = 10) → (probs, εs) Distribute  X  into boxes whose size is halved in each step, according to the algorithm by  (Molteno, 1993) . Division stops if the average number of points per filled box falls below the threshold  k0 . Return  probs , a vector of  Probabilities  of finding points in boxes for different box sizes, and the corresponding box sizes  εs . These outputs are used in  molteno_dim . Description Project the  data  onto the whole interval of numbers that is covered by  UInt64 . The projected data is distributed into boxes whose size decreases by factor 2 in each step. For each box that contains more than one point  2^D  new boxes are created where  D  is the dimension of the data. The process of dividing the data into new boxes stops when the number of points over the number of filled boxes falls below  k0 . The box sizes  εs  are calculated and returned together with the  probs . This algorithm is faster than the traditional approach of using  ValueHistogram(ε::Real) , but it is only suited for low dimensional data since it divides each box into  2^D  new boxes if  D  is the dimension. For large  D  this leads to low numbers of box divisions before the threshold is passed and the divison stops. This results to a low number of data points to fit the dimension to and thereby a poor estimate. source"},{"id":507,"pagetitle":"FractalDimensions.jl","title":"Correlation sum based dimension","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Correlation-sum-based-dimension","content":" Correlation sum based dimension"},{"id":508,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.grassberger_proccacia_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.grassberger_proccacia_dim","content":" FractalDimensions.grassberger_proccacia_dim  —  Function grassberger_proccacia_dim(X::AbstractStateSpaceSet, εs = estimate_boxsizes(data); kwargs...) Use the method of Grassberger and Proccacia  (Grassberger and Procaccia, 1983) , and the correction by  (Theiler, 1986) , to estimate the correlation dimension  Δ_C  of   X . This function does something extremely simple: cm = correlationsum(data, εs; kwargs...)\nΔ_C = slopefit(rs, ys)(log2.(sizes), log2.(cm))[1] i.e. it calculates  correlationsum  for various radii and then tries to find a linear region in the plot of the log of the correlation sum versus log(ε). See  correlationsum  for the available keywords. See also  takens_best_estimate ,  boxassisted_correlation_dim . source"},{"id":509,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.correlationsum","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.correlationsum","content":" FractalDimensions.correlationsum  —  Function correlationsum(X, ε::Real; w = 0, norm = Euclidean(), q = 2) → C_q(ε) Calculate the  q -order correlation sum of  X  ( StateSpaceSet  or timeseries) for a given radius  ε  and  norm . They keyword  show_progress = true  can be used to display a progress bar for large  X . correlationsum(X, εs::AbstractVector; w, norm, q) → C_q(ε) If  εs  is a vector,  C_q  is calculated for each  ε ∈ εs  more efficiently. Multithreading is also enabled over the available threads ( Threads.nthreads() ). The function  boxed_correlationsum  is typically faster if the dimension of  X  is small and if  maximum(εs)  is smaller than the size of  X . Keyword arguments q = 2 : order of the correlation sum norm = Euclidean() : distance norm w = 0 : Theiler window show_progress = true : display a progress bar Description The correlation sum is defined as follows for  q=2 : \\[C_2(\\epsilon) = \\frac{2}{(N-w)(N-w-1)}\\sum_{i=1}^{N}\\sum_{j=1+w+i}^{N}\nB(||X_i - X_j|| < \\epsilon)\\] for as follows for  q≠2 \\[C_q(\\epsilon) = \\left[ \\sum_{i=1}^{N} \\alpha_i\n\\left[\\sum_{j:|i-j| > w} B(||X_i - X_j|| < \\epsilon)\\right]^{q-1}\\right]^{1/(q-1)}\\] where \\[\\alpha_i = 1 / (N (\\max(N-w, i) - \\min(w + 1, i))^{(q-1)})\\] with  $N$  the length of  X  and  $B$  gives 1 if its argument is  true .  w  is the  Theiler window . See the article of Grassberger for the general definition  (Grassberger, 2007)  and the book \"Nonlinear Time Series Analysis\"  (Kantz and Schreiber, 2003) , Ch. 6, for a discussion around choosing best values for  w , and Ch. 11.3 for the explicit definition of the q-order correlationsum. Note that the formula in 11.3 is incorrect, but corrected here, indices are adapted to take advantage of all available points and also note that we immediatelly exponentiate  $C_q$  to  $1/(q-1)$ , so that it scales exponentially as  $C_q \\propto \\varepsilon ^\\Delta_q$  versus the size  $\\varepsilon$ . source"},{"id":510,"pagetitle":"FractalDimensions.jl","title":"Box-assisted version","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Box-assisted-version","content":" Box-assisted version"},{"id":511,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.boxassisted_correlation_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.boxassisted_correlation_dim","content":" FractalDimensions.boxassisted_correlation_dim  —  Function boxassisted_correlation_dim(X::AbstractStateSpaceSet; kwargs...) Use the box-assisted optimizations of  (Bueno-Orovio and P{é}rez-Garc{ı́}a, 2007)  to estimate the correlation dimension  Δ_C  of  X . This function does something extremely simple: εs, Cs = boxed_correlationsum(X; kwargs...)\nslopefit(log2.(εs), log2.(Cs))[1] and hence see  boxed_correlationsum  for more information and available keywords. source"},{"id":512,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.boxed_correlationsum","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.boxed_correlationsum","content":" FractalDimensions.boxed_correlationsum  —  Function boxed_correlationsum(X::AbstractStateSpaceSet, εs, r0 = maximum(εs); kwargs...) → Cs Estimate the  correlationsum  for each size  ε ∈ εs  using an optimized algorithm that first distributes data into boxes of size  r0 , and then computes the correlation sum for each box and each neighboring box of each box. This method is much faster than  correlationsum ,  provided that  the box size  r0  is significantly smaller than the attractor length. Good choices for  r0  are  estimate_r0_buenoorovio  or  estimate_r0_theiler . boxed_correlationsum(X::AbstractStateSpaceSet; kwargs...) → εs, Cs In this method the minimum inter-point distance and  estimate_r0_buenoorovio  of  X  are used to estimate suitable  εs  for the calculation, which are also returned. Keyword arguments q = 2  : The order of the correlation sum. P = 2  : The prism dimension. w = 0  : The  Theiler window . show_progress = false  : Whether to display a progress bar for the calculation. norm = Euclidean()  : Distance norm. Description C_q(ε)  is calculated for every  ε ∈ εs  and each of the boxes to then be summed up afterwards. The method of splitting the data into boxes was implemented according to  (Theiler, 1987) .  w  is the  Theiler window .  P  is the prism dimension. If  P  is unequal to the dimension of the data, only the first  P  dimensions are considered for the box distribution (this is called the prism-assisted version). By default  P  is 2, which is the version suggested by  [Bueno2007] . Alternative for  P  is the  prismdim_theiler . Note that only when  P = dimension(X)  the boxed version is guaranteed to be exact to the original  correlationsum . For any other  P , some point pairs that should have been included may be skipped due to having smaller distance in the remaining dimensions, but larger distance in the first  P  dimensions. source"},{"id":513,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.prismdim_theiler","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.prismdim_theiler","content":" FractalDimensions.prismdim_theiler  —  Function prismdim_theiler(X) An algorithm to find the ideal choice of a prism dimension for  boxed_correlationsum  using Theiler's original suggestion. source"},{"id":514,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_r0_buenoorovio","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.estimate_r0_buenoorovio","content":" FractalDimensions.estimate_r0_buenoorovio  —  Function estimate_r0_buenoorovio(X::AbstractStateSpaceSet, P = 2) → r0, ε0 Estimate a reasonable size for boxing  X , proposed by Bueno-Orovio and Pérez-García  (Bueno-Orovio and P{é}rez-Garc{ı́}a, 2007) , before calculating the correlation dimension as presented by  Theiler1983 . Return the size  r0  and the minimum interpoint distance  ε0  in the data. If instead of boxes, prisms are chosen everything stays the same but  P  is the dimension of the prism. To do so the dimension  ν  is estimated by running the algorithm by Grassberger and Procaccia  (Grassberger and Procaccia, 1983)  with  √N  points where  N  is the number of total data points. An effective size  ℓ  of the attractor is calculated by boxing a small subset of size  N/10  into boxes of sidelength  r_ℓ  and counting the number of filled boxes  η_ℓ . \\[\\ell = r_\\ell \\eta_\\ell ^{1/\\nu}\\] The optimal number of filled boxes  η_opt  is calculated by minimising the number of calculations. \\[\\eta_\\textrm{opt} = N^{2/3}\\cdot \\frac{3^\\nu - 1}{3^P - 1}^{1/2}.\\] P  is the dimension of the data or the number of edges on the prism that don't span the whole dataset. Then the optimal boxsize  $r_0$  computes as \\[r_0 = \\ell / \\eta_\\textrm{opt}^{1/\\nu}.\\] source"},{"id":515,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_r0_theiler","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.estimate_r0_theiler","content":" FractalDimensions.estimate_r0_theiler  —  Function estimate_r0_theiler(X::AbstractStateSpaceSet) → r0, ε0 Estimate a reasonable size for boxing the data  X  before calculating the  boxed_correlationsum  proposed by  (Theiler, 1987) . Return the boxing size  r0  and minimum inter-point distance in  X ,  ε0 . To do so the dimension is estimated by running the algorithm by Grassberger and Procaccia  (Grassberger and Procaccia, 1983)  with  √N  points where  N  is the number of total data points. Then the optimal boxsize  $r_0$  computes as \\[r_0 = R (2/N)^{1/\\nu}\\] where  $R$  is the size of the chaotic attractor and  $\\nu$  is the estimated dimension. source"},{"id":516,"pagetitle":"FractalDimensions.jl","title":"Fixed mass correlation sum","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Fixed-mass-correlation-sum","content":" Fixed mass correlation sum"},{"id":517,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.fixedmass_correlation_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.fixedmass_correlation_dim","content":" FractalDimensions.fixedmass_correlation_dim  —  Function fixedmass_correlation_dim(X [, max_j]; kwargs...) Use the fixed mass algorithm for computing the correlation sum, and use the result to compute the correlation dimension  Δ_M  of  X . This function does something extremely simple: rs, ys = fixedmass_correlationsum(X, args...; kwargs...)\nslopefit(rs, ys)[1] source"},{"id":518,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.fixedmass_correlationsum","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.fixedmass_correlationsum","content":" FractalDimensions.fixedmass_correlationsum  —  Function fixedmass_correlationsum(X [, max_j]; metric = Euclidean(), M = length(X)) → rs, ys A fixed mass algorithm for the calculation of the  correlationsum , and subsequently a fractal dimension  $\\Delta$ , with  max_j  the maximum number of neighbours that should be considered for the calculation. By default  max_j = clamp(N*(N-1)/2, 5, 32)  with  N  the data length. Keyword arguments M  defines the number of points considered for the averaging of distances, randomly subsampling them from  X . metric = Euclidean()  is the distance metric. start_j = 4  computes the equation below starting from  j = start_j . Typically the first  j  values have not converged to the correct scaling of the fractal dimension. Description \"Fixed mass\" algorithms mean that instead of trying to find all neighboring points within a radius, one instead tries to find the max radius containing  j  points. A correlation sum is obtained with this constrain, and equivalently the mean radius containing  k  points. Based on this, one can calculate  $\\Delta$  approximating the information dimension. The implementation here is due to to  (Grassberger, 1988) , which defines \\[Ψ(j) - \\log N \\sim \\Delta \\times \\overline{\\log \\left( r_{(j)}\\right)}\\] where  $\\Psi(j) = \\frac{\\text{d} \\log Γ(j)}{\\text{d} j}$  is the digamma function,  rs  =  $\\overline{\\log \\left( r_{(j)}\\right)}$  is the mean logarithm of a radius containing  j  neighboring points, and  ys  =  $\\Psi(j) - \\log N$  ( $N$  is the length of the data). The amount of neighbors found  $j$  range from 2 to  max_j . The numbers are also converted to base  $2$  from base  $e$ . $\\Delta$  can be computed by using  linear_region(rs, ys) . source"},{"id":519,"pagetitle":"FractalDimensions.jl","title":"Takens best estimate","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Takens-best-estimate","content":" Takens best estimate"},{"id":520,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.takens_best_estimate_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.takens_best_estimate_dim","content":" FractalDimensions.takens_best_estimate_dim  —  Function takens_best_estimate_dim(X, εmax, metric = Chebyshev(), εmin = 0) Use the \"Takens' best estimate\"  [Takens1985] [Theiler1988]  method for estimating the correlation dimension. The original formula is \\[\\Delta_C \\approx \\frac{C(\\epsilon_\\text{max})}{\\int_0^{\\epsilon_\\text{max}}(C(\\epsilon) / \\epsilon) \\, d\\epsilon}\\] where  $C$  is the  correlationsum  and  $\\epsilon_\\text{max}$  is an upper cutoff. Here we use the later expression \\[\\Delta_C \\approx - \\frac{1}{\\eta},\\quad \\eta = \\frac{1}{(N-1)^*}\\sum_{[i, j]^*}\\log(||X_i - X_j|| / \\epsilon_\\text{max})\\] where the sum happens for all  $i, j$  so that  $i < j$  and  $||X_i - X_j|| < \\epsilon_\\text{max}$ . In the above expression, the bias in the original paper has already been corrected, as suggested in  [Borovkova1999] . According to  [Borovkova1999] , introducing a lower cutoff  εmin  can make the algorithm more stable (no divergence), this option is given but defaults to zero. If  X  comes from a delay coordinates embedding of a timseries  x , a recommended value for  $\\epsilon_\\text{max}$  is  std(x)/4 . You may also use Δ_C, Δu_C, Δl_C = FractalDimensions.takens_best_estimate(args...) to obtain the upper and lower 95% confidence intervals. The intervals are estimated from the log-likelihood function by finding the values of  Δ_C  where the function has fallen by 2 from its maximum, see e.g.  [Barlow]  chapter 5.3. source"},{"id":521,"pagetitle":"FractalDimensions.jl","title":"Kaplan-Yorke dimension","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Kaplan-Yorke-dimension","content":" Kaplan-Yorke dimension"},{"id":522,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.kaplanyorke_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.kaplanyorke_dim","content":" FractalDimensions.kaplanyorke_dim  —  Function kaplanyorke_dim(λs::AbstractVector) Calculate the Kaplan-Yorke dimension, a.k.a. Lyapunov dimension  (Kaplan and Yorke, 1979)  from the given Lyapunov exponents  λs . Description The Kaplan-Yorke dimension is simply the point where  cumsum(λs)  becomes zero (interpolated): \\[ D_{KY} = k + \\frac{\\sum_{i=1}^k \\lambda_i}{|\\lambda_{k+1}|},\\quad k = \\max_j \\left[ \\sum_{i=1}^j \\lambda_i > 0 \\right].\\] If the sum of the exponents never becomes negative the function will return the length of the input vector. Useful in combination with  lyapunovspectrum  from ChaosTools.jl. source"},{"id":523,"pagetitle":"FractalDimensions.jl","title":"Higuchi dimension","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Higuchi-dimension","content":" Higuchi dimension"},{"id":524,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.higuchi_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.higuchi_dim","content":" FractalDimensions.higuchi_dim  —  Function higuchi_dim(x::AbstractVector [, ks]) Estimate the Higuchi dimension  (Higuchi, 1988)  of the graph of  x . Description The Higuchi dimension is a number  Δ ∈ [1, 2]  that quantifies the roughness of the graph of the function  x(t) , assuming here that  x  is equi-sampled, like in the original paper. The method estimates how the length of the graph increases as a function of the indices difference (which, in this context, is equivalent with differences in  t ). Specifically, we calculate the average length versus  k  as \\[L_m(k) = \\frac{N-1}{\\lfloor \\frac{N-m}{k} \nfloor k^2}\n\\sum_{i=1}^{\\lfloor \\frac{N-m}{k} \\rfloor} |X_N(m+ik)-X_N(m+(i-1)k)| \\\\\n\nL(k) = \\frac{1}{k} \\sum_{m=1}^k L_m(k)\\] and then use  linear_region  in  -log2.(k)  vs  log2.(L)  as per usual when computing a fractal dimension. The algorithm chooses default  ks  to be exponentially spaced in base-2, up to at most  2^8 . A user can provide their own  ks  as a second argument otherwise. Use  FractalDimensions.higuchi_length(x, ks)  to obtain  $L(k)$  directly. source"},{"id":525,"pagetitle":"FractalDimensions.jl","title":"Extreme value value theory dimensions","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Extreme-value-value-theory-dimensions","content":" Extreme value value theory dimensions The central function for this is  extremevaltheory_dims_persistences  which utilizes either  Exceedances  or  BlockMaxima ."},{"id":526,"pagetitle":"FractalDimensions.jl","title":"Main functions","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Main-functions","content":" Main functions"},{"id":527,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_dims_persistences","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.extremevaltheory_dims_persistences","content":" FractalDimensions.extremevaltheory_dims_persistences  —  Function extremevaltheory_dims_persistences(x::AbstractStateSpaceSet, est; kwargs...) Return the local dimensions  Δloc  and the persistences  θloc  for each point in the given set according to extreme value theory  (Lucarini  et al. , 2016) . The type of  est  decides which approach to use when computing the dimension. The possible estimators are: BlockMaxima Exceedances The computation is parallelized to available threads ( Threads.nthreads() ). See also  extremevaltheory_gpdfit_pvalues  for obtaining confidence on the results. Keyword arguments show_progress = true : displays a progress bar. compute_persistence = true:  whether to aso compute local persistences  θloc  (also called extremal indices). If  false ,  θloc  are  NaN s. source"},{"id":528,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_dim","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.extremevaltheory_dim","content":" FractalDimensions.extremevaltheory_dim  —  Function extremevaltheory_dim(X::StateSpaceSet, p; kwargs...) → Δ Convenience syntax that returns the mean of the local dimensions of  extremevaltheory_dims_persistences  with  X, p . source"},{"id":529,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_dims","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.extremevaltheory_dims","content":" FractalDimensions.extremevaltheory_dims  —  Function extremevaltheory_dims(X::StateSpaceSet, p; kwargs...) → Δloc Convenience syntax that returns the local dimensions of  extremevaltheory_dims_persistences  with  X, p . source"},{"id":530,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_local_dim_persistence","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.extremevaltheory_local_dim_persistence","content":" FractalDimensions.extremevaltheory_local_dim_persistence  —  Function extremevaltheory_local_dim_persistence(X::StateSpaceSet, ζ, p; kw...) Return the local values  Δ, θ  of the fractal dimension and persistence of  X  around a state space point  ζ .  p  and  kw  are as in  extremevaltheory_dims_persistences . source"},{"id":531,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremal_index_sueveges","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.extremal_index_sueveges","content":" FractalDimensions.extremal_index_sueveges  —  Function extremal_index_sueveges(y::AbstractVector, p) Compute the extremal index θ of  y  through the Süveges formula for quantile probability  p , using the algorithm of  (Süveges, 2007) . source"},{"id":532,"pagetitle":"FractalDimensions.jl","title":"Exceedances estimator","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Exceedances-estimator","content":" Exceedances estimator"},{"id":533,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.Exceedances","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.Exceedances","content":" FractalDimensions.Exceedances  —  Type Exceedances(p::Real, estimator::Symbol) Instructions type for  extremevaltheory_dims_persistences  and related functions. This method sets a threshold and fits the exceedances to Generalized Pareto Distribution (GPD). The parameter  p  is a number between 0 and 1 that determines the p-quantile for the threshold and computation of the extremal index. The argument  estimator  is a symbol that decides how the GPD is fitted to the data. It can take the values  :exp, :pwm, :mm , as in  estimate_gpd_parameters . Description For each state space point  $\\mathbf{x}_i$  in  X  we compute  $g_i = -\\log(||\\mathbf{x}_i - \\mathbf{x}_j|| ) \\; \\forall j = 1, \\ldots, N$  with  $||\\cdot||$  the Euclidean distance. Next, we choose an extreme quantile probability  $p$  (e.g., 0.99) for the distribution of  $g_i$ . We compute  $g_p$  as the  $p$ -th quantile of  $g_i$ . Then, we collect the exceedances of  $g_i$ , defined as  $E_i = \\{ g_i - g_p: g_i \\ge g_p \\}$ , i.e., all values of  $g_i$  larger or equal to  $g_p$ , also shifted by  $g_p$ . There are in total  $n = N(1-q)$  values in  $E_i$ . According to extreme value theory, in the limit  $N \\to \\infty$  the values  $E_i$  follow a two-parameter Generalized Pareto Distribution (GPD) with parameters  $\\sigma,\\xi$  (the third parameter  $\\mu$  of the GPD is zero due to the positive-definite construction of  $E$ ). Within this extreme value theory approach, the local dimension  $\\Delta^{(E)}_i$  assigned to state space point  $\\textbf{x}_i$  is given by the inverse of the  $\\sigma$  parameter of the GPD fit to the data [Lucarini2012] ,  $\\Delta^{(E)}_i = 1/\\sigma$ .  $\\sigma$  is estimated according to the  estimator  keyword. A more precise description of this process is given in the review paper  (Datseris  et al. , 2023) . source"},{"id":534,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_gpd_parameters","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.estimate_gpd_parameters","content":" FractalDimensions.estimate_gpd_parameters  —  Function estimate_gpd_parameters(X::AbstractVector{<:Real}, estimator::Symbol) Estimate and return the parameters  σ, ξ  of a Generalized Pareto Distribution fit to  X  (which typically is the exceedances of the log distance of a state space set), assuming that  minimum(X) ≥ 0  and hence the parameter  μ  is 0 (if not, simply shift  X  by its minimum), according to the methods provided in  Pons2023 . The estimator can be: :exp : Assume the distribution is exponential instead of GP and get  σ  from mean of  X  and set  ξ = 0 . mm : Standing for \"method of moments\", estimants are given by \\[\\xi = (\\bar{x}^2/s^2 - 1)/2, \\quad \\sigma = \\bar{x}(\\bar{x}^2/s^2 + 1)/2\\] with  $\\bar{x}$  the sample mean and  $s^2$  the sample variance. This estimator only exists if the true distribution  ξ  value is < 0.5. source"},{"id":535,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_gpdfit_pvalues","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.extremevaltheory_gpdfit_pvalues","content":" FractalDimensions.extremevaltheory_gpdfit_pvalues  —  Function extremevaltheory_gpdfit_pvalues(X, p; kw...) Return various computed quantities that may quantify the significance of the results of  extremevaltheory_dims_persistences (X, p; kw...) , terms of quantifying how well a Generalized Pareto Distribution (GPD) describes exceedences in the input data. Keyword arguments show_progress = true : display a progress bar. TestType = ApproximateOneSampleKSTest : the test type to use. It can be  ApproximateOneSampleKSTest, ExactOneSampleKSTest, CramerVonMises . We noticed that  OneSampleADTest  sometimes yielded nonsensical results: all p-values were equal and were very small ≈ 1e-6. nbins = round(Int, length(X)*(1-p)/20) : number of bins to use when computing the histogram of the exceedances for computing the NRMSE. The default value will use equally spaced bins that are equal to the length of the exceedances divided by 20. Description The function computes the exceedances  $E_i$  for each point  $x_i \\in X$  as in  extremevaltheory_dims_persistences . It returns 5 quantities, all being vectors of length  length(X) : Es , all exceedences, as a vector of vectors. sigmas, xis  the fitted σ, ξ to the GPD fits for each exceedance nrmses  the normalized root mean square distance of the fitted GPD to the histogram of the exceedances pvalues  the pvalues of a statistical test of the appropriateness of the GPD fit The output  nrmses  quantifies the distance between the fitted GPD and the empirical histogram of the exceedances. It is computed as \\[NRMSE = \\sqrt{\\frac{\\sum{(P_j - G_j)^2}{\\sum{(P_j - U)^2}}\\] where  $P_j$  the empirical (observed) probability at bin  $j$ ,  $G_j$  the fitted GPD probability at the midpoint of bin  $j$ , and  $U$  same as  $G_j$  but for the uniform distribution. The divisor of the equation normalizes the expression, so that the error of the empirical distribution is normalized to the error of the empirical distribution with fitting it with the uniform distribution. It is expected that NRMSE < 1. The smaller it is, the better the data are approximated by GPD versus uniform distribution. The output  pvalues  is a vector of p-values.  pvalues[i]  corresponds to the p-value of the hypothesis:  \"The exceedences around point  X[i]  are sampled from a GPD\"  versus the alternative hypothesis that they are not. To extract the p-values, we perform a one-sample hypothesis via HypothesisTests.jl to the fitted GPD. Very small p-values then indicate that the hypothesis should be rejected and the data are not well described by a GPD. This can be an indication that we do not have enough data, or that we choose too high of a quantile probability  p , or that the data are not suitable in general. This p-value based method for significance has been used in  [Faranda2017] , but it is unclear precisely how it was used. For more details on how these quantities may quantify significance, see our review paper. source"},{"id":536,"pagetitle":"FractalDimensions.jl","title":"Block-maxima estimator","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Block-maxima-estimator","content":" Block-maxima estimator"},{"id":537,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.BlockMaxima","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#FractalDimensions.BlockMaxima","content":" FractalDimensions.BlockMaxima  —  Type BlockMaxima(blocksize::Int, p::Real) Instructions type for  extremevaltheory_dims_persistences  and related functions. This method divides the input data into blocks of length  blocksize  and fits the maxima of each block to a Generalized Extreme Value distribution. In order for this method to work correctly, both the  blocksize  and the number of blocks must be high. Note that there are data points that are not used by the algorithm. Since it is not always possible to express the number of input data poins as  N = blocksize * nblocks + 1 . To reduce the number of unused data, chose an  N  equal or superior to  blocksize * nblocks + 1 . This method and several variants of it has been studied in  (Faranda  et al. , 2011) The parameter  p  is a number between 0 and 1 that determines the p-quantile for the computation of the extremal index and hence is irrelevant if  compute_persistences = false  in  extremevaltheory_dims_persistences . See also  estimate_gev_parameters . source"},{"id":538,"pagetitle":"FractalDimensions.jl","title":"Theiler window","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#Theiler-window","content":" Theiler window The Theiler window is a concept that is useful when finding neighbors in a dataset that is coming from the sampling of a continuous dynamical system. Itt tries to eliminate spurious \"correlations\" (wrongly counted neighbors) due to a potentially dense sampling of the trajectory. Typically a good choice for  w  coincides with the choice an optimal delay time, see  DelayEmbeddings.estimate_delay , for any of the timeseries of the dataset. For more details, see Chapter 5 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022."},{"id":539,"pagetitle":"FractalDimensions.jl","title":"StateSpaceSet  reference","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#StateSpaceSet-reference","content":" StateSpaceSet  reference"},{"id":540,"pagetitle":"FractalDimensions.jl","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple."},{"id":541,"pagetitle":"FractalDimensions.jl","title":"StateSpaceSets.standardize","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#StateSpaceSets.standardize","content":" StateSpaceSets.standardize  —  Function standardize(d::StateSpaceSet) → r Create a standardized version of the input set where each column is transformed to have mean 0 and standard deviation 1. standardize(x::AbstractVector{<:Real}) = (x - mean(x))/std(x)"},{"id":542,"pagetitle":"FractalDimensions.jl","title":"References","ref":"/DynamicalSystemsDocs.jl/fractaldimensions/stable/#References","content":" References Bueno-Orovio, A. and P{é}rez-Garc{ı́}a, V. M. (2007).  Enhanced box and prism assisted algorithms for computing the correlation dimension .  Chaos,  Solitons {\\&}amp$\\mathsemicolon$ Fractals  34 , 509–518 . Datseris, G.; Kottlarz, I.; Braun, A. P. and Parlitz, U. (2023).  Estimating fractal dimensions: A comparative review and open source implementations .  Chaos: An Interdisciplinary Journal of Nonlinear Science  33 . Deshmukh, V.; Bradley, E.; Garland, J. and Meiss, J. D. (2021).  Toward automated extraction and characterization of scaling regions in dynamical systems .  Chaos: An Interdisciplinary Journal of Nonlinear Science  31 . Faranda, D.; Lucarini, V.; Turchetti, G. and Vaienti, S. (2011).  Numerical convergence of the block-maxima approach to the generalized extreme value distribution . Journal of statistical physics  145 , 1156–1180. Grassberger, P. (1988).  Finite sample corrections to entropy and dimension estimates .  Physics Letters A  128 , 369–373 . Grassberger, P. (2007).  Grassberger-Procaccia algorithm .  Scholarpedia  2 , 3043 . Grassberger, P. and Procaccia, I. (1983).  Characterization of Strange Attractors .  Physical Review Letters  50 , 346–349 . Higuchi, T. (1988).  Approach to an irregular time series on the basis of the fractal theory .  Physica D: Nonlinear Phenomena  31 , 277–283 . Kantz, H. and Schreiber, T. (2003).  Nonlinear Time Series Analysis .  Cambridge University Press . Kaplan, J. L. and Yorke, J. A. (1979).  Chaotic behavior of multidimensional difference equations .  In: Functional Differential Equations and Approximation of Fixed Points, editors, 204–227. Springer Berlin Heidelberg . Lucarini, V.; Faranda, D.; Moreira de Freitas, A. C.; de Freitas, J. M.; Holland, M.; Kuna, T.; Nicol, M.; Todd, M. and Vaienti, S. (2016).  Extremes and recurrence in dynamical systems . John Wiley \\& Sons, Nashville, TN. Molteno, T. C. (1993).  Fast$\\less$i$\\greater$O$\\less$/i$\\greater$($\\less$i$\\greater$N$\\less$/i$\\greater$) box-counting algorithm for estimating dimensions .  Physical Review E  48 , R3263–R3266 . Russell, D. A.; Hanson, J. D. and Ott, E. (1980).  Dimension of Strange Attractors .  Physical Review Letters  45 , 1175–1178 . Süveges, M. (2007).  Likelihood estimation of the extremal index .  Extremes  10 , 41–55 . Theiler, J. (1986).  Spurious dimension from correlation algorithms applied to limited time-series data .  Physical Review A  34 , 2427–2432 . Theiler, J. (1987).  Efficient algorithm for estimating the correlation dimension from a set of discrete points .  Physical Review A  36 , 4456–4462 . Takens1985 Takens, On the numerical determination of the dimension of an attractor, in: B.H.W. Braaksma, B.L.J.F. Takens (Eds.), Dynamical Systems and Bifurcations, in: Lecture Notes in Mathematics, Springer, Berlin, 1985, pp. 99–106. Theiler1988 Theiler,  Lacunarity in a best estimator of fractal dimension. Physics Letters A, 133(4–5) Borovkova1999 Borovkova et al.,  Consistency of the Takens estimator for the correlation dimension. The Annals of Applied Probability, 9, 05 1999. Barlow Barlow, R., Statistics - A Guide to the Use of Statistical Methods in the Physical Sciences. Vol 29. John Wiley & Sons, 1993 Faranda2017 Faranda et al. (2017), Dynamical proxies of North Atlantic predictability and extremes,  Scientific Reports, 7"},{"id":547,"pagetitle":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#ComplexityMeasures.jl","content":" ComplexityMeasures.jl"},{"id":548,"pagetitle":"ComplexityMeasures.jl","title":"ComplexityMeasures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#ComplexityMeasures","content":" ComplexityMeasures  —  Module ComplexityMeasures.jl ComplexityMeasures.jl is a software for calculating 1000s of various kinds of probabilities, entropies, and other so-called  complexity measures  from a single input dataset. For relational measures across many input datasets see its extension  CausalityTools.jl . The key features that it provides can be summarized as: A rigorous framework for extracting probabilities from data, based on the mathematical formulation of  probability spaces . Several (12+) outcome spaces, i.e., ways to discretize data into probabilities. Several estimators for estimating probabilities given an outcome space, which correct theoretically known estimation biases. Several definitions of information measures, such as various flavours of entropies (Shannon, Tsallis, Curado...), extropies, and other complexity measures, that are used in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems. Several discrete and continuous (differential) estimators for entropies, which correct theoretically known estimation biases. An extendable interface and well thought out API accompanied by dedicated developer documentation. This makes it trivial to define new outcome spaces, or new estimators for probabilities, information measures, or complexity measures and integrate them with everything else in the software without boilerplate code. ComplexityMeasures.jl can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . To install it, run  import Pkg; Pkg.add(\"ComplexityMeasures\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was called Entropies.jl. source"},{"id":549,"pagetitle":"ComplexityMeasures.jl","title":"Latest news","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#Latest-news","content":" Latest news ComplexityMeasures.jl has been updated to v3! The software has been massively improved and its core principles were redesigned to be extendable, accessible, and more closely based on the rigorous mathematics of probabilities and entropies. For more details of this new release, please see our  announcement post on discourse  or the central  Tutorial  of the v3 documentation. In this v3 many concepts were renamed, but there is no formally breaking change. Everything that changed has been deprecated and is backwards compatible. You can see the  CHANGELOG.md  for more details!"},{"id":550,"pagetitle":"ComplexityMeasures.jl","title":"Documentation contents","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#Documentation-contents","content":" Documentation contents Before anything else, we recommend users to go through our overarching  Tutorial , which teaches not only central API functions, but also terminology and crucial core concepts: Probabilities  lists all outcome spaces and probabilities estimators. Information measures  lists all implemented information measure definitions and estimators (both discrete and differential). Complexity measures  lists all implemented complexity measures that are not functionals of probabilities (unlike information measures). The  Examples  page lists dozens of runnable example code snippets along with their outputs."},{"id":551,"pagetitle":"ComplexityMeasures.jl","title":"Input data for ComplexityMeasures.jl","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#input_data","content":" Input data for ComplexityMeasures.jl The input data type typically depend on the outcome space chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data: Timeseries , which are  AbstractVector{<:Real} , used in e.g. with  WaveletOverlap . Multi-variate timeseries, or datasets, or state space sets , which are  StateSpaceSet s, used e.g. with  NaiveKernel . The short syntax  SSSet  may be used instead of  StateSpaceSet . Spatial data , which are higher dimensional standard  Array s, used e.g. with   SpatialOrdinalPatterns ."},{"id":552,"pagetitle":"ComplexityMeasures.jl","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. source"},{"id":553,"pagetitle":"ComplexityMeasures.jl","title":"Total entropy/information/complexity measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/#Total-entropy/information/complexity-measures","content":" Total entropy/information/complexity measures ComplexityMeasures.jl offers thousands of measures computable right out of the box. To see an exact number of how many, see this  calculation page ."},{"id":556,"pagetitle":"Complexity measures","title":"Complexity measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#complexity_measures","content":" Complexity measures Note Be sure you have gone through the  Tutorial  before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl."},{"id":557,"pagetitle":"Complexity measures","title":"Complexity measures API","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Complexity-measures-API","content":" Complexity measures API The complexity measure API is defined by the  complexity  function, which may take as an input an  ComplexityEstimator . The function  complexity_normalized  is also useful."},{"id":558,"pagetitle":"Complexity measures","title":"ComplexityMeasures.complexity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.complexity","content":" ComplexityMeasures.complexity  —  Function complexity(c::ComplexityEstimator, x) → m::Real Estimate a complexity measure according to  c  for  input data x , where  c  is an instance of any subtype of  ComplexityEstimator : ApproximateEntropy . LempelZiv76 . MissingDispersionPatterns . ReverseDispersion . SampleEntropy . BubbleEntropy . StatisticalComplexity . source"},{"id":559,"pagetitle":"Complexity measures","title":"ComplexityMeasures.complexity_normalized","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.complexity_normalized","content":" ComplexityMeasures.complexity_normalized  —  Function complexity_normalized(c::ComplexityEstimator, x) → m::Real ∈ [a, b] The same as  complexity , but the result is normalized to the interval  [a, b] , where  [a, b]  depends on  c . source"},{"id":560,"pagetitle":"Complexity measures","title":"ComplexityMeasures.ComplexityEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.ComplexityEstimator","content":" ComplexityMeasures.ComplexityEstimator  —  Type ComplexityEstimator Supertype for estimators for various complexity measures that are not entropies in the strict mathematical sense. See  complexity  for all available estimators. source"},{"id":561,"pagetitle":"Complexity measures","title":"Approximate entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Approximate-entropy","content":" Approximate entropy"},{"id":562,"pagetitle":"Complexity measures","title":"ComplexityMeasures.ApproximateEntropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.ApproximateEntropy","content":" ComplexityMeasures.ApproximateEntropy  —  Type ApproximateEntropy <: ComplexityEstimator\nApproximateEntropy([x]; r = 0.2std(x), kwargs...) An estimator for the approximate entropy ( Pincus, 1991 ) complexity measure, used with  complexity . The keyword argument  r  is mandatory if an input timeseries  x  is not provided. Keyword arguments r::Real : The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data. m::Int = 2 : The embedding dimension. τ::Int = 1 : The embedding lag. base::Real = MathConstants.e : The base to use for the logarithm. Pincus (1991) uses the   natural logarithm. Description Approximate entropy (ApEn) is defined as \\[ApEn(m ,r) = \\lim_{N \\to \\infty} \\left[ \\phi(x, m, r) - \\phi(x, m + 1, r) \\right].\\] Approximate entropy is estimated for a timeseries  x , by first embedding  x  using embedding dimension  m  and embedding lag  τ , then searching for similar vectors within tolerance radius  r , using the estimator described below, with logarithms to the given  base  (natural logarithm is used in Pincus, 1991). Specifically, for a finite-length timeseries  x , an estimator for  $ApEn(m ,r)$  is \\[ApEn(m, r, N) = \\phi(x, m, r, N) -  \\phi(x, m + 1, r, N),\\] where  N = length(x)  and \\[\\phi(x, k, r, N) =\n\\dfrac{1}{N-(k-1)\\tau} \\sum_{i=1}^{N - (k-1)\\tau}\n\\log{\\left(\n    \\sum_{j = 1}^{N-(k-1)\\tau} \\dfrac{\\theta(d({\\bf x}_i^m, {\\bf x}_j^m) \\leq r)}{N-(k-1)\\tau}\n    \\right)}.\\] Here,  $\\theta(\\cdot)$  returns 1 if the argument is true and 0 otherwise,   $d({\\bf x}_i, {\\bf x}_j)$  returns the Chebyshev distance between vectors   ${\\bf x}_i$  and  ${\\bf x}_j$ , and the  k -dimensional embedding vectors are constructed from the input timeseries  $x(t)$  as \\[{\\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \\ldots, x(i+(k-1)\\tau)).\\] Flexible embedding lag In the original paper, they fix  τ = 1 . In our implementation, the normalization constant is modified to account for embeddings with  τ != 1 . source"},{"id":563,"pagetitle":"Complexity measures","title":"Sample entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Sample-entropy","content":" Sample entropy"},{"id":564,"pagetitle":"Complexity measures","title":"ComplexityMeasures.SampleEntropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.SampleEntropy","content":" ComplexityMeasures.SampleEntropy  —  Type SampleEntropy([x]; r = 0.2std(x), kwargs...) <: ComplexityEstimator An estimator for the sample entropy complexity measure ( Richman and Moorman, 2000 ), used with  complexity  and  complexity_normalized . The keyword argument  r  is mandatory if an input timeseries  x  is not provided. Keyword arguments r::Real : The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data. m::Int = 2 : The embedding dimension. τ::Int = 1 : The embedding lag. Description An  estimator  for sample entropy using radius  r , embedding dimension  m , and embedding lag  τ  is \\[SampEn(m,r, N) = -\\ln{\\dfrac{A(r, N)}{B(r, N)}}.\\] Here, \\[\\begin{aligned}\nB(r, m, N) = \\sum_{i = 1}^{N-m\\tau} \\sum_{j = 1, j \\neq i}^{N-m\\tau} \\theta(d({\\bf x}_i^m, {\\bf x}_j^m) \\leq r) \\\\\nA(r, m, N) = \\sum_{i = 1}^{N-m\\tau} \\sum_{j = 1, j \\neq i}^{N-m\\tau} \\theta(d({\\bf x}_i^{m+1}, {\\bf x}_j^{m+1}) \\leq r) \\\\\n\\end{aligned},\\] where  $\\theta(\\cdot)$  returns 1 if the argument is true and 0 otherwise, and  $d(x, y)$  computes the Chebyshev distance between  $x$  and  $y$ , and   ${\\bf x}_i^{m}$  and  ${\\bf x}_i^{m+1}$  are  m -dimensional and  m+1 -dimensional embedding vectors, where  k -dimensional embedding vectors are constructed from the input timeseries  $x(t)$  as \\[{\\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \\ldots, x(i+(k-1)\\tau)).\\] Quoting Richman & Moorman (2002): \"SampEn(m,r,N) will be defined except when B = 0, in which case no regularity has been detected, or when A = 0, which corresponds to a conditional probability of 0 and an infinite value of SampEn(m,r,N)\". In these cases,  NaN  is returned. If computing the normalized measure, then the resulting sample entropy is on  [0, 1] . Flexible embedding lag The original algorithm fixes  τ = 1 . All formulas here are modified to account for any  τ . See also:  entropy_sample . source"},{"id":565,"pagetitle":"Complexity measures","title":"Missing dispersion patterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Missing-dispersion-patterns","content":" Missing dispersion patterns"},{"id":566,"pagetitle":"Complexity measures","title":"ComplexityMeasures.MissingDispersionPatterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.MissingDispersionPatterns","content":" ComplexityMeasures.MissingDispersionPatterns  —  Type MissingDispersionPatterns <: ComplexityEstimator\nMissingDispersionPatterns(o = Dispersion()) → mdp An estimator for the number of missing dispersion patterns (MDP), a complexity measure which can be used to detect nonlinearity in time series ( Zhou  et al. , 2023 ). Used with  complexity  or  complexity_normalized . Description When used with  complexity ,  complexity(mdp)  is syntactically equivalent with just  missing_outcomes (o) . When used with  complexity_normalized , the normalization is simply  missing_outcomes(o)/total_outcomes(o) . Encoding Dispersion 's linear mapping from CDFs to integers is based on equidistant partitioning of the interval  [0, 1] . This is slightly different from  Zhou  et al.  (2023) , which uses the linear mapping  $s_i := \\text{round}(y + 0.5)$ . Usage In  Zhou  et al.  (2023) ,  MissingDispersionPatterns  is used to detect nonlinearity in time series by comparing the MDP for a time series  x  to values for an ensemble of surrogates of  x , as per the standard analysis of  TimeseriesSurrogates.jl  If the MDP value of  $x$  is significantly larger than some high quantile of the surrogate distribution, then it is taken as evidence for nonlinearity. See also:  Dispersion ,  ReverseDispersion ,  total_outcomes . source"},{"id":567,"pagetitle":"Complexity measures","title":"Reverse dispersion entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Reverse-dispersion-entropy","content":" Reverse dispersion entropy"},{"id":568,"pagetitle":"Complexity measures","title":"ComplexityMeasures.ReverseDispersion","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.ReverseDispersion","content":" ComplexityMeasures.ReverseDispersion  —  Type ReverseDispersion <: ComplexityEstimator\nReverseDispersion(; c = 3, m = 2, τ = 1, check_unique = true) Estimator for the reverse dispersion entropy complexity measure ( Li  et al. , 2019 ). Description Li  et al.  (2019)  defines the reverse dispersion entropy as \\[H_{rde} = \\sum_{i = 1}^{c^m} \\left(p_i - \\dfrac{1}{{c^m}} \\right)^2 =\n\\left( \\sum_{i=1}^{c^m} p_i^2 \\right) - \\dfrac{1}{c^{m}}\\] where the probabilities  $p_i$  are obtained precisely as for the  Dispersion  probability estimator. Relative frequencies of dispersion patterns are computed using the given  encoding  scheme , which defaults to encoding using the normal cumulative distribution function (NCDF), as implemented by  GaussianCDFEncoding , using embedding dimension  m  and embedding delay  τ . Recommended parameter values( Li  et al. , 2019 ) are  m ∈ [2, 3] ,  τ = 1  for the embedding, and  c ∈ [3, 4, …, 8]  categories for the Gaussian mapping. If normalizing, then the reverse dispersion entropy is normalized to  [0, 1] . The minimum value of  $H_{rde}$  is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all  $p_i$ s are equal to  $1/c^m$ . Because  $H_{rde} \\geq 0$ ,  $H_{rde}$  can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise. Data requirements The input must have more than one unique element for the default  GaussianCDFEncoding  to be well-defined.  Li  et al.  (2019)  recommends that  x  has at least 1000 data points. If  check_unique == true  (default), then it is checked that the input has more than one unique value. If  check_unique == false  and the input only has one unique element, then a  InexactError  is thrown when trying to compute probabilities. source"},{"id":569,"pagetitle":"Complexity measures","title":"Statistical complexity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Statistical-complexity","content":" Statistical complexity"},{"id":570,"pagetitle":"Complexity measures","title":"ComplexityMeasures.StatisticalComplexity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.StatisticalComplexity","content":" ComplexityMeasures.StatisticalComplexity  —  Type StatisticalComplexity <: ComplexityEstimator\nStatisticalComplexity(; kwargs...) An estimator for the statistical complexity and entropy, originally by ( Rosso  et al. , 2007 ) and generalized by  Rosso  et al.  (2013) . Our implementation extends the generalization to any valid distance metric, any  OutcomeSpace  with a priori known  total_outcomes , any  ProbabilitiesEstimator , and any normalizable discrete  InformationMeasure . Used with  complexity . Keyword arguments o::OutcomeSpace = OrdinalPatterns{3}() . The  OutcomeSpace , which controls how   the input data are discretized. pest::ProbabilitiesEstimator = RelativeAmount() : The    ProbabilitiesEstimator  used to estimate probabilities over the discretized   input data. hest = Renyi() : A  DiscreteInfoEstimator  or an  InformationMeasure .   Any information   measure that defines  information_maximum  is valid here including extropies.   The measure will be estimated using the  PlugIn  estimator if not given an   estimator. dist <: SemiMetric = JSDivergence() : The distance measure (from Distances.jl) to use for   estimating the distance between the estimated probability distribution and a uniform   distribution with the same maximal number of outcomes. Description Statistical complexity is defined as \\[C_q[P] = \\mathcal{H}_q\\cdot \\mathcal{Q}_q[P],\\] where  $Q_q$  is a \"disequilibrium\" obtained from a distance-measure and  $H_q$  a disorder measure. In the original paper( Rosso  et al. , 2007 ), this complexity measure was defined via an ordinal pattern-based probability distribution (see  OrdinalPatterns ), using  Shannon  entropy as the information measure, and the Jensen-Shannon divergence as a distance measure. Our implementation is a further generalization of the complexity measure developed in  Rosso  et al.  (2013) . We let  $H_q$ be any normalizable  InformationMeasure , e.g.  Shannon ,  Renyi  or  Tsallis  entropy, and we let   $Q_q$  be either on the Euclidean, Wooters, Kullback, q-Kullback, Jensen or q-Jensen  distance as \\[Q_q[P] = Q_q^0\\cdot D[P, P_e],\\] where  $D[P, P_e]$  is the distance between the obtained distribution  $P$  and a uniform distribution with the same maximum number of bins, measured by the distance measure  dist . Usage The statistical complexity is exclusively used in combination with the chosen information measure (typically an entropy). The estimated value of the information measure can be accessed as a  Ref  value of the struct as x = randn(100)\nc = StatisticalComplexity()\ncompl = complexity(c, x)\nentr = first(entropy_complexity(c, x)) # both complexity and entropy value complexity(c::StatisticalComplexity, x)  returns only the statistical complexity. To obtain both the value of the entropy (or other information measure) and the statistical complexity together as a  Tuple , use the wrapper  entropy_complexity . See also:  entropy_complexity_curves . source"},{"id":571,"pagetitle":"Complexity measures","title":"ComplexityMeasures.entropy_complexity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.entropy_complexity","content":" ComplexityMeasures.entropy_complexity  —  Function entropy_complexity(c::StatisticalComplexity, x) → (h, compl) Return a information measure  h  and the corresponding  StatisticalComplexity  value  compl . Useful when wanting to plot data on the \"entropy-complexity plane\". See also  entropy_complexity_curves . source"},{"id":572,"pagetitle":"Complexity measures","title":"ComplexityMeasures.entropy_complexity_curves","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.entropy_complexity_curves","content":" ComplexityMeasures.entropy_complexity_curves  —  Function entropy_complexity_curves(c::StatisticalComplexity;\n    num_max=1, num_min=1000) -> (min_entropy_complexity, max_entropy_complexity) Calculate the maximum complexity-entropy curve for the statistical complexity according to  Rosso  et al.  (2007)  for  num_max * total_outcomes(c.o)  different values of the normalized information measure of choice (in case of the maximum complexity curves) and  num_min  different values of the normalized information measure of choice (in case of the minimum complexity curve). This function can also be used to compute the maximum \"complexity-extropy curve\" if  c.hest  is e.g.  ShannonExtropy , which is the equivalent of the complexity-entropy curves, but using extropy instead of entropy. Description The way the statistical complexity is designed, there is a minimum and maximum possible complexity for data with a given value of an information measure. The calculation time of the maximum complexity curve grows as  O(total_outcomes(c.o)^2) , and thus takes very long for high numbers of outcomes. This function is inspired by S. Sippels implementation in statcomp ( Sippel  et al. , 2016 ). This function will work with any  ProbabilitiesEstimator  where  total_outcomes  is known a priori. source"},{"id":573,"pagetitle":"Complexity measures","title":"Lempel-Ziv complexity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Lempel-Ziv-complexity","content":" Lempel-Ziv complexity"},{"id":574,"pagetitle":"Complexity measures","title":"ComplexityMeasures.LempelZiv76","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.LempelZiv76","content":" ComplexityMeasures.LempelZiv76  —  Type LempelZiv76 <: ComplexityEstimator\nLempelZiv76() The Lempel-Ziv, or  LempelZiv76 , complexity measure ( Lempel and Ziv, 1976 ), which is used with  complexity  and  complexity_normalized . For results to be comparable across sequences with different length, use the normalized version. Normalized  LempelZiv76 -complexity is implemented as given in  Amigó  et al.  (2004) . The normalized measure is close to zero for very regular signals, while for random sequences, it is close to 1 with high probability [Amigó2004] . Note: the normalized  LempelZiv76  complexity can be higher than 1 [Amigó2004] . The  LempelZiv76  measure applies only to binary sequences, i.e. sequences with a two-element alphabet (precisely two distinct outcomes). For performance optimization, we do not check the number of unique elements in the input. If your input sequence is not binary, you must  encode  it first using one of the implemented  Encoding  schemes (or encode your data manually). source"},{"id":575,"pagetitle":"Complexity measures","title":"Bubble entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#Bubble-entropy","content":" Bubble entropy"},{"id":576,"pagetitle":"Complexity measures","title":"ComplexityMeasures.BubbleEntropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/complexity/#ComplexityMeasures.BubbleEntropy","content":" ComplexityMeasures.BubbleEntropy  —  Type BubbleEntropy <: ComplexityEstimator\nBubbleEntropy(; m = 3, τ = 1, definition = Renyi(q = 2)) The  BubbleEntropy  complexity estimator ( Manis  et al. , 2017 ) is just a difference between two entropies, each computed with the  BubbleSortSwaps  outcome space, for embedding dimensions  m + 1  and  m , respectively.  Manis  et al.  (2017)  use the  Renyi  entropy of order  q = 2  as the  information measure  definition , but here you can use any  InformationMeasure .  Manis  et al.  (2017)  formulates the \"bubble entropy\" as the normalized measure below,  while here you can also compute the unnormalized measure. Definition For input data  x , the \"bubble entropy\" is computed by first embedding the input data using embedding dimension  m  and embedding delay  τ  (call the embedded pts  y ), and  then computing the difference between the two entropies: \\[BubbleEn_T(τ) = H_T(y, m + 1) - H_T(y, m)\\] where  $H_T(y, m)$  and  $H_T(y, m + 1)$  are entropies of type  $T$  (e.g.  Renyi ) computed with the input data  x  embedded to dimension  $m$  and   $m+1$ , respectively. Use  complexity  to compute this non-normalized version.  Use  complexity_normalized  to compute the normalized difference of entropies: \\[BubbleEn_H(τ)^{norm} = \n\\dfrac{H_T(x, m + 1) - H_T(x, m)}{max(H_T(x, m + 1)) - max(H_T(x, m))},\\] where the maximum of the entropies for dimensions  m  and  m + 1  are computed using  information_maximum . Example using ComplexityMeasures\nx = rand(1000)\nest = BubbleEntropy(m = 5, τ = 3)\ncomplexity(est, x) source"},{"id":579,"pagetitle":"Convenience functions","title":"Convenience functions","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#convenience","content":" Convenience functions"},{"id":580,"pagetitle":"Convenience functions","title":"Common entropy-based measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#Common-entropy-based-measures","content":" Common entropy-based measures We provide a few convenience functions for widely used names for entropy or \"entropy-like\" quantities. Other arbitrary specialized convenience functions can easily be defined in a couple lines of code. We emphasize that these functions really aren't anything more than 2-lines-of-code wrappers that call  information  with the appropriate  OutcomeSpace  and  InformationMeasure ."},{"id":581,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_permutation","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_permutation","content":" ComplexityMeasures.entropy_permutation  —  Function entropy_permutation(x; τ = 1, m = 3, base = 2) Compute the permutation entropy of  x  of order  m  with delay/lag  τ . This function is just a convenience call to: est = OrdinalPatterns(; m, τ)\ninformation(Shannon(base), est, x) See  OrdinalPatterns  for more info. Similarly, one can use  WeightedOrdinalPatterns  or  AmplitudeAwareOrdinalPatterns  for the weighted/amplitude-aware versions. source"},{"id":582,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_wavelet","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_wavelet","content":" ComplexityMeasures.entropy_wavelet  —  Function entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2) Compute the wavelet entropy. This function is just a convenience call to: est = WaveletOverlap(wavelet)\ninformation(Shannon(base), est, x) See  WaveletOverlap  for more info. source"},{"id":583,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_dispersion","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_dispersion","content":" ComplexityMeasures.entropy_dispersion  —  Function entropy_dispersion(x; base = 2, kwargs...) Compute the dispersion entropy. This function is just a convenience call to: est = Dispersion(kwargs...)\ninformation(Shannon(base), est, x) See  Dispersion  for more info. source"},{"id":584,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_approx","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_approx","content":" ComplexityMeasures.entropy_approx  —  Function entropy_approx(x; m = 2, τ = 1, r = 0.2 * Statistics.std(x), base = MathConstants.e) Convenience syntax for computing the approximate entropy (Pincus, 1991) for timeseries  x . This is just a wrapper for  complexity(ApproximateEntropy(; m, τ, r, base), x)  (see also  ApproximateEntropy ). source"},{"id":585,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_sample","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_sample","content":" ComplexityMeasures.entropy_sample  —  Function entropy_sample(x; r = 0.2std(x), m = 2, τ = 1, normalize = true) Convenience syntax for estimating the (normalized) sample entropy (Richman & Moorman, 2000) of timeseries  x . This is just a wrapper for  complexity(SampleEntropy(; r, m, τ, base), x) . See also:  SampleEntropy ,  complexity ,  complexity_normalized ). source"},{"id":586,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_distribution","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_distribution","content":" ComplexityMeasures.entropy_distribution  —  Function entropy_distribution(x; τ = 1, m = 3, n = 3, base = 2) Compute the distribution entropy ( Li  et al. , 2015 ) of  x  using embedding dimension  m  with delay/lag  τ , using the Chebyshev distance metric, and using an  n -element equally-spaced binning over the distribution of distances to estimate probabilities. This function is just a convenience call to: x = rand(1000000)\no = SequentialPairDistances(x, n, m, τ, metric = Chebyshev())\nh = information(Shannon(base = 2), o, x) See  SequentialPairDistances  for more info. source"},{"id":589,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#ComplexityMeasures.jl-Dev-Docs","content":" ComplexityMeasures.jl Dev Docs Good practices in developing a code base apply in every Pull Request. The  Good Scientific Code Workshop  is worth checking out for this. All PRs contributing new functionality must be well tested and well documented. You only need to add tests for methods that you  explicitly  extended."},{"id":590,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  OutcomeSpace","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Adding-a-new-OutcomeSpace","content":" Adding a new  OutcomeSpace"},{"id":591,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Mandatory-steps","content":" Mandatory steps Decide on the outcome space and how the estimator will map probabilities to outcomes. Define your type and make it subtype  OutcomeSpace . Add a docstring to your type following the style of the docstrings of other estimators. If suitable, the estimator may be able to operate based on  Encoding s. If so,  it is preferred to implement an  Encoding  subtype and extend the methods   encode  and  decode . This will allow your outcome space to be used  with a larger span of entropy and complexity methods without additional effort.  Have a look at the file defining  OrdinalPatterns  for an idea of how this  works. If your new outcome space is counting-based, then Implement dispatch for  counts_and_outcomes  for your  OutcomeSpace   type. If the outcomes do not come for free, then instead you can extend   counts  and then explicitly add another method for   counts_and_outcomes  that calls  counts  first and then decodes  the outcomes. Follow existing implementations for guidelines (see for example source  code for  Dispersion ). Implement dispatch for  codify . This will ensure that the outcome space  also works automatically with any discrete estimators in the downstream CausalityTools.jl. If your new outcome space is not counting-based, then Implement dispatch for  probabilities_and_outcomes  for your   OutcomeSpace  type. If the outcomes do not come for free, then instead you  can extend  probabilities  and then explicitly add another method for   probabilities_and_outcomes  that calls  probabilities  first and  then decodes the outcomes.  Follow existing implementations for guidelines (see for example source code for   NaiveKernel ). Finally, Implement dispatch for  outcome_space  and your  OutcomeSpace  type.  The return value of  outcome_space  must be sorted (as in the default behavior of   sort , in ascending order). Add your outcome space type to the table list in the documentation string of   OutcomeSpace . If you made an encoding, also add it to corresponding table  in the encodings section."},{"id":592,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Optional-steps","content":" Optional steps The following methods may be extended for your  OutcomeSpace  if doing so leads to performance benefits. total_outcomes . By default it returns the  length  of  outcome_space .  This is the function that most typically has performance benefits if implemented  explicitly, so most existing estimators extend it by default."},{"id":593,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  ProbabilitiesEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Adding-a-new-[ProbabilitiesEstimator](@ref)","content":" Adding a new  ProbabilitiesEstimator"},{"id":594,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Mandatory-steps-2","content":" Mandatory steps Define your type and make it subtype  ProbabilitiesEstimator . Add a docstring to your type following the style of the docstrings of other   ProbabilitiesEstimator s. Implement dispatch for  probabilities  for your   ProbabilitiesEstimator  type. You'll then get   probabilities_and_outcomes  for free. Implement dispatch for  allprobabilities_and_outcomes  for your   ProbabilitiesEstimator  type.  Add your new  ProbabilitiesEstimator  type to the list of probabilities  estimators in the probabilities estimators documentation section."},{"id":595,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  InformationMeasureEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Adding-a-new-InformationMeasureEstimator","content":" Adding a new  InformationMeasureEstimator The type implementation should follow the declared API of  InformationMeasureEstimator . If the type is a discrete measure, then extend  information(e::YourType, p::Probabilities) . If it is a differential measure, then extend  information(e::YourType, x::InputData) ."},{"id":596,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.InformationMeasureEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#ComplexityMeasures.InformationMeasureEstimator","content":" ComplexityMeasures.InformationMeasureEstimator  —  Type InformationMeasureEstimator{I <: InformationMeasure} The supertype of all information measure estimators. Its direct subtypes are  DiscreteInfoEstimator  and  DifferentialInfoEstimator . Since all estimators must reference a measure definition in some way, we made the following interface decisions: all estimators have as first type parameter  I <: InformationMeasure all estimators reference the information measure in a  definition  field all estimators are defined using  Base.@kwdef  so that they may be initialized with the syntax  Estimator(; definition = Shannon())  (or any other). Any concrete subtypes must follow the above, e.g.: Base.@kwdef struct MyEstimator{I <: InformationMeasure, X} <: DiscreteInfoEstimator{I}\n    definition::I\n    x::X\nend Why separate the *definition* of a measure from *estimators* of a measure? In real applications, we generally don't have access to the underlying probability mass functions or densities required to compute the various entropy or extropy definitons. Therefore, these information measures must be  estimated  from finite data. Estimating a particular measure (e.g.  Shannon  entropy) can be done in many ways, each with its own own pros and cons. We aim to provide a complete library of literature estimators of the various information measures (PRs are welcome!). source"},{"id":597,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  InformationMeasure","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Adding-a-new-InformationMeasure","content":" Adding a new  InformationMeasure This amounts to adding a new definition of an information measure, not an estimator. It de-facto means adding a method for the discrete Plug-In estimator."},{"id":598,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Mandatory-steps-3","content":" Mandatory steps Define your information measure definition type and make it subtype  InformationMeasure . Implement dispatch for  information (def::YourType, p::Probabilities) . This is the Plug-In estimator for the discrete measure. Add a docstring to your type following the style of the docstrings of other information  measure definitions, and should include the mathematical definition of the measure. Add your information measure definition type to the list of definitions in the   docs/src/information_measures.md  documentation page. Add a reference to your information measure definition in the docstring for   InformationMeasure ."},{"id":599,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Optional-steps-2","content":" Optional steps If the maximum value of your information measure type is analytically computable for a  probability distribution with a known number of elements, implementing dispatch for   information_maximum  automatically enables  information_normalized   for your type."},{"id":600,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  MultiScaleAlgorithm","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Adding-a-new-MultiScaleAlgorithm","content":" Adding a new  MultiScaleAlgorithm A new  MultiScaleAlgorithm  is simply a new way of coarse-graining input time series  across multiple scales."},{"id":601,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/devdocs/#Mandatory-steps-4","content":" Mandatory steps Define a new type  YourNewMultiScaleType <: MultiScaleAlgorithm . This type will  define how coarse graining is performed. Implement dispatch for  downsample , which transforms the original time series  into a vector of coarse-grained time series, one per scale (may be nested if needed). Implement dispatch for the internal  apply_multiscale  function. Add an entry for your new type in the  multiscale.md  file. Add tests for your new type. You specifically need to implement analytical tests  that verify that  downsample  is correctly implemented. For API tests,  simply copy the tests from e.g.  tests/multiscale/Composite.jl , and replace the  multiscale coarse-graining algorithm with an instance of your algorithm. Hooray! You're new coarse-graining procedure is integrated with the entire  ComplexityMeasures.jl ecosystem!"},{"id":604,"pagetitle":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#examples","content":" ComplexityMeasures.jl Examples"},{"id":605,"pagetitle":"ComplexityMeasures.jl Examples","title":"Probabilities: kernel density","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Probabilities:-kernel-density","content":" Probabilities: kernel density Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point  p , measured by how many points are within radius  1.5  of  p . Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot. using ComplexityMeasures\nusing CairoMakie\nusing Distributions: MvNormal\nusing LinearAlgebra\n\nμ = [1.0, -4.0]\nσ = [2.0, 2.0]\n𝒩 = MvNormal(μ, LinearAlgebra.Diagonal(map(abs2, σ)))\nN = 500\nD = StateSpaceSet(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(NaiveKernel(1.5), D)\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig"},{"id":606,"pagetitle":"ComplexityMeasures.jl Examples","title":"Probabilities: KL-divergence of histograms","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Probabilities:-KL-divergence-of-histograms","content":" Probabilities: KL-divergence of histograms In this example we show how simple it is to compute the  KL-divergence  (or any other distance function for probability distributions) using ComplexityMeasures.jl. For simplicity, we will compute the KL-divergence between the  ValueBinning s of two timeseries. Note that it is  crucial  to use  allprobabilities_and_outcomes  instead of  probabilities_and_outcomes . using ComplexityMeasures\n\nN = 1000\nt = range(0, 20π; length=N)\nx = @. clamp(sin(t), -0.5, 1)\ny = @. sin(t + cos(2t))\n\nr = -1:0.1:1\nest = ValueBinning(FixedRectangularBinning(r))\npx, outsx = allprobabilities_and_outcomes(est, x)\npy, outsy = allprobabilities_and_outcomes(est, y)\n\n# Visualize\nusing CairoMakie\nbins = r[1:end-1] .+ step(r)/2\nfig, ax = barplot(bins, px; label = L\"p_x\")\nbarplot!(ax, bins, py; label = L\"p_y\")\naxislegend(ax; labelsize = 30)\nfig using StatsBase: kldivergence\n\nkldivergence(px, py) 0.7899097070515434 kldivergence(py, px) Inf ( Inf  because there are events with 0 probability in  px )"},{"id":607,"pagetitle":"ComplexityMeasures.jl Examples","title":"Differential entropy: estimator comparison","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Differential-entropy:-estimator-comparison","content":" Differential entropy: estimator comparison"},{"id":608,"pagetitle":"ComplexityMeasures.jl Examples","title":"Shannon entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Shannon-entropy","content":" Shannon entropy Here, we compare how the nearest neighbor differential entropy estimators ( Kraskov ,  KozachenkoLeonenko ,  Zhu ,  ZhuSingh , etc.) converge towards the true  Shannon  entropy value for increasing time series length. ComplexityMeasures.jl also provides entropy estimators based on  order statistics . These estimators are only defined for scalar-valued vectors, in this example, so we compute these estimates separately, and add these estimators ( Vasicek ,  Ebrahimi ,  AlizadehArghami  and  Correa ) to the comparison. Input data are from a normal 1D distribution  $\\mathcal{N}(0, 1)$ , for which the true entropy is  0.5*log(2π) + 0.5  nats when using natural logarithms. using ComplexityMeasures\nusing CairoMakie, Statistics\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ne = Shannon(; base = MathConstants.e)\n\n# --------------------------\n# kNN estimators\n# --------------------------\nw = 0 # Theiler window of 0 (only exclude the point itself during neighbor searches)\nent = Shannon(; base = ℯ)\nknn_estimators = [\n    # with k = 1, Kraskov is virtually identical to\n    # Kozachenko-Leonenko, so pick a higher number of neighbors for Kraskov\n    Kraskov(ent; k = 3, w),\n    KozachenkoLeonenko(ent; w),\n    Zhu(ent; k = 3, w),\n    ZhuSingh(ent; k = 3, w),\n    Gao(ent; k = 3, corrected = false, w),\n    Gao(ent; k = 3, corrected = true, w),\n    Goria(ent; k = 3, w),\n    Lord(ent; k = 20, w), # more neighbors for accurate ellipsoid estimation\n    LeonenkoProzantoSavani(ent; k = 3),\n]\n\n# Test each estimator `nreps` times over time series of varying length.\nHs_uniform_knn = [[zeros(nreps) for N in Ns] for e in knn_estimators]\nfor (i, est) in enumerate(knn_estimators)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) |> StateSpaceSet\n        for (k, N) in enumerate(Ns)\n            Hs_uniform_knn[i][k][j] = information(est, pts[1:N])\n        end\n    end\nend\n\n# --------------------------\n# Order statistic estimators\n# --------------------------\n\n# Just provide types here, they are instantiated inside the loop\nestimators_os = [Vasicek, Ebrahimi, AlizadehArghami, Correa]\nHs_uniform_os = [[zeros(nreps) for N in Ns] for e in estimators_os]\nfor (i, est_os) in enumerate(estimators_os)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) # raw timeseries, not a `StateSpaceSet`\n        for (k, N) in enumerate(Ns)\n            m = floor(Int, N / 100) # Scale `m` to timeseries length\n            est = est_os(ent; m) # Instantiate estimator with current `m`\n            Hs_uniform_os[i][k][j] = information(est, pts[1:N])\n        end\n    end\nend\n\n# -------------\n# Plot results\n# -------------\nfig = Figure(resolution = (700, 11 * 200))\nlabels_knn = [\"KozachenkoLeonenko\", \"Kraskov\", \"Zhu\", \"ZhuSingh\", \"Gao (not corrected)\",\n    \"Gao (corrected)\", \"Goria\", \"Lord\", \"LeonenkoProzantoSavani\"]\nlabels_os = [\"Vasicek\", \"Ebrahimi\", \"AlizadehArghami\", \"Correa\"]\n\nfor (i, e) in enumerate(knn_estimators)\n    Hs = Hs_uniform_knn[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_knn[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs); alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, linewidth = 5, linestyle = :dash)\n\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfor (i, e) in enumerate(estimators_os)\n    Hs = Hs_uniform_os[i]\n    ax = Axis(fig[i + length(knn_estimators),1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_os[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs), alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, linewidth = 5, linestyle = :dash)\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfig All estimators approach the true differential entropy, but those based on order statistics are negatively biased for small sample sizes."},{"id":609,"pagetitle":"ComplexityMeasures.jl Examples","title":"Rényi entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Rényi-entropy","content":" Rényi entropy Here, we see how the  LeonenkoProzantoSavani  estimator approaches the known target  Renyi  entropy of a multivariate normal distribution for increasing time series length. We'll consider the Rényi entropy with  q = 2 . using ComplexityMeasures\nimport ComplexityMeasures: information # we're overriding this function in the example\nusing CairoMakie, Statistics\nusing Distributions: MvNormal\nimport Distributions.entropy as dentropy\nusing Random\nrng = MersenneTwister(1234)\n\n\"\"\"\n    information(e::Renyi, 𝒩::MvNormal; base = 2)\n\nCompute the analytical value of the `Renyi` entropy for a multivariate normal distribution.\n\"\"\"\nfunction information(e::Renyi, 𝒩::MvNormal; base = 2)\n    q = e.q\n    if q ≈ 1.0\n        h = dentropy(𝒩)\n    else\n        Σ = 𝒩.Σ\n        D = length(𝒩.μ)\n        h = dentropy(𝒩) - (D / 2) * (1 + log(q) / (1 - q))\n    end\n    return convert_logunit(h, ℯ, base)\nend\n\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ndef = Renyi(q = 2, base = 2)\n\nμ = [-1, 1]\nσ = [1, 0.5]\n𝒩 = MvNormal(μ, LinearAlgebra.Diagonal(map(abs2, σ)))\nh_true = information(def, 𝒩; base = 2)\n\n# Estimate `nreps` times for each time series length\n\nhs = [zeros(nreps) for N in Ns]\nfor (i, N) in enumerate(Ns)\n    for j = 1:nreps\n        pts = StateSpaceSet(transpose(rand(rng, 𝒩, N)))\n        hs[i][j] = information(LeonenkoProzantoSavani(def; k = 5), pts)\n    end\nend\n\n# We plot the mean and standard deviation of the estimator again the true value\nhs_mean, hs_stdev = mean.(hs), std.(hs)\n\nfig = Figure()\nax = Axis(fig[1, 1]; ylabel = \"h (bits)\")\nlines!(ax, Ns, hs_mean; color = Cycled(1), label = \"LeonenkoProzantoSavani\")\nband!(ax, Ns, hs_mean .+ hs_stdev, hs_mean .- hs_stdev,\n    alpha = 0.5, color = (Main.COLORS[1], 0.5))\nhlines!(ax, [h_true], color = :black, linewidth = 5, linestyle = :dash)\naxislegend()\nfig"},{"id":610,"pagetitle":"ComplexityMeasures.jl Examples","title":"Tsallis entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Tsallis-entropy","content":" Tsallis entropy Here, we see how the  LeonenkoProzantoSavani  estimator approaches the known target  Tsallis  entropy of a multivariate normal distribution for increasing time series length. We'll consider the Rényi entropy with  q = 2 . using ComplexityMeasures\nimport ComplexityMeasures: information # we're overriding this function in the example\nusing CairoMakie, Statistics\nusing Distributions: MvNormal\nimport Distributions.entropy as dentropy\nusing Random\nrng = MersenneTwister(1234)\n\n\"\"\"\n    information(e::Tsallis, 𝒩::MvNormal; base = 2)\n\nCompute the analytical value of the `Tsallis` entropy for a multivariate normal distribution.\n\"\"\"\nfunction information(e::Tsallis, 𝒩::MvNormal; base = 2)\n    q = e.q\n    Σ = 𝒩.Σ\n    D = length(𝒩.μ)\n    # uses the function from the example above\n    hr = information(Renyi(q = q), 𝒩; base = ℯ) # stick with natural log, convert after\n    h = (exp((1 - q) * hr) - 1) / (1 - q)\n    return convert_logunit(h, ℯ, base)\nend\n\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ndef = Tsallis(q = 2, base = 2)\n\nμ = [-1, 1]\nσ = [1, 0.5]\n𝒩 = MvNormal(μ, LinearAlgebra.Diagonal(map(abs2, σ)))\nh_true = information(def, 𝒩; base = 2)\n\n# Estimate `nreps` times for each time series length\n\nhs = [zeros(nreps) for N in Ns]\nfor (i, N) in enumerate(Ns)\n    for j = 1:nreps\n        pts = StateSpaceSet(transpose(rand(rng, 𝒩, N)))\n        hs[i][j] = information(LeonenkoProzantoSavani(def; k = 5), pts)\n    end\nend\n\n# We plot the mean and standard deviation of the estimator again the true value\nhs_mean, hs_stdev = mean.(hs), std.(hs)\n\nfig = Figure()\nax = Axis(fig[1, 1]; ylabel = \"h (bits)\")\nlines!(ax, Ns, hs_mean; color = Cycled(1), label = \"LeonenkoProzantoSavani\")\nband!(ax, Ns, hs_mean .+ hs_stdev, hs_mean .- hs_stdev,\n    alpha = 0.5, color = (Main.COLORS[1], 0.5))\nhlines!(ax, [h_true], color = :black, linewidth = 5, linestyle = :dash)\naxislegend()\nfig"},{"id":611,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: permutation entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Discrete-entropy:-permutation-entropy","content":" Discrete entropy: permutation entropy This example plots permutation entropy for time series of the chaotic logistic map. Entropy estimates using  WeightedOrdinalPatterns  and  AmplitudeAwareOrdinalPatterns  are added here for comparison. The entropy behaviour can be parallelized with the  ChaosTools.lyapunov  of the map. using DynamicalSystemsBase, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nhs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n\n    x, t = trajectory(ds, N_ent)\n    ## `x` is a 1D dataset, need to recast into a timeseries\n    x = columns(x)[1]\n    hs_perm[i] = information(OrdinalPatterns(; m, τ), x)\n    hs_wtperm[i] = information(WeightedOrdinalPatterns(; m, τ), x)\n    hs_ampperm[i] = information(AmplitudeAwareOrdinalPatterns(; m, τ), x)\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"h_6 (SP)\")\nlines!(a1, rs, hs_perm; color = Cycled(2))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (WT)\")\nlines!(a2, rs, hs_wtperm; color = Cycled(3))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (SAAP)\", xlabel = L\"r\")\nlines!(a3, rs, hs_ampperm; color = Cycled(4))\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig"},{"id":612,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: wavelet entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Discrete-entropy:-wavelet-entropy","content":" Discrete entropy: wavelet entropy The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales). using CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig"},{"id":613,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropies: properties","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Discrete-entropies:-properties","content":" Discrete entropies: properties Here, we show the sensitivity of the various entropies to variations in their parameters."},{"id":614,"pagetitle":"ComplexityMeasures.jl Examples","title":"Curado entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Curado-entropy","content":" Curado entropy Here, we reproduce Figure 2 from  Curado and Nobre (2004) , showing how the  Curado  entropy changes as function of the parameter  a  for a range of two-element probability distributions given by  Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0) . using ComplexityMeasures, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[information(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig"},{"id":615,"pagetitle":"ComplexityMeasures.jl Examples","title":"Kaniadakis entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Kaniadakis-entropy","content":" Kaniadakis entropy Here, we show how  Kaniadakis  entropy changes as function of the parameter  a  for a range of two-element probability distributions given by  Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0) . using ComplexityMeasures\nusing CairoMakie\n\nprobs = [Probabilities([p, 1-p]) for p in 0.0:0.01:1.0]\nps = collect(0.0:0.01:1.0);\nκs = [-0.99, -0.66, -0.33, 0, 0.33, 0.66, 0.99];\nHs = [[information(Kaniadakis(κ = κ), p) for p in probs] for κ in κs];\n\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"p\", ylabel = \"H(p)\")\n\nfor (i, H) in enumerate(Hs)\n    lines!(ax, ps, H, label = \"$(κs[i])\")\nend\n\naxislegend()\n\nfig"},{"id":616,"pagetitle":"ComplexityMeasures.jl Examples","title":"Stretched exponential entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Stretched-exponential-entropy","content":" Stretched exponential entropy Here, we reproduce the example from  Anteneodo and Plastino (1999) , showing how the stretched exponential entropy changes as function of the parameter  η  for a range of two-element probability distributions given by  Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0) . using ComplexityMeasures, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[information(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig"},{"id":617,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: dispersion entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#dispersion_example","content":" Discrete entropy: dispersion entropy Here we compute dispersion entropy ( Rostaghi and Azami, 2016 ), using the use the  Dispersion  probabilities estimator, for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example is adapted from  Li  et al.  (2019) . using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(c = c, m = m, τ = 1)\nfor (i, window) in enumerate(windows)\n    des[i] = information_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = '●', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig"},{"id":618,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: normalized entropy for comparing different signals","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Discrete-entropy:-normalized-entropy-for-comparing-different-signals","content":" Discrete entropy: normalized entropy for comparing different signals When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the wavelet entropy example where we use the spectral entropy instead of the wavelet entropy: using ComplexityMeasures\nN1, N2, a = 101, 10001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n\n    for q in (x, y, z)\n        local h = information(PowerSpectrum(), q)\n        local n = information_normalized(PowerSpectrum(), q)\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend entropy: 0.3131510976800182, normalized: 0.05520585619046334.\nentropy: 1.2651873361559447, normalized: 0.22304169026158024.\nentropy: 3.020995583762845, normalized: 0.5325756447440302.\nentropy: 7.57800737722389e-5, normalized: 6.1669977445812415e-6.\nentropy: 0.8397257036159657, normalized: 0.06833704775520644.\nentropy: 5.26448681003326, normalized: 0.42842500234865644. You see that while the direct entropy values of noisy signal changes strongly with  N  but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant."},{"id":619,"pagetitle":"ComplexityMeasures.jl Examples","title":"Spatiotemporal permutation entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Spatiotemporal-permutation-entropy","content":" Spatiotemporal permutation entropy Usage of a  SpatialOrdinalPatterns  estimator is straightforward. Here we get the spatial permutation entropy of a 2D array (e.g., an image): using ComplexityMeasures\nx = rand(50, 50) # some image\nstencil = [1 1; 0 1] # or one of the other ways of specifying stencils\nest = SpatialOrdinalPatterns(stencil, x)\nh = information(est, x) 2.584179830215719 To apply this to timeseries of spatial data, simply loop over the call, e.g.: data = [rand(50, 50) for i in 1:10] # e.g., evolution of a 2D field of a PDE\nest = SpatialOrdinalPatterns(stencil, first(data))\nh_vs_t = map(d -> information(est, d), data) 10-element Vector{Float64}:\n 2.582577619176741\n 2.584009350779486\n 2.584421152032926\n 2.5839649464772583\n 2.58412951002139\n 2.584444216300012\n 2.584586532197249\n 2.584274097615506\n 2.5845314250249602\n 2.5841435009411655 Computing any other generalized spatiotemporal permutation entropy is trivial, e.g. with  Renyi : x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialOrdinalPatterns(stencil, x)\ninformation(Renyi(q = 2), est, x) 1.5563431096659388"},{"id":620,"pagetitle":"ComplexityMeasures.jl Examples","title":"Spatial discrete entropy: Fabio","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Spatial-discrete-entropy:-Fabio","content":" Spatial discrete entropy: Fabio Let's see how the normalized permutation and dispersion entropies increase for an image that gets progressively more noise added to it. using ComplexityMeasures\nusing Distributions: Uniform\nusing CairoMakie\nusing Statistics\nusing TestImages, ImageTransformations, CoordinateTransformations, Rotations\n\nimg = testimage(\"fabio_grey_256\")\nrot = warp(img, recenter(RotMatrix(-3pi/2), center(img));)\noriginal = Float32.(rot)\nnoise_levels = collect(0.0:0.25:1.0) .* std(original) * 5 # % of 1 standard deviation\n\nnoisy_imgs = [i == 1 ? original : original .+ rand(Uniform(0, nL), size(original))\n    for (i, nL) in enumerate(noise_levels)]\n\n# a 2x2 stencil (i.e. dispersion/permutation patterns of length 4)\nstencil = ((2, 2), (1, 1))\n\nest_disp = SpatialDispersion(stencil, original; c = 5, periodic = false)\nest_perm = SpatialOrdinalPatterns(stencil, original; periodic = false)\nhs_disp = [information_normalized(est_disp, img) for img in noisy_imgs]\nhs_perm = [information_normalized(est_perm, img) for img in noisy_imgs]\n\n# Plot the results\nfig = Figure(size = (800, 1000))\nax = Axis(fig[1, 1:length(noise_levels)],\n    xlabel = \"Noise level\",\n    ylabel = \"Normalized entropy\")\nscatterlines!(ax, noise_levels, hs_disp, label = \"Dispersion\")\nscatterlines!(ax, noise_levels, hs_perm, label = \"Permutation\")\nylims!(ax, 0, 1.05)\naxislegend(position = :rb)\nfor (i, nl) in enumerate(noise_levels)\n    ax_i = Axis(fig[2, i])\n    image!(ax_i, Matrix(Float32.(noisy_imgs[i])), label = \"$nl\")\n    hidedecorations!(ax_i)  # hides ticks, grid and lables\n    hidespines!(ax_i)  # hide the frame\nend\nfig While the normalized  SpatialOrdinalPatterns  entropy quickly approaches its maximum value, the normalized  SpatialDispersion  entropy much better resolves the increase in entropy as the image gets noiser. This can probably be explained by the fact that the number of possible states (or  total_outcomes ) for any given  stencil  is larger for  SpatialDispersion  than for  SpatialOrdinalPatterns , so the dispersion approach is much less sensitive to noise addition (i.e. noise saturation over the possible states is slower for  SpatialDispersion )."},{"id":621,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: reverse dispersion entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Complexity:-reverse-dispersion-entropy","content":" Complexity: reverse dispersion entropy Here, we compare regular dispersion entropy ( Rostaghi and Azami, 2016 ), and reverse dispersion entropy ( Li  et al. , 2019 ) for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example reproduces parts of figure 3 in ( Li  et al. , 2019 ), but results here are not exactly the same as in the original paper, because their examples are based on randomly generated numbers and do not provide code that specify random number seeds. using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_rd = ReverseDispersion(; c, m, τ = 1)\nest_de = Dispersion(; c, m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = complexity_normalized(est_rd, y[window])\n    des[i] = information_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig"},{"id":622,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: missing dispersion patterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Complexity:-missing-dispersion-patterns","content":" Complexity: missing dispersion patterns using ComplexityMeasures\nusing CairoMakie\nusing DynamicalSystemsBase\nusing TimeseriesSurrogates\n\nest = MissingDispersionPatterns(Dispersion(m = 3, c = 7))\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nsys = DeterministicIteratedMap(logistic_rule, [0.6], [4.0])\nLs = collect(100:100:1000)\nnL = length(Ls)\nnreps = 30 # should be higher for real applications\nmethod = WLS(IAAFT(), rescale = true)\n\nr_det, r_noise = zeros(length(Ls)), zeros(length(Ls))\nr_det_surr, r_noise_surr = [zeros(nreps) for L in Ls], [zeros(nreps) for L in Ls]\ny = rand(maximum(Ls))\n\nfor (i, L) in enumerate(Ls)\n    # Deterministic time series\n    x, t = trajectory(sys, L - 1, Ttr = 5000)\n    x = columns(x)[1] # remember to make it `Vector{<:Real}\n    sx = surrogenerator(x, method)\n    r_det[i] = complexity_normalized(est, x)\n    r_det_surr[i][:] = [complexity_normalized(est, sx()) for j = 1:nreps]\n\n    # Random time series\n    r_noise[i] = complexity_normalized(est, y[1:L])\n    sy = surrogenerator(y[1:L], method)\n    r_noise_surr[i][:] = [complexity_normalized(est, sy()) for j = 1:nreps]\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel = \"Time series length (L)\",\n    ylabel = \"# missing dispersion patterns (normalized)\"\n)\n\nlines!(ax, Ls, r_det, label = \"logistic(x0 = 0.6; r = 4.0)\", color = :black)\nlines!(ax, Ls, r_noise, label = \"Uniform noise\", color = :red)\nfor i = 1:nL\n    if i == 1\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black,\n            label = \"WIAAFT surrogates (logistic)\")\n         boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red,\n            label = \"WIAAFT surrogates (noise)\")\n    else\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black)\n        boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red)\n    end\nend\naxislegend(position = :rc)\nylims!(0, 1.1)\n\nfig We don't need to actually to compute the quantiles here to see that for the logistic map, across all time series lengths, the  $N_{MDP}$  values are above the extremal values of the  $N_{MDP}$  values for the surrogate ensembles. Thus, we conclude that the logistic map time series has nonlinearity (well, of course). For the univariate noise time series, there is considerable overlap between  $N_{MDP}$  for the surrogate distributions and the original signal, so we can't claim nonlinearity for this signal. Of course, to robustly reject the null hypothesis, we'd need to generate a sufficient number of surrogate realizations, and actually compute quantiles to compare with."},{"id":623,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: approximate entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Complexity:-approximate-entropy","content":" Complexity: approximate entropy Here, we reproduce the Henon map example with  $R=0.8$  from  Pincus (1991) , comparing our values with relevant values from table 1 in  Pincus (1991) . We use  DiscreteDynamicalSystem  from  DynamicalSystemsBase  to represent the map, and use the  trajectory  function from the same package to iterate the map for different initial conditions, for multiple time series lengths. Finally, we summarize our results in box plots and compare the values to those obtained by  Pincus (1991) . using ComplexityMeasures\nusing DynamicalSystemsBase\nusing DelayEmbeddings\nusing CairoMakie\n\n# Equation 13 in Pincus (1991)\nfunction henon_rule(u, p, n)\n    R = p[1]\n    x, y = u\n    dx = R*y + 1 - 1.4*x^2\n    dy = 0.3*R*x\n    return SVector(dx, dy)\nend\n\nfunction henon(; u₀ = rand(2), R = 0.8)\n    DeterministicIteratedMap(henon_rule, u₀, [R])\nend\n\nts_lengths = [300, 1000, 2000, 3000]\nnreps = 100\napens_08 = [zeros(nreps) for i = 1:length(ts_lengths)]\n\n# For some initial conditions, the Henon map as specified here blows up,\n# so we need to check for infinite values.\ncontainsinf(x) = any(isinf.(x))\n\nc = ApproximateEntropy(r = 0.05, m = 2)\n\nfor (i, L) in enumerate(ts_lengths)\n    k = 1\n    while k <= nreps\n        sys = henon(u₀ = rand(2), R = 0.8)\n        t = trajectory(sys, L; Ttr = 5000)[1]\n\n        if !any([containsinf(tᵢ) for tᵢ in t])\n            x, y = columns(t)\n            apens_08[i][k] = complexity(c, x)\n            k += 1\n        end\n    end\nend\n\nfig = Figure()\n\n# Example time series\na1 = Axis(fig[1,1]; xlabel = \"Time (t)\", ylabel = \"Value\")\nsys = henon(u₀ = [0.5, 0.1], R = 0.8)\nx, y = columns(first(trajectory(sys, 100, Ttr = 500))) # we don't need time indices\nlines!(a1, 1:length(x), x, label = \"x\")\nlines!(a1, 1:length(y), y, label = \"y\")\n\n# Approximate entropy values, compared to those of the original paper (black dots).\na2 = Axis(fig[2, 1];\n    xlabel = \"Time series length (L)\",\n    ylabel = \"ApEn(m = 2, r = 0.05)\")\n\n# hacky boxplot, but this seems to be how it's done in Makie at the moment\nn = length(ts_lengths)\nfor i = 1:n\n    boxplot!(a2, fill(ts_lengths[i], n), apens_08[i];\n        width = 200)\nend\n\nscatter!(a2, ts_lengths, [0.337, 0.385, NaN, 0.394];\n    label = \"Pincus (1991)\", color = :black)\nfig"},{"id":624,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: sample entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Complexity:-sample-entropy","content":" Complexity: sample entropy Completely regular signals should have sample entropy approaching zero, while less regular signals should have higher sample entropy. using ComplexityMeasures\nusing CairoMakie\nN, a = 2000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5));\nz = rand(N)\n\nh_x, h_y, h_z = map(t -> complexity(SampleEntropy(t), t), (x, y, z))\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig Next, we compare the sample entropy obtained for different values of the radius  r  for uniform noise, normally distributed noise, and a periodic signal. using ComplexityMeasures\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\nN = 2000\nx_U = rand(N)\nx_N = rand(Normal(0, 3), N)\nx_periodic = repeat(rand(20), N ÷ 20)\n\nx_U .= (x_U .- mean(x_U)) ./ std(x_U)\nx_N .= (x_N .- mean(x_N)) ./ std(x_N)\nx_periodic .= (x_periodic .- mean(x_periodic)) ./ std(x_periodic)\n\nrs = 10 .^ range(-1, 0, length = 30)\nbase = 2\nm = 2\nhs_U = [complexity_normalized(SampleEntropy(m = m, r = r), x_U) for r in rs]\nhs_N = [complexity_normalized(SampleEntropy(m = m, r = r), x_N) for r in rs]\nhs_periodic = [complexity_normalized(SampleEntropy(m = m, r = r), x_periodic) for r in rs]\n\nfig = Figure()\n# Time series\na1 = Axis(fig[1,1]; xlabel = \"r\", ylabel = \"Sample entropy\")\nlines!(a1, rs, hs_U, label = \"Uniform noise, U(0, 1)\")\nlines!(a1, rs, hs_N, label = \"Gaussian noise, N(0, 1)\")\nlines!(a1, rs, hs_periodic, label = \"Periodic signal\")\naxislegend()\nfig"},{"id":625,"pagetitle":"ComplexityMeasures.jl Examples","title":"Statistical complexity of iterated maps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#Statistical-complexity-of-iterated-maps","content":" Statistical complexity of iterated maps In this example, we reproduce parts of Fig. 1 in  Rosso  et al.  (2007) : We compute the statistical complexity of the Henon, logistic and Schuster map, as well as that of k-noise. using ComplexityMeasures\nusing Distances\nusing DynamicalSystemsBase\nusing CairoMakie\nusing FFTW\nusing Statistics\n\nN = 2^15\n\nfunction logistic(x0=0.4; r = 4.0)\n    return DeterministicIteratedMap(logistic_rule, SVector(x0), [r])\nend\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1 - x[1]))\nlogistic_jacob(x, p, n) = @inbounds SMatrix{1,1}(p[1]*(1 - 2x[1]))\n\nfunction henon(u0=zeros(2); a = 1.4, b = 0.3)\n    return DeterministicIteratedMap(henon_rule, u0, [a,b])\nend\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\n\nfunction schuster(x0=0.5, z=3.0/2)\n    return DeterministicIteratedMap(schuster_rule, SVector(x0), [z])\nend\nschuster_rule(x, p, n) = @inbounds SVector((x[1]+x[1]^p[1]) % 1)\n\n# generate noise with power spectrum that falls like 1/f^k\nfunction k_noise(k=3)\n    function f(N)\n        x = rand(Float64, N)\n        # generate power spectrum of random numbers and multiply by f^(-k/2)\n        x_hat = fft(x) .* abs.(vec(fftfreq(length(x)))) .^ (-k/2)\n        # set to zero for frequency zero\n        x_hat[1] = 0\n        return real.(ifft(x_hat))\n    end\n    return f\nend\n\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=L\"H_S\", ylabel=L\"C_{JS}\")\n\nm, τ = 6, 1\nm_kwargs = (\n        (color=:transparent,\n        strokecolor=:red,\n        marker=:utriangle,\n        strokewidth=2),\n        (color=:transparent,\n        strokecolor=:blue,\n        marker=:rect,\n        strokewidth=2),\n        (color=:magenta,\n        marker=:circle),\n        (color=:blue,\n        marker=:rect)\n    )\n\nn = 100\n\nc = StatisticalComplexity(\n    dist=JSDivergence(),\n    est=OrdinalPatterns(; m, τ),\n    entr=Renyi()\n)\nfor (j, (ds_gen, sym, ds_name)) in enumerate(zip(\n        (logistic, henon, schuster, k_noise),\n        (:utriangle, :rect, :dtriangle, :diamond),\n        (\"Logistic map\", \"Henon map\", \"Schuster map\", \"k-noise (k=3)\"),\n    ))\n\n    if j < 4\n        dim = dimension(ds_gen())\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            ic = rand(dim) * 0.3\n            ds = ds_gen(SVector{dim}(ic))\n            x, t = trajectory(ds, N, Ttr=100)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    else\n        ds = ds_gen()\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            x = ds(N)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    end\nend\n\nmin_curve, max_curve = entropy_complexity_curves(c)\nlines!(ax, min_curve; color=:black)\nlines!(ax, max_curve; color=:black)\naxislegend(; position=:lt)\nfig"},{"id":626,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: multiscale","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#multiscale_example","content":" Complexity: multiscale Let's use  multiscale  analysis to investigate the  SampleEntropy  of a signal across coarse-graining scales. using ComplexityMeasures\nusing CairoMakie\n\nN, a = 2000, 20\nt = LinRange(0, 2*a*π, N)\nscales = 1:10\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5)) .+ 0.2 .* x\nhs = multiscale_normalized(RegularDownsampling(; scales), SampleEntropy(y), y)\n\nfig = Figure()\nax1 = Axis(fig[1,1]; ylabel = \"y\")\nlines!(ax1, t, y; color = Cycled(1));\nax2 = Axis(fig[2, 1]; ylabel = \"Sample entropy (h)\", xlabel = \"Scale\")\nscatterlines!(ax2, scales |> collect, hs; color = Cycled(1));\nfig"},{"id":629,"pagetitle":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#information_measures","content":" Information measures (entropies and co.) Note Be sure you have gone through the  Tutorial  before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl."},{"id":630,"pagetitle":"Information measures (entropies and co.)","title":"Information measures API","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#Information-measures-API","content":" Information measures API The information measure API is defined by the  information  function, which takes as an input an  InformationMeasure , or some specialized  DiscreteInfoEstimator  or  DifferentialInfoEstimator  for estimating the discrete or differential variant of the measure. The functions  information_maximum  and  information_normalized  are also useful."},{"id":631,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.InformationMeasure","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure","content":" ComplexityMeasures.InformationMeasure  —  Type InformationMeasure InformationMeasure  is the supertype of all information measure  definitions . In this package, we define \"information measures\" as functionals of probability mass functions (\"discrete\" measures), or of probability density functions (\"differential\" measures). Examples are (generalized) entropies such as  Shannon  or  Renyi , or extropies like  ShannonExtropy .  Amigó  et al.  (2018)  provides a useful review of generalized entropies. Used with Any of the information measures listed below can be used with information , to compute a numerical value for the measure, given some input data. information_maximum , to compute the maximum possible value for the measure. information_normalized , to compute the normalized form of the   measure (divided by the maximum possible value). The  information_maximum / information_normalized  functions only works with the discrete version of the measure. See docstrings for the above functions for usage examples. Implementations Renyi . Tsallis . Shannon , which is a subcase of the above two in the limit  q → 1 . Kaniadakis . Curado . StretchedExponential . RenyiExtropy . TsallisExtropy . ShannonExtropy , which is a subcase of the above two in the limit  q → 1 . FluctuationComplexity . Estimators A particular information measure may have both a discrete and a continuous/differential definition, which are estimated using a  DifferentialInfoEstimator  or a  DifferentialInfoEstimator , respectively. source"},{"id":632,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.information","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}","content":" ComplexityMeasures.information  —  Method information([die::DiscreteInfoEstimator,] [est::ProbabilitiesEstimator,] o::OutcomeSpace, x) → h::Real\ninformation(o::OutcomeSpace, x) → h::Real Estimate a discrete information measure from input data  x  using the provided  DiscreteInfoEstimator  and  ProbabilitiesEstimator  over the given  OutcomeSpace . As an alternative, you can provide an  InformationMeasure  for the first argument ( die ) which will default to  PlugIn  estimation) for the information estimation. You may also skip the first argument ( die ), in which case  Shannon()  will be used. You may also skip the second argument ( est ), which will default to the  RelativeAmount  probabilities estimator. Note that some information measure estimators (e.g.,  GeneralizedSchuermann ) operate directly on counts and hence ignore  est . information([e::DiscreteInfoEstimator,] p::Probabilities) → h::Real\ninformation([e::DiscreteInfoEstimator,] c::Counts) → h::Real Like above, but estimate the information measure from the pre-computed  Probabilities p  or  Counts . Counts are converted into probabilities using  RelativeAmount , unless the estimator  e  uses counts directly. See also:  information_maximum ,  information_normalized  for a normalized version. Examples (naive estimation) The simplest way to estimate a discrete measure is to provide the  InformationMeasure  directly in combination with an  OutcomeSpace . This will use the \"naive\"  PlugIn  estimator for the measure, and the \"naive\"  RelativeAmount  estimator for the probabilities. x = randn(100) # some input data\no = ValueBinning(RectangularBinning(5)) # a 5-bin histogram outcome space\nh_s = information(Shannon(), o, x) Here are some more examples: x = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = information(ps) # gives 1, about 1 bit by definition (Shannon entropy by default)\nh = information(Shannon(), ps) # syntactically equivalent to the above\nh = information(Shannon(), UniqueElements(), x) # syntactically equivalent to above\nh = information(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\nh = information(OrdinalPatterns(;m=3), x) # gives about 2, again by definition Examples (bias-corrected estimation) It is known that both  PlugIn  estimation for information measures and  RelativeAmount  estimation for probabilities are biased. The scientific literature abounds with estimators that correct for this bias, both on the measure-estimation level and on the probability-estimation level. We thus provide the option to use any  DiscreteInfoEstimator  in combination with any  ProbabilitiesEstimator  for improved estimates. Note that custom probabilites estimators will only work with counting-compatible  OutcomeSpace . x = randn(100)\no = ValueBinning(RectangularBinning(5))\n\n# Estimate Shannon entropy estimation using various dedicated estimators\nh_s = information(MillerMadow(Shannon()), RelativeAmount(), o, x)\nh_s = information(HorvitzThompson(Shannon()), Shrinkage(), o, x)\nh_s = information(Schuermann(Shannon()), Shrinkage(), o, x)\n\n# Estimate information measures using the generic `Jackknife` estimator\nh_r = information(Jackknife(Renyi()), Shrinkage(), o, x)\nj_t = information(Jackknife(TsallisExtropy()), BayesianRegularization(), o, x)\nj_r = information(Jackknife(RenyiExtropy()), RelativeAmount(), o, x) source"},{"id":633,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.information","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.information-Tuple{DifferentialInfoEstimator, Any}","content":" ComplexityMeasures.information  —  Method information(est::DifferentialInfoEstimator, x) → h::Real Estimate a  differential information measure  using the provided  DifferentialInfoEstimator  and input data  x . Description The overwhelming majority of differential estimators estimate the  Shannon  entropy. If the same estimator can estimate different information measures (e.g. it can estimate both  Shannon  and  Tsallis ), then the information measure is provided as an argument to the estimator itself. See the  table of differential information measure estimators  in the docs for all differential information measure estimators. Currently, unlike for the discrete information measures, this method doesn't involve explicitly first computing a probability density function and then passing this density to an information measure definition. But in the future, we want to establish a  density  API similar to the  probabilities  API. Examples To compute the differential version of a measure, give it as the first argument to a  DifferentialInfoEstimator  and pass it to  information . x = randn(1000)\nh_sh = information(Kraskov(Shannon()), x)\nh_vc = information(Vasicek(Shannon()), x) A normal distribution has a base-e Shannon differential entropy of  0.5*log(2π) + 0.5  nats. est = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.\nh = information(est, randn(2_000_000))\nabs(h - 0.5*log(2π) - 0.5) # ≈ 0.0001 source"},{"id":634,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.information_maximum","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.information_maximum","content":" ComplexityMeasures.information_maximum  —  Function information_maximum(e::InformationMeasure, o::OutcomeSpace [, x]) Return the maximum value of the given information measure can have, given input data  x  and the given outcome space (the  OutcomeSpace  may also be specified by a  ProbabilitiesEstimator ). Like in  outcome_space , for some outcome spaces, the possible outcomes are known without knowledge of input  x , in which case the function dispatches to  information_maximum(e, o) . information_maximum(e::InformationMeasure, L::Int) The same as above, but computed directly from the number of total outcomes  L . source"},{"id":635,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.information_normalized","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.information_normalized","content":" ComplexityMeasures.information_normalized  —  Function information_normalized([e::DiscreteInfoEstimator,] [est::ProbabilitiesEstimator,] o::OutcomeSpace, x) → h::Real Estimate the normalized version of the given discrete information measure, This is just the value of  information  divided its maximum possible value given  o . The same convenience syntaxes as in  information  can be used here. Notice that there is no method  information_normalized(e::DiscreteInfoEstimator, probs::Probabilities) , because there is no way to know the number of  possible  outcomes (i.e., the  total_outcomes ) from  probs . Normalized values For the  PlugIn  estimator, it is guaranteed that  h̃ ∈ [0, 1] . For any other estimator, we can't guarantee this, since the estimator might over-correct. You should know what you're doing if using anything but  PlugIn  to estimate normalized values. source"},{"id":636,"pagetitle":"Information measures (entropies and co.)","title":"Entropies","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#Entropies","content":" Entropies"},{"id":637,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.entropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.entropy","content":" ComplexityMeasures.entropy  —  Function entropy(args...) entropy  is nothing more than a call to  information  that will simply throw an error if used with an information measure that is not an entropy. source"},{"id":638,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Shannon","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Shannon","content":" ComplexityMeasures.Shannon  —  Type Shannon <: InformationMeasure\nShannon(; base = 2) The Shannon ( Shannon, 1948 ) entropy, used with  information  to compute: \\[H(p) = - \\sum_i p[i] \\log(p[i])\\] with the  $\\log$  at the given  base . The maximum value of the Shannon entropy is  $\\log_{base}(L)$ , which is the entropy of the uniform distribution with  $L$  the  total_outcomes . source"},{"id":639,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Renyi","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Renyi","content":" ComplexityMeasures.Renyi  —  Type Renyi <: InformationMeasure\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2) The Rényi generalized order- q  entropy ( Rényi, 1961 ), used with  information  to compute an entropy with units given by  base  (typically  2  or  MathConstants.e ). Description Let  $p$  be an array of probabilities (summing to 1). Then the Rényi generalized entropy is \\[H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)\\] and generalizes other known entropies, like e.g. the information entropy ( $q = 1$ , see  Shannon (1948) ), the maximum entropy ( $q=0$ , also known as Hartley entropy), or the correlation entropy ( $q = 2$ , also known as collision entropy). The maximum value of the Rényi entropy is  $\\log_{base}(L)$ , which is the entropy of the uniform distribution with  $L$  the  total_outcomes . source"},{"id":640,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Tsallis","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Tsallis","content":" ComplexityMeasures.Tsallis  —  Type Tsallis <: InformationMeasure\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2) The Tsallis generalized order- q  entropy ( Tsallis, 1988 ), used with  information  to compute an entropy. base  only applies in the limiting case  q == 1 , in which the Tsallis entropy reduces to  Shannon  entropy. Description The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with  k  standing for the Boltzmann constant. It is defined as \\[S_q(p) = \\frac{k}{q - 1}\\left(1 - \\sum_{i} p[i]^q\\right)\\] The maximum value of the Tsallis entropy is  $k(L^{1 - q} - 1)/(1 - q)$ , with  $L$  the  total_outcomes . source"},{"id":641,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Kaniadakis","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Kaniadakis","content":" ComplexityMeasures.Kaniadakis  —  Type Kaniadakis <: InformationMeasure\nKaniadakis(; κ = 1.0, base = 2.0) The Kaniadakis entropy ( Tsallis, 2009 ), used with  information  to compute \\[H_K(p) = -\\sum_{i=1}^N p_i f_\\kappa(p_i),\\] \\[f_\\kappa (x) = \\dfrac{x^\\kappa - x^{-\\kappa}}{2\\kappa},\\] where if  $\\kappa = 0$ , regular logarithm to the given  base  is used, and 0 probabilities are skipped. source"},{"id":642,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Curado","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Curado","content":" ComplexityMeasures.Curado  —  Type Curado <: InformationMeasure\nCurado(; b = 1.0) The Curado entropy ( Curado and Nobre, 2004 ), used with  information  to compute \\[H_C(p) = \\left( \\sum_{i=1}^N e^{-b p_i} \\right) + e^{-b} - 1,\\] with  b ∈ ℛ, b > 0 , and the terms outside the sum ensures that  $H_C(0) = H_C(1) = 0$ . The maximum entropy for  Curado  is  $L(1 - \\exp(-b/L)) + \\exp(-b) - 1$  with  $L$  the  total_outcomes . source"},{"id":643,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.StretchedExponential","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.StretchedExponential","content":" ComplexityMeasures.StretchedExponential  —  Type StretchedExponential <: InformationMeasure\nStretchedExponential(; η = 2.0, base = 2) The stretched exponential, or Anteneodo-Plastino, entropy ( Anteneodo and Plastino, 1999 ), used with  information  to compute \\[S_{\\eta}(p) = \\sum_{i = 1}^N\n\\Gamma \\left( \\dfrac{\\eta + 1}{\\eta}, - \\log_{base}(p_i) \\right) -\np_i \\Gamma \\left( \\dfrac{\\eta + 1}{\\eta} \\right),\\] where  $\\eta \\geq 0$ ,  $\\Gamma(\\cdot, \\cdot)$  is the upper incomplete Gamma function, and  $\\Gamma(\\cdot) = \\Gamma(\\cdot, 0)$  is the Gamma function. Reduces to  Shannon  entropy for  η = 1.0 . The maximum entropy for  StrechedExponential  is a rather complicated expression involving incomplete Gamma functions (see source code). source"},{"id":644,"pagetitle":"Information measures (entropies and co.)","title":"Other information measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#Other-information-measures","content":" Other information measures"},{"id":645,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.ShannonExtropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.ShannonExtropy","content":" ComplexityMeasures.ShannonExtropy  —  Type ShannonExtropy <: InformationMeasure\nShannonExtropy(; base = 2) The Shannon extropy ( Lad  et al. , 2015 ), used with  information  to compute \\[J(x) = -\\sum_{i=1}^N (1 - p[i]) \\log{(1 - p[i])},\\] for a probability distribution  $P = \\{p_1, p_2, \\ldots, p_N\\}$ , with the  $\\log$  at the given  base . source"},{"id":646,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.RenyiExtropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.RenyiExtropy","content":" ComplexityMeasures.RenyiExtropy  —  Type RenyiExtropy <: InformationMeasure\nRenyiExtropy(; q = 1.0, base = 2) The Rényi extropy ( Liu and Xiao, 2023 ). Description RenyiExtropy  is used with  information  to compute \\[J_R(P) = \\dfrac{-(n - 1) \\log{(n - 1)} + (n - 1) \\log{ \\left( \\sum_{i=1}^N {(1 - p[i])}^q \\right)} }{q - 1}\\] for a probability distribution  $P = \\{p_1, p_2, \\ldots, p_N\\}$ , with the  $\\log$  at the given  base . Alternatively,  RenyiExtropy  can be used with  information_normalized , which ensures that the computed extropy is on the interval  $[0, 1]$  by normalizing to to the maximal Rényi extropy, given by \\[J_R(P) = (N - 1)\\log \\left( \\dfrac{n}{n-1} \\right) .\\] source"},{"id":647,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.TsallisExtropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.TsallisExtropy","content":" ComplexityMeasures.TsallisExtropy  —  Type TsallisExtropy <: InformationMeasure\nTsallisExtropy(; base = 2) The Tsallis extropy ( Xue and Deng, 2023 ). Description TsallisExtropy  is used with  information  to compute \\[J_T(P) = k \\dfrac{N - 1 - \\sum_{i=1}^N ( 1 - p[i])^q}{q - 1}\\] for a probability distribution  $P = \\{p_1, p_2, \\ldots, p_N\\}$ , with the  $\\log$  at the given  base . Alternatively,  TsallisExtropy  can be used with  information_normalized , which ensures that the computed extropy is on the interval  $[0, 1]$  by normalizing to to the maximal Tsallis extropy, given by \\[J_T(P) = \\dfrac{(N - 1)N^{q - 1} - (N - 1)^q}{(q - 1)N^{q - 1}}\\] source"},{"id":648,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.ElectronicEntropy","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.ElectronicEntropy","content":" ComplexityMeasures.ElectronicEntropy  —  Type ElectronicEntropy <: InformationMeasure\nElectronicEntropy(; h = Shannon(; base = 2), j = ShannonExtropy(; base = 2)) The  \"electronic entropy\"  measure is defined in discrete form in  Lad  et al.  (2015)  as \\[H_{EL}(p) = H_S(p) + J_S(P),\\] where  $H_S(p)$  is the  Shannon  entropy and  $J_S(p)$  is the  ShannonExtropy  extropy of the probability vector  $p$ . source"},{"id":649,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.FluctuationComplexity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.FluctuationComplexity","content":" ComplexityMeasures.FluctuationComplexity  —  Type FluctuationComplexity <: InformationMeasure\nFluctuationComplexity(; definition = Shannon(; base = 2), base = 2) The \"fluctuation complexity\" quantifies the standard deviation of the information content of the states   $\\omega_i$  around some summary statistic ( InformationMeasure ) of a PMF. Specifically, given some  outcome space  $\\Omega$  with outcomes  $\\omega_i \\in \\Omega$   and a probability mass function  $p(\\Omega) = \\{ p(\\omega_i) \\}_{i=1}^N$ , it is defined as \\[\\sigma_I(p) := \\sqrt{\\sum_{i=1}^N p_i(I_i - H_*)^2}\\] where  $I_i = -\\log_{base}(p_i)$  is the information content of the i-th outcome. The type of information measure  $*$  is controlled by  definition .  The  base  controls the base of the logarithm that goes into the information content terms. Make sure that  you pick a  base  that is consistent with the base chosen for the  definition  (relevant for e.g.  Shannon ). Properties If  definition  is the  Shannon  entropy, then we recover  the  Shannon-type information fluctuation complexity   from ( Bates and Shepard, 1993 ). Then the fluctuation complexity is zero for PMFs with only a single non-zero element, or  for the uniform distribution. If  definition  is not Shannon entropy, then the properties of the measure varies, and does not necessarily share the  properties ( Bates and Shepard, 1993 ).  Potential for new research As far as we know, using other information measures besides Shannon entropy for the  fluctuation complexity hasn't been explored in the literature yet. Our implementation, however, allows for it. Please inform us if you try some new combinations! source"},{"id":650,"pagetitle":"Information measures (entropies and co.)","title":"Discrete information estimators","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#Discrete-information-estimators","content":" Discrete information estimators"},{"id":651,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.DiscreteInfoEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.DiscreteInfoEstimator","content":" ComplexityMeasures.DiscreteInfoEstimator  —  Type DiscreteInfoEstimator The supertype of all discrete information measure estimators, which are used in combination with a  ProbabilitiesEstimator  as input to   information  or related functions. The first argument to a discrete estimator is always an  InformationMeasure  (defaults to  Shannon ). Description A discrete  InformationMeasure  is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a  ProbabilitiesEstimator  from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the  Shannon  entropy is typically computed using the  RelativeAmount  estimator to compute probabilities, which are then given to the  PlugIn  estimator. Many other estimators exist, not only for  Shannon  entropy, but other information measures as well. We provide a library of both generic estimators such as  PlugIn  or  Jackknife  (which can be applied to any measure), as well as dedicated estimators such as  MillerMadow , which computes  Shannon  entropy using the Miller-Madow bias correction. The list below gives a complete overview. Implementations The following estimators are generic and can compute any  InformationMeasure . PlugIn . The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the computed   probability mass function. Jackknife . Uses the a combination of the plug-in estimator and the jackknife   principle to estimate the information measure. Shannon  entropy estimators The following estimators are dedicated  Shannon  entropy estimators, which provide improvements over the naive  PlugIn  estimator. MillerMadow . HorvitzThompson . Schuermann . GeneralizedSchuermann . ChaoShen . Info Any of the implemented  DiscreteInfoEstimator s can be used in combination with  any ProbabilitiesEstimator  as input to  information . What this means is that every estimator actually comes in many different variants - one for each  ProbabilitiesEstimator . For example, the  MillerMadow  estimator of  Shannon  entropy is typically calculated with  RelativeAmount  probabilities. But here, you can use for example the  BayesianRegularization  or the  Shrinkage  probabilities estimators instead, i.e.  information(MillerMadow(), RelativeAmount(outcome_space), x)  and  information(MillerMadow(), BayesianRegularization(outcomes_space), x)  are distinct estimators. This holds for all  DiscreteInfoEstimator s. Many of these estimators haven't been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination! source"},{"id":652,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.PlugIn","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.PlugIn","content":" ComplexityMeasures.PlugIn  —  Type PlugIn(e::InformationMeasure) <: DiscreteInfoEstimatorGeneric The  PlugIn  estimator is also called the empirical/naive/\"maximum likelihood\" estimator, and is used with  information  to any discrete  InformationMeasure . It computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood ( RelativeAmount  estimates of the probabilities. Bias of plug-in estimates The plugin-estimator of  Shannon  entropy underestimates the true entropy, with a bias that grows with the number of distinct  outcomes  (Arora et al., 2022)( Arora  et al. , 2022 ), \\[bias(H_S^{plugin}) = -\\dfrac{K-1}{2N} + o(N^-1).\\] where  K  is the number of distinct outcomes, and  N  is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the  MillerMadow  estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see  DiscreteInfoEstimator s for an overview. source"},{"id":653,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.MillerMadow","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.MillerMadow","content":" ComplexityMeasures.MillerMadow  —  Type MillerMadow <: DiscreteInfoEstimatorShannon\nMillerMadow(measure::Shannon = Shannon()) The  MillerMadow  estimator is used with  information  to compute the discrete  Shannon  entropy according to  Miller (1955) . Description The Miller-Madow estimator of Shannon entropy is given by \\[H_S^{MM} = H_S^{plugin} + \\dfrac{m - 1}{2N},\\] where  $H_S^{plugin}$  is the Shannon entropy estimated using the  PlugIn  estimator,  m  is the number of bins with nonzero probability (as defined in  Paninski (2003) ), and  N  is the number of observations. source"},{"id":654,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Schuermann","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Schuermann","content":" ComplexityMeasures.Schuermann  —  Type Schuermann <: DiscreteInfoEstimatorShannon\nSchuermann(definition::Shannon; a = 1.0) The  Schuermann  estimator is used with  information  to compute the discrete  Shannon  entropy with the bias-corrected estimator given in  Schürmann (2004) . See detailed description for  GeneralizedSchuermann  for details. source"},{"id":655,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.GeneralizedSchuermann","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.GeneralizedSchuermann","content":" ComplexityMeasures.GeneralizedSchuermann  —  Type GeneralizedSchuermann <: DiscreteInfoEstimatorShannon\nGeneralizedSchuermann(definition = Shannon(); a = 1.0) The  GeneralizedSchuermann  estimator is used with  information  to compute the discrete  Shannon  entropy with the bias-corrected estimator given in  Grassberger (2022) . The \"generalized\" part of the name, as opposed to the  Schürmann (2004)  estimator ( Schuermann ), is due to the possibility of picking difference parameters  $a_i$  for different outcomes. If different parameters are assigned to the different outcomes,  a  must be a vector of parameters of length  length(outcomes) , where the outcomes are obtained using  outcomes . See  Grassberger (2022)  for more information. If  a  is a real number, then  $a_i = a \\forall i$ , and the estimator reduces to the  Schuermann  estimator. Description For a set of  $N$  observations over  $M$  outcomes, the estimator is given by \\[H_S^{opt} = \\varphi(N) - \\dfrac{1}{N} \\sum_{i=1}^M n_i G_{n_i}(a_i),\\] where  $n_i$  is the observed frequency of the i-th outcome, \\[G_n(a) = \\varphi(n) + (-1)^n \\int_0^a \\dfrac{x^{n - 1}}{x + 1} dx,\\] $G_n(1) = G_n$  and  $G_n(0) = \\varphi(n)$ , and \\[G_n = \\varphi(n) + (-1)^n \\int_0^1 \\dfrac{x^{n - 1}}{x + 1} dx.\\] source"},{"id":656,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Jackknife","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Jackknife","content":" ComplexityMeasures.Jackknife  —  Type Jackknife <: DiscreteInfoEstimatorGeneric\nJackknife(definition::InformationMeasure = Shannon()) The  Jackknife  estimator is used with  information  to compute any discrete  InformationMeasure . The  Jackknife  estimator uses the generic jackknife principle to reduce bias.  Zahl (1977)  was the first to apply the jaccknife technique in the context of  Shannon  entropy estimation. Here, we've generalized his estimator to work with any  InformationMeasure . Description As an example of the jackknife technique, here is the formula for a jackknife estimate of  Shannon  entropy \\[H_S^{J} = N H_S^{plugin} - \\dfrac{N-1}{N} \\sum_{i=1}^N {H_S^{plugin}}^{-\\{i\\}},\\] where  $N$  is the sample size,  $H_S^{plugin}$  is the plugin estimate of Shannon entropy, and  ${H_S^{plugin}}^{-\\{i\\}}$  is the plugin estimate, but computed with the  $i$ -th sample left out. source"},{"id":657,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.HorvitzThompson","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.HorvitzThompson","content":" ComplexityMeasures.HorvitzThompson  —  Type HorvitzThompson <: DiscreteInfoEstimatorShannon\nHorvitzThompson(measure::Shannon = Shannon()) The  HorvitzThompson  estimator is used with  information  to compute the discrete  Shannon  entropy according to  Horvitz and Thompson (1952) . Description The Horvitz-Thompson estimator of  Shannon  entropy is given by \\[H_S^{HT} = -\\sum_{i=1}^M \\dfrac{p_i \\log(p_i) }{1 - (1 - p_i)^N},\\] where  $N$  is the sample size and  $M$  is the number of  outcomes . Given the true probability  $p_i$  of the  $i$ -th outcome,  $1 - (1 - p_i)^N$  is the probability that the outcome appears at least once in a sample of size  $N$  ( Arora  et al. , 2022 ). Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions. source"},{"id":658,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.ChaoShen","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.ChaoShen","content":" ComplexityMeasures.ChaoShen  —  Type ChaoShen <: DiscreteInfoEstimatorShannon\nChaoShen(definition::Shannon = Shannon()) The  ChaoShen  estimator is used with  information  to compute the discrete  Shannon  entropy according to  Chao and Shen (2003) . Description This estimator is a modification of the  HorvitzThompson  estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If  $f_1$  is the number of singletons (outcomes that occur only once) in a sample of length  $N$ , then the sample coverage is  $C = 1 - \\dfrac{f_1}{N}$ . The Chao-Shen estimator of Shannon entropy is then \\[H_S^{CS} = -\\sum_{i=1}^M \\left( \\dfrac{C p_i \\log(C p_i)}{1 - (1 - C p_i)^N} \\right),\\] where  $N$  is the sample size and  $M$  is the number of  outcomes . If  $f_1 = N$ , then  $f_1$  is set to  $f_1 = N - 1$  to ensure positive entropy ( Arora  et al. , 2022 ). source"},{"id":659,"pagetitle":"Information measures (entropies and co.)","title":"Differential information estimators","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#Differential-information-estimators","content":" Differential information estimators"},{"id":660,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.DifferentialInfoEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.DifferentialInfoEstimator","content":" ComplexityMeasures.DifferentialInfoEstimator  —  Type DifferentialInfoEstimator The supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution. Each  DifferentialInfoEstimator s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example,  Kraskov  estimates the  Shannon  entropy. See  information  for usage. Implementations KozachenkoLeonenko . Kraskov . Goria . Gao . Zhu ZhuSingh . Lord . AlizadehArghami . Correa . Vasicek . Ebrahimi . LeonenkoProzantoSavani . source"},{"id":661,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Kraskov","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Kraskov","content":" ComplexityMeasures.Kraskov  —  Type Kraskov <: DifferentialInfoEstimator\nKraskov(definition = Shannon(); k::Int = 1, w::Int = 0) The  Kraskov  estimator computes the  Shannon  differential  information  of a multi-dimensional  StateSpaceSet  using the  k -th nearest neighbor searches method from  Kraskov  et al.  (2004) , with logarithms to the  base  specified in  definition . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Kraskov  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] See also:  information ,  KozachenkoLeonenko ,  DifferentialInfoEstimator . source"},{"id":662,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.KozachenkoLeonenko","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.KozachenkoLeonenko","content":" ComplexityMeasures.KozachenkoLeonenko  —  Type KozachenkoLeonenko <: DifferentialInfoEstimator\nKozachenkoLeonenko(definition = Shannon(); w::Int = 0) The  KozachenkoLeonenko  estimator ( Kozachenko and Leonenko, 1987 ) computes the  Shannon  differential  information  of a multi-dimensional  StateSpaceSet , with logarithms to the  base  specified in  definition . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  KozachenkoLeonenko  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))]\\] using the nearest neighbor method from  Kozachenko and Leonenko (1987) , as described in  Charzyńska and Gambin (2016) . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). In contrast to  Kraskov , this estimator uses only the  closest  neighbor. See also:  information ,  Kraskov ,  DifferentialInfoEstimator . source"},{"id":663,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Zhu","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Zhu","content":" ComplexityMeasures.Zhu  —  Type Zhu <: DifferentialInfoEstimator\nZhu(; definition = Shannon(), k = 1, w = 0) The  Zhu  estimator ( Zhu  et al. , 2015 ) is an extension to  KozachenkoLeonenko , and computes the  Shannon  differential  information  of a multi-dimensional  StateSpaceSet , with logarithms to the  base  specified in  definition . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Zhu  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))]\\] by approximating densities within hyperrectangles surrounding each point  xᵢ ∈ x  using using  k  nearest neighbor searches.  w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). See also:  information ,  KozachenkoLeonenko ,  DifferentialInfoEstimator . source"},{"id":664,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.ZhuSingh","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.ZhuSingh","content":" ComplexityMeasures.ZhuSingh  —  Type ZhuSingh <: DifferentialInfoEstimator\nZhuSingh(definition = Shannon(); k = 1, w = 0) The  ZhuSingh  estimator ( Zhu  et al. , 2015 ) computes the  Shannon  differential  information  of a multi-dimensional  StateSpaceSet , with logarithms to the  base  specified in  definition . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  ZhuSingh  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] Like  Zhu , this estimator approximates probabilities within hyperrectangles surrounding each point  xᵢ ∈ x  using using  k  nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in  Singh  et al.  (2003) . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). See also:  information ,  DifferentialInfoEstimator . source"},{"id":665,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Gao","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Gao","content":" ComplexityMeasures.Gao  —  Type Gao <: DifferentialInfoEstimator\nGao(definition = Shannon(); k = 1, w = 0, corrected = true) The  Gao  estimator ( Gao  et al. , 09–12 May 2015 ) computes the  Shannon  differential  information , using a  k -th nearest-neighbor approach based on  Singh  et al.  (2003) , with logarithms to the  base  specified in  definition . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). Gao  et al.  (09–12 May 2015)  give two variants of this estimator. If  corrected == false , then the uncorrected version is used. If  corrected == true , then the corrected version is used, which ensures that the estimator is asymptotically unbiased. Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  KozachenkoLeonenko  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] source"},{"id":666,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Goria","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Goria","content":" ComplexityMeasures.Goria  —  Type Goria <: DifferentialInfoEstimator\nGoria(measure = Shannon(); k = 1, w = 0) The  Goria  estimator ( Goria  et al. , 2005 ) computes the  Shannon  differential  information  of a multi-dimensional  StateSpaceSet , with logarithms to the  base  specified in  definition . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Goria  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] Specifically, let  $\\bf{n}_1, \\bf{n}_2, \\ldots, \\bf{n}_N$  be the distance of the samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  to their  k -th nearest neighbors. Next, let the geometric mean of the distances be \\[\\hat{\\rho}_k = \\left( \\prod_{i=1}^N \\right)^{\\dfrac{1}{N}}\\] Goria  et al.  (2005) 's estimate of Shannon differential entropy is then \\[\\hat{H} = m\\hat{\\rho}_k + \\log(N - 1) - \\psi(k) + \\log c_1(m),\\] where  $c_1(m) = \\dfrac{2\\pi^\\frac{m}{2}}{m \\Gamma(m/2)}$  and  $\\psi$  is the digamma function. source"},{"id":667,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Lord","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Lord","content":" ComplexityMeasures.Lord  —  Type Lord <: DifferentialInfoEstimator\nLord(measure = Shannon(); k = 10, w = 0) The  Lord  estimator ( Lord  et al. , 2018 ) estimates the  Shannon  differential  information  using a nearest neighbor approach with a local nonuniformity correction (LNC), with logarithms to the  base  specified in  definition . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). Description Assume we have samples  $\\bar{X} = \\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function  $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Lord  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))],\\] by using the resubstitution formula \\[\\hat{\\bar{X}, k} = -\\mathbb{E}[\\log(f(X))]\n\\approx \\sum_{i = 1}^N \\log(\\hat{f}(\\bf{x}_i)),\\] where  $\\hat{f}(\\bf{x}_i)$  is an estimate of the density at  $\\bf{x}_i$  constructed in a manner such that  $\\hat{f}(\\bf{x}_i) \\propto \\dfrac{k(x_i) / N}{V_i}$ , where  $k(x_i)$  is the number of points in the neighborhood of  $\\bf{x}_i$ , and  $V_i$  is the volume of that neighborhood. While most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities  $\\hat{f}(\\bf{x}_i)$ , the  Lord  estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point  xᵢ , estimated using singular value decomposition (SVD) on the  k -th nearest neighbors of  xᵢ . Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes  Lord  a well-suited entropy estimator for a wide range of systems. source"},{"id":668,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.LeonenkoProzantoSavani","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.LeonenkoProzantoSavani","content":" ComplexityMeasures.LeonenkoProzantoSavani  —  Type LeonenkoProzantoSavani <: DifferentialInfoEstimator\nLeonenkoProzantoSavani(definition = Shannon(); k = 1, w = 0) The  LeonenkoProzantoSavani  estimator ( Leonenko  et al. , 2008 ) computes the   Shannon ,  Renyi , or  Tsallis  differential  information  of a multi-dimensional  StateSpaceSet , with logarithms to the  base  specified in  definition . Description The estimator uses  k -th nearest-neighbor searches.   w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). For details, see  Leonenko  et al.  (2008) . source"},{"id":669,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Vasicek","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Vasicek","content":" ComplexityMeasures.Vasicek  —  Type Vasicek <: DifferentialInfoEstimator\nVasicek(definition = Shannon(); m::Int = 1) The  Vasicek  estimator computes the  Shannon  differential  information  of a timeseries using the method from  Vasicek (1976) , with logarithms to the  base  specified in  definition . The  Vasicek  estimator belongs to a class of differential entropy estimators based on  order statistics , of which  Vasicek (1976)  was the first. It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  Vasicek  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly, it makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ , \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ . The  Vasicek Shannon  differential entropy estimate is then \\[\\hat{H}_V(\\bar{X}, m) =\n\\dfrac{1}{n}\n\\sum_{i = 1}^n \\log \\left[ \\dfrac{n}{2m} (\\bar{X}_{(i+m)} - \\bar{X}_{(i-m)}) \\right]\\] Usage In practice, choice of  m  influences how fast the entropy converges to the true value. For small value of  m , convergence is slow, so we recommend to scale  m  according to the time series length  n  and use  m >= n/100  (this is just a heuristic based on the tests written for this package). See also:  information ,  Correa ,  AlizadehArghami ,  Ebrahimi ,  DifferentialInfoEstimator . source"},{"id":670,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.AlizadehArghami","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.AlizadehArghami","content":" ComplexityMeasures.AlizadehArghami  —  Type AlizadehArghami <: DifferentialInfoEstimator\nAlizadehArghami(definition = Shannon(); m::Int = 1) The  AlizadehArghami  estimator computes the  Shannon  differential  information  of a timeseries using the method from  Alizadeh and Arghami (2010) , with logarithms to the  base  specified in  definition . The  AlizadehArghami  estimator belongs to a class of differential entropy estimators based on  order statistics . It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  AlizadehArghami  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly, it makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ : \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp.\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ . The  AlizadehArghami Shannon  differential entropy estimate is then the the  Vasicek  estimate  $\\hat{H}_{V}(\\bar{X}, m, n)$ , plus a correction factor \\[\\hat{H}_{A}(\\bar{X}, m, n) = \\hat{H}_{V}(\\bar{X}, m, n) +\n\\dfrac{2}{n}\\left(m \\log(2) \\right).\\] See also:  information ,  Correa ,  Ebrahimi ,  Vasicek ,  DifferentialInfoEstimator . source"},{"id":671,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Ebrahimi","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Ebrahimi","content":" ComplexityMeasures.Ebrahimi  —  Type Ebrahimi <: DifferentialInfoEstimator\nEbrahimi(definition = Shannon(); m::Int = 1) The  Ebrahimi  estimator computes the  Shannon information  of a timeseries using the method from  Ebrahimi  et al.  (1994) , with logarithms to the  base  specified in  definition . The  Ebrahimi  estimator belongs to a class of differential entropy estimators based on  order statistics . It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  Ebrahimi  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly, it makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ , \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ . The  Ebrahimi Shannon  differential entropy estimate is then \\[\\hat{H}_{E}(\\bar{X}, m) =\n\\dfrac{1}{n} \\sum_{i = 1}^n \\log\n\\left[ \\dfrac{n}{c_i m} (\\bar{X}_{(i+m)} - \\bar{X}_{(i-m)}) \\right],\\] where \\[c_i =\n\\begin{cases}\n    1 + \\frac{i - 1}{m}, & 1 \\geq i \\geq m \\\\\n    2,                    & m + 1 \\geq i \\geq n - m \\\\\n    1 + \\frac{n - i}{m} & n - m + 1 \\geq i \\geq n\n\\end{cases}.\\] See also:  information ,  Correa ,  AlizadehArghami ,  Vasicek ,  DifferentialInfoEstimator . source"},{"id":672,"pagetitle":"Information measures (entropies and co.)","title":"ComplexityMeasures.Correa","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.Correa","content":" ComplexityMeasures.Correa  —  Type Correa <: DifferentialInfoEstimator\nCorrea(definition = Shannon(); m::Int = 1) The  Correa  estimator computes the  Shannon  differential  information  of a timeseries using the method from  Correa (1995) , with logarithms to the  base  specified in  definition . The  Correa  estimator belongs to a class of differential entropy estimators based on  order statistics . It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  Correa  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly,  Correa  makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ , \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ , ensuring that end points are included. The  Correa  estimate of  Shannon  differential entropy is then \\[H_C(\\bar{X}, m, n) =\n\\dfrac{1}{n} \\sum_{i = 1}^n \\log\n\\left[ \\dfrac{ \\sum_{j=i-m}^{i+m}(\\bar{X}_{(j)} -\n\\tilde{X}_{(i)})(j - i)}{n \\sum_{j=i-m}^{i+m} (\\bar{X}_{(j)} - \\tilde{X}_{(i)})^2}\n\\right],\\] where \\[\\tilde{X}_{(i)} = \\dfrac{1}{2m + 1} \\sum_{j = i - m}^{i + m} X_{(j)}.\\] See also:  information ,  AlizadehArghami ,  Ebrahimi ,  Vasicek ,  DifferentialInfoEstimator . source"},{"id":673,"pagetitle":"Information measures (entropies and co.)","title":"Table of differential information measure estimators","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#table_diff_ent_est","content":" Table of differential information measure estimators The following estimators are  differential  information measure estimators, and can also be used with  information . Each  DifferentialInfoEstimator s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example,  Kraskov  estimates the  Shannon  entropy. Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential KozachenkoLeonenko Nearest neighbors StateSpaceSet ✓ x x x x x Kraskov Nearest neighbors StateSpaceSet ✓ x x x x x Zhu Nearest neighbors StateSpaceSet ✓ x x x x x ZhuSingh Nearest neighbors StateSpaceSet ✓ x x x x x Gao Nearest neighbors StateSpaceSet ✓ x x x x x Goria Nearest neighbors StateSpaceSet ✓ x x x x x Lord Nearest neighbors StateSpaceSet ✓ x x x x x Vasicek Order statistics Vector ✓ x x x x x Ebrahimi Order statistics Vector ✓ x x x x x Correa Order statistics Vector ✓ x x x x x AlizadehArghami Order statistics Vector ✓ x x x x x"},{"id":676,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Counting the number of measures in ComplexityMeasures.jl","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#total_measures","content":" Counting the number of measures in ComplexityMeasures.jl In this page we will count all the possible complexity measures than one can compute with the current version of ComplexityMeasures.jl! using ComplexityMeasures\nusing InteractiveUtils: subtypes\nimport Pkg; Pkg.status(\"ComplexityMeasures\") Status `~/work/ComplexityMeasures.jl/ComplexityMeasures.jl/docs/Project.toml`\n  [ab4b797d] ComplexityMeasures v3.6.1 `~/work/ComplexityMeasures.jl/ComplexityMeasures.jl` First let's define a function that counts concrete subtypes that we will be re-using to count measures in ComplexityMeasures.jl. concrete_subtypes(type::Type) = concrete_subtypes!(Any[], type)\nfunction concrete_subtypes!(out, type::Type)\n    if !isabstracttype(type)\n        push!(out, type)\n    else\n        foreach(T -> concrete_subtypes!(out, T), subtypes(type))\n    end\n    out\nend concrete_subtypes! (generic function with 1 method)"},{"id":677,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Count Based Outcome Spaces","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Count-Based-Outcome-Spaces","content":" Count Based Outcome Spaces Each  OutcomeSpace  is a possible way of discretizing the input data. For the purpose of counting measures,  we treat the an outcome space with different input parameters as the same outcome space overall. Some outcome spaces are count-based. We estimate these separately, because they may be estimated with various different probabilities estimators. For our counting here it doesn't matter whether the outcome space supports spatiotemporal or trajectory (uni/multi variate) date. We only care if it is counting based or not. OUTCOME_SPACES_COUNT = concrete_subtypes(ComplexityMeasures.CountBasedOutcomeSpace) 12-element Vector{Any}:\n BubbleSortSwaps\n AmplitudeAwareOrdinalPatterns\n OrdinalPatterns\n WeightedOrdinalPatterns\n SpatialBubbleSortSwaps\n SpatialDispersion\n SpatialOrdinalPatterns\n CosineSimilarityBinning\n Dispersion\n SequentialPairDistances\n UniqueElements\n ValueBinning We do a small correction here because two outcome spaces aren't count-based but for internal convenience they satisfy the subtyping relationship correction_ospaces = (AmplitudeAwareOrdinalPatterns, WeightedOrdinalPatterns)\nforeach(\n    T -> deleteat!(OUTCOME_SPACES_COUNT, findfirst(isequal(T), OUTCOME_SPACES_COUNT)),\n    correction_ospaces\n)\n\nOUTCOME_SPACES_COUNT 10-element Vector{Any}:\n BubbleSortSwaps\n OrdinalPatterns\n SpatialBubbleSortSwaps\n SpatialDispersion\n SpatialOrdinalPatterns\n CosineSimilarityBinning\n Dispersion\n SequentialPairDistances\n UniqueElements\n ValueBinning Probabilities can be estimated from count-based outcome spaces in different ways. We count the same  ProbabilitiesEstimators  with different input parameters as the same estimator. Each probabilities estimator can be combined with any outcome space. PROBESTS_COUNT = concrete_subtypes(ProbabilitiesEstimator) 4-element Vector{Any}:\n AddConstant\n BayesianRegularization\n RelativeAmount\n Shrinkage and we count the combinations n_outcome_spaces_count = length(OUTCOME_SPACES_COUNT)\nn_probests_count = length(PROBESTS_COUNT)\nn_probs_count = n_outcome_spaces_count * n_probests_count 40"},{"id":678,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Non-count-based outcome spaces","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Non-count-based-outcome-spaces","content":" Non-count-based outcome spaces We also provide some outcome spaces that are not count-based, but can still be used to estimate discrete probabilities by using some sort of \"relative amount\" estimation. OUTCOME_SPACES_NOCOUNT = setdiff(\n    concrete_subtypes(ComplexityMeasures.OutcomeSpace),\n    concrete_subtypes(ComplexityMeasures.CountBasedOutcomeSpace),\n) 4-element Vector{Any}:\n NaiveKernel\n PowerSpectrum\n TransferOperator\n WaveletOverlap to which we add back the outcome spaces correction push!(OUTCOME_SPACES_NOCOUNT, correction_ospaces...)\nOUTCOME_SPACES_NOCOUNT 6-element Vector{Any}:\n NaiveKernel\n PowerSpectrum\n TransferOperator\n WaveletOverlap\n AmplitudeAwareOrdinalPatterns\n WeightedOrdinalPatterns Only  RelativeAmount  probabilities estimator works with non-count-based outcome spaces n_probs_noncount = length(OUTCOME_SPACES_NOCOUNT) * 1 6"},{"id":679,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Grand total of extracting PMFs from data","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Grand-total-of-extracting-PMFs-from-data","content":" Grand total of extracting PMFs from data Therefore the total ways to estimate discrete probabilities from data in ComplexityMeasures.jl is just n_probs_discrete = n_probs_noncount + n_probs_count 46"},{"id":680,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Discrete Information measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Discrete-Information-measures","content":" Discrete Information measures Currently, the InformationMeasures implemented are different types of entropies and the lesser-known extropies. Each of these measures, in their discrete form, are functions of probability mass functions (PMFs). Therefore, we can combine each of these measure with probabilities estimated using any count-based outcome space and any probabilities estimator. Let's collect all of these discrete measures INFO_MEASURES_DISCRETE = concrete_subtypes(InformationMeasure) 12-element Vector{Any}:\n Curado\n Identification\n Kaniadakis\n Renyi\n Shannon\n StretchedExponential\n Tsallis\n ElectronicEntropy\n FluctuationComplexity\n RenyiExtropy\n ShannonExtropy\n TsallisExtropy Each information measure can be estimated using any of the generic estimators: INFO_MEASURE_ESTIMATOR_GENERIC = concrete_subtypes(ComplexityMeasures.DiscreteInfoEstimatorGeneric) 2-element Vector{Any}:\n Jackknife\n PlugIn so we count by multiplying n_discrete_infoest_generic = length(INFO_MEASURES_DISCRETE)*length(INFO_MEASURE_ESTIMATOR_GENERIC) 24 In addition to the generic estimators, we also provide additional estimators specific to Shannon entropy. INFO_MEASURE_ESTIMATOR_SHANNON = concrete_subtypes(ComplexityMeasures.DiscreteInfoEstimatorShannon) 5-element Vector{Any}:\n ChaoShen\n GeneralizedSchuermann\n HorvitzThompson\n MillerMadow\n Schuermann For these there is no variability of the information measure so n_discrete_estimators_shannon = length(INFO_MEASURE_ESTIMATOR_SHANNON) 5 This gives us the total possible ways of estimating information measures given a PMF: n_discrete_info_est = n_discrete_estimators_shannon + n_discrete_infoest_generic 29"},{"id":681,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Grand total of discrete information measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Grand-total-of-discrete-information-measures","content":" Grand total of discrete information measures This total is obtained as the direct multiplication of all ways to obtain a PMF and all ways to compute an information measure from PMF n_discrete_info = n_discrete_info_est * n_probs_discrete 1334 That's quite a lot and we are only half-way done!"},{"id":682,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Differential information measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Differential-information-measures","content":" Differential information measures The differential information measures and their estimators are all grouped into one level of abstraction as long as the user is concerned, so counting things here is very simple! DIFF_INFO_EST = concrete_subtypes(DifferentialInfoEstimator) 12-element Vector{Any}:\n AlizadehArghami\n Gao\n Goria\n KozachenkoLeonenko\n Kraskov\n Lord\n Zhu\n ZhuSingh\n Correa\n Ebrahimi\n LeonenkoProzantoSavani\n Vasicek All of these estimate one quantity (the differential Shannon entropy), with the exception of one particular estimator ( LeonenkoProzantoSavani ) that can estimate also Tsallis and Renyi entropies. Therefore, the number of differential measures one can estimate within ComplexityMeasures.jl is: n_diff_info = length(DIFF_INFO_EST) + 2 14"},{"id":683,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Complexity measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Complexity-measures","content":" Complexity measures We also provide a number of estimators that are not probability based, which we call just complexity estimators for this discussion. We count each of these as a separate measure. COMPLEXITY_ESTIMATORS = concrete_subtypes(ComplexityEstimator) 7-element Vector{Any}:\n ApproximateEntropy\n BubbleEntropy\n LempelZiv76\n MissingDispersionPatterns\n ReverseDispersion\n SampleEntropy\n StatisticalComplexity However, from these we need to treat  StatisticalComplexity  separately, so we have n_complexity_measures_basic = length(COMPLEXITY_ESTIMATORS) - 1 6 In ComplexityMeasures.jl  StatisticalComplexity  can be combined with any discrete information measure, any information estimator, and any count-based outcome space. Additionally,  StatisticalComplexity  in ComplexityMeasures.jl can be combined with any metric from the Distances.jl package. For  StatisticalComplexity , counting all possible combinations of outcome spaces, probabilities estimators, information measure definitions, information measure estimators, along with distance measures, as unique measures would over-inflate the measure count. For practicality, we here count different version of  StatisticalComplexity  by considering the number of statistical complexity measures resulting from counting unique outcome spaces and information measures, since these are the largest contributors to changes in the computed numerical value of the measure. Therefore we have: n_complexity_measures_statistical_complexity = length(INFO_MEASURES_DISCRETE) * length(concrete_subtypes(OutcomeSpace)) 192 which gives us the following total number of complexity estimators n_complexity_measures_total = n_complexity_measures_basic + n_complexity_measures_statistical_complexity 198"},{"id":684,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Probabilities functions","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Probabilities-functions","content":" Probabilities functions Besides calculating complexity measures, ComplexityMeasures.jl gives the user the unique possibility of accessing the probability mass function directly. As we show in the associated article, this allows rather straightforwardly defining new, or expanding existing, complexity measures. For example, the  MissingDispersionPatterns  is just a wrapper of the  missing_outcomes  function. Therefore, we believe it is fair to count a couple of probabilities functions by themselves as additional complexity measures. In particular, we count here the functions  probabilities, allproabilities  as candidates for it, as all other functions of the library are simple processing of these two. Given that each function can work with any type of outcome space and probability estimation technique, we obtain n_extra_prob_measures = 2 * n_probs_discrete 92"},{"id":685,"pagetitle":"Counting the number of measures in ComplexityMeasures.jl","title":"Grand total of measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/measure_count/#Grand-total-of-measures","content":" Grand total of measures Right, so the grand total of all measures that can be estimated with ComplexityMeasures.jl are: n_grand_total =\n  n_discrete_info +\n  n_diff_info +\n  n_complexity_measures_total +\n  n_extra_prob_measures 1638"},{"id":688,"pagetitle":"Multiscale","title":"Multiscale","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#Multiscale","content":" Multiscale"},{"id":689,"pagetitle":"Multiscale","title":"Introduction","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#Introduction","content":" Introduction Multiscale complexity analysis is pervasive in the nonlinear time series analysis literature. Although their names, like \"refined composite multiscale dispersion entropy\",   might seem daunting, they're actually conceptually very simple. A multiscale complexity measure is just any regular complexity measure computed on several gradually more coarse-grained samplings of the input data ( example ). We've generalized this type of analysis to work with complexity measure that can  be estimated with ComplexityMeasures.jl."},{"id":690,"pagetitle":"Multiscale","title":"Multiscale API","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#Multiscale-API","content":" Multiscale API The multiscale API is defined by the functions multiscale multiscale_normalized downsample which dispatch any of the  MultiScaleAlgorithm s listed below."},{"id":691,"pagetitle":"Multiscale","title":"ComplexityMeasures.multiscale","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#ComplexityMeasures.multiscale","content":" ComplexityMeasures.multiscale  —  Function multiscale(algorithm::MultiScaleAlgorithm, [args...], x) A convenience function to compute the multiscale version of any  InformationMeasureEstimator  or  ComplexityEstimator The return type of  multiscale  is either a  Vector{Real}  or a  Vector{Vector{Real}} , see the available coarse-graining methods below. It utilizes  downsample  with the given  algorithm  to first produce coarse-grained, downsampled versions of  x  for scale factors  algorithm.scales . Then,  information  or  complexity , depending on the input arguments, is applied to each of the coarse-grained timeseries. If  N = length(x) , then the length of the most severely downsampled version of  x  is  N ÷ maximum(algorithm.scales) , while for scale factor  1 , the original time series is considered. Description This function generalizes the multiscale entropy of ( Costa  et al. , 2002 ) to any discrete information measure, any differential information measure, and any other complexity measure. Coarse-graining algorithms The available downsampling routines are: RegularDownsampling  yields a single  Vector  per scale. CompositeDownsampling  yields a  Vector{Vector}  per scale. Examples multiscale  can be used with any discrete or differential information measure estimator. For example, here's two ways of computing multiscale Tsallis entropy: using ComplexityMeasures\nx = randn(1000)\ndownsampling = RegularDownsampling(scales = 1:5) # multiscale algorithm\n\n# Symbolic (ordinal-pattern-based) probabilities estimation using Bayesian regularization,\n# jackknife estimation of the entropy.\no = OrdinalPatterns{3}(2) # outcome space\nprobest = BayesianRegularization() # probabilities estimator\nhest = Jackknife(Tsallis(q = 1.5)) # entropy estimator\nmultiscale(downsampling, hest, probest, o, x)\n\n# Differential kNN-based estimator:\nhest = LeonenkoProzantoSavani(Tsallis(q = 1.5), k = 10) # 10 neighbors\nmultiscale(downsampling, hest, x) Multiscale variants of any  ComplexityEstimator  are also trivial to compute. Let's compute the \"generalized multiscale sample entropy ( Costa and Goldberger, 2015 )\" using the second-order moment. using ComplexityMeasures, Statistics\nmultiscale(CompositeDownsampling(; f = Statistics.var), SampleEntropy(x), x) source"},{"id":692,"pagetitle":"Multiscale","title":"ComplexityMeasures.multiscale_normalized","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#ComplexityMeasures.multiscale_normalized","content":" ComplexityMeasures.multiscale_normalized  —  Function multiscale_normalized(algorithm::MultiScaleAlgorithm, [args...], x) The same as  multiscale , but computes the normalized version of the complexity measure. source"},{"id":693,"pagetitle":"Multiscale","title":"ComplexityMeasures.MultiScaleAlgorithm","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#ComplexityMeasures.MultiScaleAlgorithm","content":" ComplexityMeasures.MultiScaleAlgorithm  —  Type MultiScaleAlgorithm The supertype for all multiscale coarse-graining/downsampling algorithms. Concrete subtypes are: RegularDownsampling CompositeDownsampling source"},{"id":694,"pagetitle":"Multiscale","title":"ComplexityMeasures.RegularDownsampling","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#ComplexityMeasures.RegularDownsampling","content":" ComplexityMeasures.RegularDownsampling  —  Type RegularDownsampling <: MultiScaleAlgorithm\nRegularDownsampling(; f::Function = Statistics.mean, scales = 1:8) The original multi-scale algorithm for multiscale entropy analysis ( Costa  et al. , 2002 ), which yields a single downsampled time series per scale  s . Description Given a scalar-valued input time series  x , the  Regular  multiscale algorithm downsamples and coarse-grains  x  by splitting it into non-overlapping windows of length  s , and then constructing a new downsampled time series  $D_t(s, f)$  by applying the function  f  to each of the resulting length- s  windows. The downsampled time series  D_t(s)  with  t ∈ [1, 2, …, L] , where  L = floor(N / s) , is given by: \\[\\{ D_t(s, f)  \\}_{t = 1}^{L} = \\left\\{ f \\left( \\bf x_t \\right) \\right\\}_{t = 1}^{L} =\n\\left\\{\n    {f\\left( (x_i)_{i = (t - 1)s + 1}^{ts} \\right)}\n\\right\\}_{t = 1}^{L}\\] where  f  is some summary statistic applied to the length- ts-((t - 1)s + 1)  tuples  xₖ . Different choices of  f  have yield different multiscale methods appearing in the literature. For example: f == Statistics.mean  yields the original first-moment multiscale sample entropy   ( Costa  et al. , 2002 ). f == Statistics.var  yields the generalized multiscale sample entropy ( Costa and Goldberger, 2015 ),   which uses the second-moment (variance) instead of the mean. Keyword Arguments scales . The downsampling levels. If  scales  is set to an integer, then this integer   is taken as maximum number of scales (i.e. levels of downsampling), and downsampling   is done over levels  1:scales . Otherwise, downsampling is done over the provided    scales  (which may be a range, or some specific scales (e.g.  scales = [1, 5, 6] ).   The maximum scale level is  length(x) ÷ 2 , but to avoid applying the method to time   series that are extremely short, consider limiting the maximum scale  (e.g.    scales = length(x) ÷ 5 ). See also:  CompositeDownsampling . source"},{"id":695,"pagetitle":"Multiscale","title":"ComplexityMeasures.CompositeDownsampling","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#ComplexityMeasures.CompositeDownsampling","content":" ComplexityMeasures.CompositeDownsampling  —  Type CompositeDownsampling <: MultiScaleAlgorithm\nCompositeDownsampling(; f::Function = Statistics.mean, scales = 1:8) Composite multi-scale algorithm for multiscale entropy analysis ( Wu  et al. , 2013 ), used with  multiscale  to compute, for example, composite multiscale entropy (CMSE). Description Given a scalar-valued input time series  x , the composite multiscale algorithm, like  RegularDownsampling , downsamples and coarse-grains  x  by splitting it into non-overlapping windows of length  s , and then constructing downsampled time series by applying the function  f  to each of the resulting length- s  windows. However,  Wu  et al.  (2013)  realized that for each scale  s , there are actually  s  different ways of selecting windows, depending on where indexing starts/ends. These  s  different downsampled time series  D_t(s, f)  at each scale  s  are constructed as follows: \\[\\{ D_{k}(s) \\} = \\{ D_{t, k}(s) \\}_{t = 1}^{L}, = \\{ f \\left( \\bf x_{t, k} \\right) \\} =\n\\left\\{\n    {f\\left( (x_i)_{i = (t - 1)s + k}^{ts + k - 1} \\right)}\n\\right\\}_{t = 1}^{L},\\] where  L = floor((N - s + 1) / s)  and  1 ≤ k ≤ s , such that  $D_{i, k}(s)$  is the  i -th element of the  k -th downsampled time series at scale  s . Finally, compute  $\\dfrac{1}{s} \\sum_{k = 1}^s g(D_{k}(s))$ , where  g  is some summary function, for example  information  or  complexity . Keyword Arguments scales . The downsampling levels. If  scales  is set to an integer, then this integer   is taken as maximum number of scales (i.e. levels of downsampling), and downsampling   is done over levels  1:scales . Otherwise, downsampling is done over the provided    scales  (which may be a range, or some specific scales (e.g.  scales = [1, 5, 6] ).   The maximum scale level is  length(x) ÷ 2 , but to avoid applying the method to time   series that are extremely short, consider limiting the maximum scale  (e.g.    scales = length(x) ÷ 5 ). Relation to RegularDownsampling The downsampled time series  $D_{t, 1}(s)$  constructed using the composite multiscale method is equivalent to the downsampled time series  $D_{t}(s)$  constructed using the  RegularDownsampling  method, for which  k == 1  is fixed, such that only a single time series is returned. See also:  RegularDownsampling . source"},{"id":696,"pagetitle":"Multiscale","title":"ComplexityMeasures.downsample","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#ComplexityMeasures.downsample","content":" ComplexityMeasures.downsample  —  Function downsample(algorithm::MultiScaleAlgorithm, s::Int, x) Downsample and coarse-grain  x  to scale  s  according to the given  MultiScaleAlgorithm . The return type depends on  algorithm . source"},{"id":697,"pagetitle":"Multiscale","title":"Example literature methods","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/multiscale/#Example-literature-methods","content":" Example literature methods A non-exhaustive list of literature methods, and the syntax to compute them, are listed below. Please open an issue or make a pull-request to  ComplexityMeasures.jl  if you find a literature method missing from this list, or if you publish a paper based on some new multiscale combination. Method Syntax example Reference Refined composite multiscale dispersion entropy multiscale(CompositeDownsampling(), Dispersion(), est, x, normalized = true) ( Azami  et al. , 2017 ) Multiscale sample entropy (first moment) multiscale(RegularDownsampling(f = mean), SampleEntropy(x), x) ( Costa  et al. , 2002 ) Generalized multiscale sample entropy (second moment) multiscale(RegularDownsampling(f = std), SampleEntropy(x),  x) ( Costa and Goldberger, 2015 )"},{"id":700,"pagetitle":"Probabilities","title":"Probabilities","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Probabilities","content":" Probabilities Note Be sure you have gone through the  Tutorial  before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl. ComplexityMeasures.jl implements an interface for probabilities that exactly follows the mathematically rigorous formulation of  probability spaces . Probability spaces are formalized by an  OutcomeSpace $\\Omega$ . Probabilities are extracted from data then by referencing an outcome space in the functions  counts  and  probabilities . The mathematical formulation of probabilities spaces is further enhanced by  ProbabilitiesEstimator  and its subtypes, which may correct theoretically known biases when estimating probabilities from finite data. In reality, probabilities can be either discrete ( mass functions ) or continuous ( density functions ). Currently in ComplexityMeasures.jl, only probability mass functions (i.e., countable  $\\Omega$ ) are implemented explicitly. Quantities that are estimated from probability density functions (i.e., uncountable  $\\Omega$ ) also exist and are implemented in ComplexityMeasures.jl. However, these are estimated by a one-step processes without the intermediate estimation of probabilities. If  $\\Omega$  is countable, the process of estimating the outcomes from input data is also called  discretization  of the input data."},{"id":701,"pagetitle":"Probabilities","title":"Outcome spaces","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#outcome_spaces","content":" Outcome spaces"},{"id":702,"pagetitle":"Probabilities","title":"ComplexityMeasures.OutcomeSpace","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.OutcomeSpace","content":" ComplexityMeasures.OutcomeSpace  —  Type OutcomeSpace The supertype for all outcome space implementation. Description In ComplexityMeasures.jl, an outcome space defines a set of possible outcomes  $\\Omega = \\{\\omega_1, \\omega_2, \\ldots, \\omega_L \\}$  (some form of discretization). In the literature, the outcome space is often also called an \"alphabet\", while each outcome is called a \"symbol\" or an \"event\". An outcome space also defines a set of rules for mapping input data to to each outcome  $\\omega_i$ , a processes called  encoding  or  symbolizing  or  discretizing  in the literature (see  encodings ). Some  OutcomeSpace s first apply a transformation, e.g. a delay embedding, to the data before discretizing/encoding, while other  OutcomeSpace s discretize/encode the data directly. Implementations Outcome space Principle Input data Counting-compatible UniqueElements Count of unique elements Any ✔ ValueBinning Binning (histogram) Vector ,  StateSpaceSet ✔ OrdinalPatterns Ordinal patterns Vector ,  StateSpaceSet ✔ SpatialOrdinalPatterns Ordinal patterns in space Array ✔ Dispersion Dispersion patterns Vector ✔ SpatialDispersion Dispersion patterns in space Array ✔ CosineSimilarityBinning Cosine similarity Vector ✔ BubbleSortSwaps Swap counts when sorting Vector ✔ SequentialPairDistances Sequential state vector distances Vector ,  StateSpaceSet ✔ TransferOperator Binning (transfer operator) Vector ,  StateSpaceSet ✖ NaiveKernel Kernel density estimation StateSpaceSet ✖ WeightedOrdinalPatterns Ordinal patterns Vector ,  StateSpaceSet ✖ AmplitudeAwareOrdinalPatterns Ordinal patterns Vector ,  StateSpaceSet ✖ WaveletOverlap Wavelet transform Vector ✖ PowerSpectrum Fourier transform Vector ✖ In the column \"input data\" it is assumed that the  eltype  of the input is  <: Real . Usage Outcome spaces are used as input to probabilities / allprobabilities_and_outcomes  for computing   probability mass functions. outcome_space , which returns the elements of the outcome space. total_outcomes , which returns the cardinality of the outcome space. counts / counts_and_outcomes / allcounts_and_outcomes , for    obtaining raw counts instead of probabilities (only for counting-compatible outcome   spaces). Counting-compatible vs. non-counting compatible outcome spaces There are two main types of outcome spaces. Counting-compatible outcome spaces have a well-defined   way of counting how often each point in the (encoded) input data is mapped to a   particular outcome  $\\omega_i$ . These outcome spaces use    encode  to discretize the input data. Examples are    OrdinalPatterns  (which encodes input data into ordinal patterns) or    ValueBinning  (which discretizes points onto a regular grid).   The table below lists which outcome spaces are counting compatible. Non-counting compatible outcome spaces have no well-defined way of counting explicitly   how often each point in the input data is mapped to a particular outcome  $\\omega_i$ .   Instead, these outcome spaces returns a vector of pre-normalized \"relative counts\", one   for each outcome  $\\omega_i$ . Examples are  WaveletOverlap  or    PowerSpectrum . Counting-compatible outcome spaces can be used with  any ProbabilitiesEstimator  to convert counts into probability mass functions. Non-counting-compatible outcome spaces can only be used with the maximum likelihood ( RelativeAmount ) probabilities estimator, which estimates probabilities precisely by the relative frequency of each outcome (formally speaking, the  RelativeAmount  estimator also requires counts, but for the sake of code consistency, we allow it to be used with relative frequencies as well). The function  is_counting_based  can be used to check whether an outcome space is based on counting. Deducing the outcome space (from data) Some outcome space models can deduce  $\\Omega$  without knowledge of the input, such as  OrdinalPatterns . Other outcome spaces require knowledge of the input data for concretely specifying  $\\Omega$ , such as  ValueBinning  with  RectangularBinning . If  o  is some outcome space model and  x  some input data, then  outcome_space (o, x)  returns the possible outcomes  $\\Omega$ . To get the cardinality of  $\\Omega$ , use  total_outcomes . Implementation details The element type of  $\\Omega$  varies between outcome space models, but it is guaranteed to be  hashable  and  sortable . This allows for conveniently tracking the counts of a specific event across experimental realizations, by using the outcome as a dictionary key and the counts as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization). source"},{"id":703,"pagetitle":"Probabilities","title":"ComplexityMeasures.outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcomes","content":" ComplexityMeasures.outcomes  —  Function outcomes(o::OutcomeSpace, x) Return all (unique) outcomes that appear in the (encoded) input data  x , according to the given  OutcomeSpace . Equivalent to  probabilities_and_outcomes(o, x)[2] , but for some estimators it may be explicitly extended for better performance. source"},{"id":704,"pagetitle":"Probabilities","title":"ComplexityMeasures.outcome_space","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcome_space","content":" ComplexityMeasures.outcome_space  —  Function outcome_space(o::OutcomeSpace, x) → Ω Return a sorted container containing all  possible  outcomes of  o  for input  x . For some estimators the concrete outcome space is known without knowledge of input  x , in which case the function dispatches to  outcome_space(o) . In general it is recommended to use the 2-argument version irrespectively of estimator. source"},{"id":705,"pagetitle":"Probabilities","title":"ComplexityMeasures.total_outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.total_outcomes","content":" ComplexityMeasures.total_outcomes  —  Function total_outcomes(o::OutcomeSpace, x) Return the length (cardinality) of the outcome space  $\\Omega$  of  est . For some  OutcomeSpace , the cardinality is known without knowledge of input  x , in which case the function dispatches to  total_outcomes(est) . In general it is recommended to use the 2-argument version irrespectively of estimator. source"},{"id":706,"pagetitle":"Probabilities","title":"ComplexityMeasures.missing_outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.missing_outcomes","content":" ComplexityMeasures.missing_outcomes  —  Function missing_outcomes(o::OutcomeSpace, x; all = false) → n::Int Count the number of missing outcomes  n  (i.e., not occuring in the data) specified by  o , given input data  x . This function only works for count-based outcome spaces, use  missing_probabilities  otherwise. See also:  MissingDispersionPatterns . source"},{"id":707,"pagetitle":"Probabilities","title":"Count occurrences","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Count-occurrences","content":" Count occurrences"},{"id":708,"pagetitle":"Probabilities","title":"ComplexityMeasures.UniqueElements","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.UniqueElements","content":" ComplexityMeasures.UniqueElements  —  Type UniqueElements() An  OutcomeSpace  based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to  probabilities . Outcome space The outcome space is the unique sorted values of the input. Hence, input  x  is needed for a well-defined  outcome_space . Implements codify . Used for encoding inputs where ordering matters (e.g. time series). source"},{"id":709,"pagetitle":"Probabilities","title":"Histograms","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Histograms","content":" Histograms"},{"id":710,"pagetitle":"Probabilities","title":"ComplexityMeasures.ValueBinning","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ValueBinning","content":" ComplexityMeasures.ValueBinning  —  Type ValueBinning(b::AbstractBinning) <: OutcomeSpace An  OutcomeSpace  based on binning the values of the data as dictated by the binning scheme  b  and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is  VisitationFrequency . Available binnings are subtypes of  AbstractBinning . The  ValueBinning  estimator has a linearithmic time complexity ( n log(n)  for  n = length(x) ) and a linear space complexity ( l  for  l = dimension(x) ). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes  ε  without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered. ValueBinning(ϵ::Union{Real,Vector}) A convenience method that accepts same input as  RectangularBinning  and initializes this binning directly. Outcomes The outcome space for  ValueBinning  is the unique bins constructed from  b . Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals  [a, b) . The bins are in data units, not integer (cartesian indices units), and are returned as  SVector s, i.e., same type as input data. For convenience,  outcome_space  returns the outcomes in the same array format as the underlying binning (e.g.,  Matrix  for 2D input). For  FixedRectangularBinning  the  outcome_space  is well-defined from the binning, but for  RectangularBinning  input  x  is needed as well. Implements codify . Used for encoding inputs where ordering matters (e.g. time series). source"},{"id":711,"pagetitle":"Probabilities","title":"ComplexityMeasures.AbstractBinning","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.AbstractBinning","content":" ComplexityMeasures.AbstractBinning  —  Type AbstractBinning Supertype encompassing  RectangularBinning  and  FixedRectangularBinning . source"},{"id":712,"pagetitle":"Probabilities","title":"ComplexityMeasures.RectangularBinning","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RectangularBinning","content":" ComplexityMeasures.RectangularBinning  —  Type RectangularBinning(ϵ, precise = false) <: AbstractBinning Rectangular box partition of state space using the scheme  ϵ , deducing the histogram extent and bin width from the input data. RectangularBinning  is a convenience struct. It is re-cast into  FixedRectangularBinning  once the data are provided, so see that docstring for info on the bin calculation and the meaning of  precise . Binning instructions are deduced from the type of  ϵ  as follows: ϵ::Int  divides each coordinate axis into  ϵ  equal-length intervals  that cover all data. ϵ::Float64  divides each coordinate axis into intervals of fixed size  ϵ , starting  from the axis minima until the data is completely covered by boxes. ϵ::Vector{Int}  divides the i-th coordinate axis into  ϵ[i]  equal-length  intervals that cover all data. ϵ::Vector{Float64}  divides the i-th coordinate axis into intervals of fixed size   ϵ[i] , starting from the axis minima until the data is completely covered by boxes. RectangularBinning  ensures all input data are covered by extending the created ranges if need be. source"},{"id":713,"pagetitle":"Probabilities","title":"ComplexityMeasures.FixedRectangularBinning","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.FixedRectangularBinning","content":" ComplexityMeasures.FixedRectangularBinning  —  Type FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false) Rectangular box partition of state space where the partition along each dimension is explicitly given by each range  ranges , which is a tuple of  AbstractRange  subtypes. Typically, each range is the output of the  range  Base function, e.g.,  ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)) . All ranges must be sorted. The optional second argument  precise  dictates whether Julia Base's  TwicePrecision  is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is  false . Points falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open:  [a, b) .  This means that the last value of each of the ranges dictates the last right-closing value.  This value does  not  belong to the histogram! E.g., if given a range  r = range(0, 1; length = 11) , with  r[end] = 1 , the value  1  is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here  [0.9, 1) )! Equivalently, the size of the histogram is  histsize = map(r -> length(r)-1, ranges) ! FixedRectangularBinning  leads to a well-defined outcome space without knowledge of input data, see  ValueBinning . source"},{"id":714,"pagetitle":"Probabilities","title":"Symbolic permutations","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Symbolic-permutations","content":" Symbolic permutations"},{"id":715,"pagetitle":"Probabilities","title":"ComplexityMeasures.OrdinalPatterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.OrdinalPatterns","content":" ComplexityMeasures.OrdinalPatterns  —  Type OrdinalPatterns <: OutcomeSpace\nOrdinalPatterns{m}(τ = 1, lt::Function = ComplexityMeasures.isless_rand) An  OutcomeSpace  based on lengh- m  ordinal permutation patterns, originally introduced in  Bandt and Pompe (2002) 's paper on permutation entropy. Note that  m  is given as a type parameter, so that when it is a literal integer there are performance accelerations. When passed to  probabilities  the output depends on the input data type: Univariate data . If applied to a univariate timeseries ( AbstractVector ), then the timeseries   is first embedded using embedding delay  τ  and dimension  m , resulting in embedding   vectors  $\\{ \\bf{x}_i \\}_{i=1}^{N-(m-1)\\tau}$ . Then, for each  $\\bf{x}_i$ ,   we find its permutation pattern  $\\pi_{i}$ . Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using  UniqueElements . When giving the resulting probabilities to    information , the original permutation entropy is computed ( Bandt and Pompe, 2002 ). Multivariate data . If applied to a an  D -dimensional  StateSpaceSet ,   then no embedding is constructed,  m  must be equal to  D  and  τ  is ignored.   Each vector  $\\bf{x}_i$  of the dataset is mapped   directly to its permutation pattern  $\\pi_{i}$  by comparing the   relative magnitudes of the elements of  $\\bf{x}_i$ .   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy ( He  et al. , 2016 ), although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of  He  et al.  (2016) ). Internally,  OrdinalPatterns  uses the  OrdinalPatternEncoding  to represent ordinal patterns as integers for efficient computations. See  WeightedOrdinalPatterns  and  AmplitudeAwareOrdinalPatterns  for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see  SpatialOrdinalPatterns . Handling equal values in ordinal patterns In  Bandt and Pompe (2002) , equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution ( Zunino  et al. , 2017 ). Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using  lt = ComplexityMeasures.isless_rand . To get the behaviour from  Bandt and Pompe (2002) , use  lt = Base.isless . Outcome space The outcome space  Ω  for  OrdinalPatterns  is the set of length- m  ordinal patterns (i.e. permutations) that can be formed by the integers  1, 2, …, m . There are  factorial(m)  such patterns. For example, the outcome  [2, 3, 1]  corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also  OrdinalPatternEncoding . In-place symbolization OrdinalPatterns  also implements the in-place  probabilities!  for  StateSpaceSet  input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example using ComplexityMeasures\nm, N = 2, 100\nest = OrdinalPatterns{m}(τ)\nx = StateSpaceSet(rand(N, m)) # some input dataset\nπs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(πs_ts, est, x) source"},{"id":716,"pagetitle":"Probabilities","title":"ComplexityMeasures.WeightedOrdinalPatterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.WeightedOrdinalPatterns","content":" ComplexityMeasures.WeightedOrdinalPatterns  —  Type WeightedOrdinalPatterns <: OutcomeSpace\nWeightedOrdinalPatterns{m}(τ = 1, lt::Function = ComplexityMeasures.isless_rand) A variant of  OrdinalPatterns  that also incorporates amplitude information, based on the weighted permutation entropy ( Fadlallah  et al. , 2013 ). The outcome space and arguments are the same as in  OrdinalPatterns . Description For each ordinal pattern extracted from each state (or delay) vector, a weight is attached to it which is the variance of the vector. Probabilities are then estimated by summing the weights corresponding to the same pattern, instead of just counting the occurrence of the same pattern. An implementation note Note: in equation 7, section III, of the original paper, the authors write \\[w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j-(k-1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.\\] *But given the formula they give for the arithmetic mean, this is  not  the variance of the delay vector  $\\mathbf{x}_i$ , because the indices are mixed:  $x_{j+(k-1)\\tau}$  in the weights formula, vs.  $x_{j+(k+1)\\tau}$  in the arithmetic mean formula. Here, delay embedding and computation of the patterns and their weights are completely separated processes, ensuring that we compute the arithmetic mean correctly for each vector of the input dataset (which may be a delay-embedded timeseries). source"},{"id":717,"pagetitle":"Probabilities","title":"ComplexityMeasures.AmplitudeAwareOrdinalPatterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.AmplitudeAwareOrdinalPatterns","content":" ComplexityMeasures.AmplitudeAwareOrdinalPatterns  —  Type AmplitudeAwareOrdinalPatterns <: OutcomeSpace\nAmplitudeAwareOrdinalPatterns{m}(τ = 1, A = 0.5, lt = ComplexityMeasures.isless_rand) A variant of  OrdinalPatterns  that also incorporates amplitude information, based on the amplitude-aware permutation entropy ( Azami and Escudero, 2016 ). The outcome space and arguments are the same as in  OrdinalPatterns . Description Similarly to  WeightedOrdinalPatterns , a weight  $w_i$  is attached to each ordinal pattern extracted from each state (or delay) vector  $\\mathbf{x}_i = (x_1^i, x_2^i, \\ldots, x_m^i)$  as \\[w_i = \\dfrac{A}{m} \\sum_{k=1}^m |x_k^i | + \\dfrac{1-A}{d-1}\n\\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\\] with  $0 \\leq A \\leq 1$ . When  $A=0$  , only internal differences between the elements of  $\\mathbf{x}_i$  are weighted. Only mean amplitude of the state vector elements are weighted when  $A=1$ . With,  $0<A<1$ , a combined weighting is used. source"},{"id":718,"pagetitle":"Probabilities","title":"Dispersion patterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Dispersion-patterns","content":" Dispersion patterns"},{"id":719,"pagetitle":"Probabilities","title":"ComplexityMeasures.Dispersion","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Dispersion","content":" ComplexityMeasures.Dispersion  —  Type Dispersion(; c = 5, m = 2, τ = 1, check_unique = true) An  OutcomeSpace  based on dispersion patterns, originally used by  Rostaghi and Azami (2016)  to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series. Recommended parameter values ( Li  et al. , 2019 ) are  m ∈ [2, 3] ,  τ = 1  for the embedding, and  c ∈ [3, 4, …, 8]  categories for the Gaussian symbol mapping. Description Assume we have a univariate time series  $X = \\{x_i\\}_{i=1}^N$ . First, this time series is encoded into a symbol timeseries  $S$  using the Gaussian encoding  GaussianCDFEncoding  with empirical mean  μ  and empirical standard deviation  σ  (both determined from  $X$ ), and  c  as given to  Dispersion . Then,  $S$  is embedded into an  $m$ -dimensional time series, using an embedding lag of  $\\tau$ , which yields a total of  $N - (m - 1)\\tau$  delay vectors  $z_i$ , or \"dispersion patterns\". Since each element of  $z_i$  can take on  c  different values, and each delay vector has  m  entries, there are  c^m  possible dispersion patterns. This number is used for normalization when computing dispersion entropy. The returned probabilities are simply the frequencies of the unique dispersion patterns present in  $S$  (i.e., the  UniqueElements  of  $S$ ). Outcome space The outcome space for  Dispersion  is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of  $S$ . Data requirements and parameters The input must have more than one unique element for the Gaussian mapping to be well-defined.  Li  et al.  (2019)  recommends that  x  has at least 1000 data points. If  check_unique == true  (default), then it is checked that the input has more than one unique value. If  check_unique == false  and the input only has one unique element, then a  InexactError  is thrown when trying to compute probabilities. Why 'dispersion patterns'? Each embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when  $m = 5$  and  $c = 3$ , and use some very imprecise terminology for illustration: When  $c = 3$ , values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector  $[2, 2, 2, 2, 2]$  consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector  $[1, 1, 2, 3, 3]$ , however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean. For a version of this estimator that can be used on high-dimensional arrays, see  SpatialDispersion . Implements codify . Used for encoding inputs where ordering matters (e.g. time series). source"},{"id":720,"pagetitle":"Probabilities","title":"Transfer operator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Transfer-operator","content":" Transfer operator"},{"id":721,"pagetitle":"Probabilities","title":"ComplexityMeasures.TransferOperator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.TransferOperator","content":" ComplexityMeasures.TransferOperator  —  Type TransferOperator <: OutcomeSpace\nTransferOperator(b::AbstractBinning; warn_precise = true, rng = Random.default_rng()) An  OutcomeSpace  based on binning data into rectangular boxes dictated by the given binning scheme  b . When used with  probabilities , then the transfer (Perron-Frobenius) operator is approximated over the bins, then bin probabilities are estimated as the invariant measure associated with that transfer operator. Assumes that the input data are sequential (time-ordered). This implementation follows the grid estimator approach in  Diego  et al.  (2019) . Precision The default behaviour when using  RectangularBinning  or  FixedRectangularBinning  is to accept some loss of precision on the  bin boundaries for speed-ups, but this may lead to issues for  TransferOperator  where some points may be encoded as the symbol  -1  (\"outside the binning\"). The  warn_precise  keyword controls whether the user is warned when a less  precise binning is used. Outcome space The outcome space for  TransferOperator  is the set of unique bins constructed from  b . Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as  SVector s. Bin ordering Bins returned by  probabilities_and_outcomes  are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if b = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries then  probs[i]  is the invariant measure (probability) of the bin  outcomes[i] , which is the  i -th bin visited by the timeseries with nonzero measure. Description The transfer operator  $P^{N}$ is computed as an  N -by- N  matrix of transition probabilities between the states defined by the partition elements, where  N  is the number of boxes in the partition that is visited by the orbit/points. If   $\\{x_t^{(D)} \\}_{n=1}^L$  are the  $L$  different  $D$ -dimensional points over which the transfer operator is approximated,  $\\{ C_{k=1}^N \\}$  are the  $N$  different partition elements (as dictated by  ϵ ) that gets visited by the points, and   $\\phi(x_t) = x_{t+1}$ , then \\[P_{ij} = \\dfrac\n{\\#\\{ x_n | \\phi(x_n) \\in C_j \\cap x_n \\in C_i \\}}\n{\\#\\{ x_m | x_m \\in C_i \\}},\\] where  $\\#$  denotes the cardinal. The element  $P_{ij}$  thus indicates how many points that are initially in box  $C_i$  end up in box  $C_j$  when the points in  $C_i$  are projected one step forward in time. Thus, the row  $P_{ik}^N$  where  $k \\in \\{1, 2, \\ldots, N \\}$  gives the probability of jumping from the state defined by box  $C_i$  to any of the other  $N$  states. It follows that  $\\sum_{k=1}^{N} P_{ik} = 1$  for all  $i$ . Thus,  $P^N$  is a row/right stochastic matrix. Invariant measure estimation from transfer operator The left invariant distribution  $\\mathbf{\\rho}^N$  is a row vector, where  $\\mathbf{\\rho}^N P^{N} = \\mathbf{\\rho}^N$ . Hence,  $\\mathbf{\\rho}^N$  is a row eigenvector of the transfer matrix  $P^{N}$  associated with eigenvalue 1. The distribution  $\\mathbf{\\rho}^N$  approximates the invariant density of the system subject to  binning , and can be taken as a probability distribution over the partition elements. In practice, the invariant measure  $\\mathbf{\\rho}^N$  is computed using  invariantmeasure , which also approximates the transfer matrix. The invariant distribution is initialized as a length- N  random distribution which is then applied to  $P^{N}$ . For reproducibility in this step, set the  rng . The resulting length- N  distribution is then applied to  $P^{N}$  again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold. See also:  RectangularBinning ,  FixedRectangularBinning ,  invariantmeasure . source"},{"id":722,"pagetitle":"Probabilities","title":"Utility methods/types","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Utility-methods/types","content":" Utility methods/types"},{"id":723,"pagetitle":"Probabilities","title":"ComplexityMeasures.InvariantMeasure","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.InvariantMeasure","content":" ComplexityMeasures.InvariantMeasure  —  Type InvariantMeasure(to, ρ) Minimal return struct for  invariantmeasure  that contains the estimated invariant measure  ρ , as well as the transfer operator  to  from which it is computed (including bin information). See also:  invariantmeasure . source"},{"id":724,"pagetitle":"Probabilities","title":"ComplexityMeasures.invariantmeasure","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.invariantmeasure","content":" ComplexityMeasures.invariantmeasure  —  Function invariantmeasure(x::AbstractStateSpaceSet, binning::RectangularBinning;\n    rng = Random.default_rng()) → iv::InvariantMeasure Estimate an invariant measure over the points in  x  based on binning the data into rectangular boxes dictated by the  binning , then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential. Details on the estimation procedure is found the  TransferOperator  docstring. Example using DynamicalSystems\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\norbit, t = trajectory(ds, 20_000; Ttr = 10)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv) Probabilities and bin information invariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector}) From a pre-computed invariant measure, return the probabilities and associated bins. The element  ρ[i]  is the probability of visitation to the box  bins[i] . Transfer operator approach vs. naive histogram approach Why bother with the transfer operator instead of using regular histograms to obtain probabilities? In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as  $n \\to \\intfy$ ), which is guaranteed by the ergodic theorem. There is a crucial difference, however: The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the  transition probabilities  between states (see  transfermatrix ). See also:  InvariantMeasure . source"},{"id":725,"pagetitle":"Probabilities","title":"ComplexityMeasures.transfermatrix","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.transfermatrix","content":" ComplexityMeasures.transfermatrix  —  Function transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector}) Return the transfer matrix/operator and corresponding bins. Here,  bins[i]  corresponds to the i-th row/column of the transfer matrix. Thus, the entry  M[i, j]  is the probability of jumping from the state defined by  bins[i]  to the state defined by  bins[j] . See also:  TransferOperator . source"},{"id":726,"pagetitle":"Probabilities","title":"Kernel density","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Kernel-density","content":" Kernel density"},{"id":727,"pagetitle":"Probabilities","title":"ComplexityMeasures.NaiveKernel","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.NaiveKernel","content":" ComplexityMeasures.NaiveKernel  —  Type NaiveKernel(ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) <: OutcomeSpace An  OutcomeSpace  based on a \"naive\" kernel density estimation approach (KDE), as discussed in  Prichard and Theiler (1995) . Probabilities  $P(\\mathbf{x}, \\epsilon)$  are assigned to every point  $\\mathbf{x}$  by counting how many other points occupy the space spanned by a hypersphere of radius  ϵ  around  $\\mathbf{x}$ , according to: \\[P_i( X, \\epsilon) \\approx \\dfrac{1}{N} \\sum_{s} B(||X_i - X_j|| < \\epsilon),\\] where  $B$  gives 1 if the argument is  true . Probabilities are then normalized. Keyword arguments method = KDTree : the search structure supported by Neighborhood.jl. Specifically, use  KDTree  to use a tree-based neighbor search, or  BruteForce  for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length. w = 0 : the Theiler window, which excludes indices  $s$  that are within  $|i - s| ≤ w$  from the given point  $x_i$ . metric = Euclidean() : the distance metric. Outcome space The outcome space  Ω  for  NaiveKernel  are the indices of the input data,  eachindex(x) . Hence, input  x  is needed for a well-defined  outcome_space . The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors). source"},{"id":728,"pagetitle":"Probabilities","title":"Timescales","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Timescales","content":" Timescales"},{"id":729,"pagetitle":"Probabilities","title":"ComplexityMeasures.WaveletOverlap","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.WaveletOverlap","content":" ComplexityMeasures.WaveletOverlap  —  Type WaveletOverlap([wavelet]) <: OutcomeSpace An  OutcomeSpace  based on the maximal overlap discrete wavelet transform (MODWT). When used with  probabilities , the MODWT is applied to a signal, then probabilities are computed as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy according to  Rosso  et al.  (2001) . Input timeseries  x  is needed for a well-defined outcome space. By default the wavelet  Wavelets.WT.Daubechies{12}()  is used. Otherwise, you may choose a wavelet from the  Wavelets  package (it must subtype  OrthoWaveletClass ). Outcome space The outcome space for  WaveletOverlap  are the integers  1, 2, …, N  enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can  view online . As such, this estimator only works for timeseries input and input  x  is needed for a well-defined  outcome_space . source"},{"id":730,"pagetitle":"Probabilities","title":"ComplexityMeasures.PowerSpectrum","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.PowerSpectrum","content":" ComplexityMeasures.PowerSpectrum  —  Type PowerSpectrum() <: OutcomeSpace An  OutcomeSpace  based on the power spectrum of a timeseries (amplitude square of its Fourier transform). If used with  probabilities , then the spectrum normalized to sum = 1 is returned as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as  spectral entropy , e.g.  Llanos  et al.  (2017)  and  Tian  et al.  (2017) . The closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input. Outcome space The outcome space  Ω  for  PowerSpectrum  is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be  1 . Input  x  is needed for a well-defined  outcome_space . source"},{"id":731,"pagetitle":"Probabilities","title":"Cosine similarity binning","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Cosine-similarity-binning","content":" Cosine similarity binning"},{"id":732,"pagetitle":"Probabilities","title":"ComplexityMeasures.CosineSimilarityBinning","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.CosineSimilarityBinning","content":" ComplexityMeasures.CosineSimilarityBinning  —  Type CosineSimilarityBinning(; m::Int, τ::Int, nbins::Int) A  OutcomeSpace  based on the cosine similarity ( Wang  et al. , 2020 ). It can be used with  information  to compute the \"diversity entropy\" of an input timeseries ( Wang  et al. , 2020 ). The implementation here allows for  τ != 1 , which was not considered in the original paper. Description CosineSimilarityBinning probabilities are computed as follows. From the input time series  x , using embedding lag  τ  and embedding dimension  m ,  construct the embedding   $Y = \\{\\bf x_i \\} = \\{(x_{i}, x_{i+\\tau}, x_{i+2\\tau}, \\ldots, x_{i+m\\tau - 1}\\}_{i = 1}^{N-mτ}$ . Compute  $D = \\{d(\\bf x_t, \\bf x_{t+1}) \\}_{t=1}^{N-mτ-1}$ ,  where  $d(\\cdot, \\cdot)$  is the cosine similarity between two  m -dimensional  vectors in the embedding. Divide the interval  [-1, 1]  into  nbins  equally sized subintervals (including the value  +1 ). Construct a histogram of cosine similarities  $d \\in D$  over those subintervals. Sum-normalize the histogram to obtain probabilities. Outcome space The outcome space for  CosineSimilarityBinning  is the bins of the  [-1, 1]  interval, and the return configuration is the same as in  ValueBinning  (left bin edge). Implements codify . Used for encoding inputs where ordering matters (e.g. time series). source"},{"id":733,"pagetitle":"Probabilities","title":"ComplexityMeasures.Diversity","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Diversity","content":" ComplexityMeasures.Diversity  —  Type Diversity An alias to  CosineSimilarityBinning . source"},{"id":734,"pagetitle":"Probabilities","title":"Sequential pair distances","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Sequential-pair-distances","content":" Sequential pair distances"},{"id":735,"pagetitle":"Probabilities","title":"ComplexityMeasures.SequentialPairDistances","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.SequentialPairDistances","content":" ComplexityMeasures.SequentialPairDistances  —  Type SequentialPairDistances <: CountBasedOutcomeSpace\nSequentialPairDistances(x::AbstractVector; n = 3, metric = Chebyshev(), m = 3, τ = 1)\nSequentialPairDistances(x::AbstractStateSpaceSet; n = 3, metric = Chebyshev()) An outcome space based on the distribution of distances of sequential pairs of points. This outcome space appears implicitly as part of the \"distribution entropy\" introduced by  Li  et al.  (2015) , which of course can be reproduced here (see example below). We've generalized the method to be used with any  InformationMeasure  and  DiscreteInfoEstimator , and with valid distance  metric  (from Distances.jl). Input data  x  are needed for initialization, because distances must be pre-computed to know the minimum/maximum distances needed for binning the distribution of pairwise distances. If the input is an  AbstractVector , then the vector is embedded before computing distances. If the input is an  AbstractStateSpaceSet , then the embedding step  is skipped and distances are computed directly on each state vector  xᵢ ∈ x . Description SequentialPairDistances  does the following:  Transforms the input timeseries  x  by first embedding it using embedding dimension    m  and embedding lag  τ  (or skip this step if the input is already embedded). Computes the distances  ds  between sequential pairs of points according to the given    metric . Divides the interval  [minimum(ds), maximum(ds)]  into  n  equal-size bins by using     RectangularBinEncoding , then maps the distances onto these bins. Outcome space The outcome space  Ω  for  SequentialPairDistances  are the bins onto which the  pairwise distances are mapped, encoded as the integers  1:n . If you need the actual bin coordinates, these can be recovered with  decode  (see example below). Implements codify . Note that the input  x  is ignored when calling  codify , because   the input data is already handled when constructing a  SequentialPairDistances . Examples The outcome bins can be retrieved as follows. using ComplexityMeasures\nx = rand(100)\no = SequentialPairDistances(x)\ncts, outs = counts_and_outcomes(o, x) Computing the \"distribution entropy\" with  n = 3  bins for the distance histogram: using ComplexityMeasures\nx = rand(1000000)\no = SequentialPairDistances(x, n = 3, metric = Chebyshev()) # metric from original paper\nh = information(Shannon(base = 2), o, x) source"},{"id":736,"pagetitle":"Probabilities","title":"Bubble sort swaps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Bubble-sort-swaps","content":" Bubble sort swaps"},{"id":737,"pagetitle":"Probabilities","title":"ComplexityMeasures.BubbleSortSwaps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.BubbleSortSwaps","content":" ComplexityMeasures.BubbleSortSwaps  —  Type BubbleSortSwaps <: CountBasedOutcomeSpace\nBubbleSortSwaps(; m = 3, τ = 1) The  BubbleSortSwaps  outcome space is based on  Manis  et al.  (2017) 's  paper on \"bubble entropy\".  Description BubbleSortSwaps  does the following: Embeds the input data using embedding dimension  m  and  embedding lag  τ For each state vector in the embedding, counting how many swaps are necessary for   the bubble sort algorithm to sort state vectors. For  counts_and_outcomes , we then define a distribution over the number of  necessary swaps. This distribution can then be used to estimate probabilities using   probabilities_and_outcomes , which again can be used to estimate any   InformationMeasure . An example of how to compute the \"Shannon bubble entropy\" is given below. Outcome space The  outcome_space  for  BubbleSortSwaps  are the integers  0:N , where  N = (m * (m - 1)) / 2 + 1  (the worst-case number of swaps). Hence, the number of  total_outcomes  is  N + 1 . Implements codify . Returns the number of swaps required for each embedded state vector. Examples With the  BubbleSortSwaps  outcome space, we can easily compute a \"bubble entropy\" inspired by ( Manis  et al. , 2017 ). Note: this is not actually a new entropy - it is just  a new way of discretizing the input data. To reproduce the bubble entropy complexity measure from ( Manis  et al. , 2017 ), see  BubbleEntropy . Examples using ComplexityMeasures\nx = rand(100000)\no = BubbleSortSwaps(; m = 5) # 5-dimensional embedding vectors\ninformation(Shannon(; base = 2), o, x)\n\n# We can also compute any other \"bubble quantity\", for example the \n# \"Tsallis bubble extropy\", with arbitrary probabilities estimators:\ninformation(TsallisExtropy(), BayesianRegularization(), o, x) source"},{"id":738,"pagetitle":"Probabilities","title":"Spatial outcome spaces","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Spatial-outcome-spaces","content":" Spatial outcome spaces"},{"id":739,"pagetitle":"Probabilities","title":"ComplexityMeasures.SpatialOrdinalPatterns","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.SpatialOrdinalPatterns","content":" ComplexityMeasures.SpatialOrdinalPatterns  —  Type SpatialOrdinalPatterns <: OutcomeSpaceModel\nSpatialOrdinalPatterns(stencil, x; periodic = true) A symbolic, permutation-based  OutcomeSpace  for spatiotemporal systems that generalises  OrdinalPatterns  to high-dimensional arrays. The order  m  of the permutation pattern is extracted from the  stencil , see below. SpatialOrdinalPatterns  is based on the 2D and 3D  spatiotemporal permutation entropy  estimators by  Ribeiro  et al.  (2012)  and  Schlemmer  et al.  (2018) , respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for  D -dimensional input array  x , with arbitrary regions (stencils) to get patterns form and (possibly) periodic boundary conditions. See below for ways to specify the  stencil . If  periodic = true , then the stencil wraps around at the ends of the array. If  false , then collected regions with indices which exceed the array bounds are skipped. In combination with  information  and  information_normalized , this probabilities estimator can be used to compute generalized spatiotemporal permutation  InformationMeasure  of any type. Outcome space The outcome space  Ω  for  SpatialOrdinalPatterns  is the set of length- m  ordinal patterns (i.e. permutations) that can be formed by the integers  1, 2, …, m , ordered lexicographically. There are  factorial(m)  such patterns. Here  m  refers to the number of points included in  stencil . Stencils The  stencil  defines what local area to use to group hypervoxels. Each grouping of hypervoxels is mapped to an order- m  permutation pattern, which is then mapped to an integer as in  OrdinalPatterns . The  stencil  is moved around the input array, in a sense \"scanning\" the input array, to collect all possible groupings allowed by the boundary condition (periodic or not). Stencils are passed in one of the following three ways: As vectors of  CartesianIndex  which encode the offset of indices to include in the  stencil, with respect to the current array index when scanning over the array.  For example  stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)]) .  Don't forget to include the zero offset index if you want to include the hypervoxel  itself, which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of  CartesianIndex ,  m = length(stencil) . As a  D -dimensional array (where  D  matches the dimensionality of the input data)  containing  0 s and  1 s, where if  stencil[index] == 1 , the corresponding pixel is  included, and if  stencil[index] == 0 , it is not included.  To generate the same estimator as in 1., use  stencil = [1 1; 1 1] .  When passing a stencil as a  D -dimensional array,  m = sum(stencil) As a  Tuple  containing two  Tuple s, both of length  D , for  D -dimensional data.  The first tuple specifies the  extent  of the stencil, where  extent[i]   dictates the number of hypervoxels to be included along the  i th axis and  lag[i]   the separation of hypervoxels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here  stencil = ((2, 2), (1, 1)) .  When passing a stencil using  extent  and  lag ,  m = prod(extent) . source"},{"id":740,"pagetitle":"Probabilities","title":"ComplexityMeasures.SpatialDispersion","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.SpatialDispersion","content":" ComplexityMeasures.SpatialDispersion  —  Type SpatialDispersion <: OutcomeSpace\nSpatialDispersion(stencil, x::AbstractArray;\n    periodic = true,\n    c = 5,\n    skip_encoding = false,\n    L = nothing,\n) A dispersion-based  OutcomeSpace  that generalises  Dispersion  for input data that are high-dimensional arrays. SpatialDispersion  is based on  Azami  et al.  (2019) 's 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for  N -dimensional input data  x , with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions. In combination with  information  and  information_normalized , this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion  InformationMeasure  of any type. Arguments stencil . Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see    SpatialOrdinalPatterns . See  SpatialOrdinalPatterns  for   more information about stencils. x::AbstractArray . The input data. Must be provided because we need to know its size   for optimization and bound checking. Keyword arguments periodic::Bool . If  periodic == true , then the stencil should wrap around at the   end of the array. If  periodic = false , then pixels whose stencil exceeds the array   bounds are skipped. c::Int . Determines how many discrete categories to use for the Gaussian encoding. skip_encoding . If  skip_encoding == true ,  encoding  is ignored, and dispersion   patterns are computed directly from  x , under the assumption that  L  is the alphabet   length for  x  (useful for categorical or integer data). Thus, if    skip_encoding == true , then  L  must also be specified. This is useful for   categorical or integer-valued data. L . If  L == nothing  (default), then the number of total outcomes is inferred from    stencil  and  encoding . If  L  is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to  L . Outcome space The outcome space for  SpatialDispersion  is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all  m -dimensional delay vectors whose elements are all possible values in  1:c . There are  c^m  such vectors. Description Estimating probabilities/entropies from higher-dimensional data is conceptually simple. Discretize each value (hypervoxel) in  x  relative to all other values  xᵢ ∈ x  using the  provided  encoding  scheme. Use  stencil  to extract relevant (discretized) points around each hypervoxel. Construct a symbol these points. Take the sum-normalized histogram of the symbol as a probability distribution. Optionally, compute  information  or  information_normalized  from this  probability distribution. Usage Here's how to compute spatial dispersion entropy using the three different ways of specifying stencils. x = rand(50, 50) # first \"time slice\" of a spatial system evolution\n\n# Cartesian stencil\nstencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])\nest = SpatialDispersion(stencil_cartesian, x)\ninformation_normalized(est, x)\n\n# Extent/lag stencil\nextent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)\nest = SpatialDispersion(stencil_ext_lag, x)\ninformation_normalized(est, x)\n\n# Matrix stencil\nstencil_matrix = [1 1; 1 1]\nest = SpatialDispersion(stencil_matrix, x)\ninformation_normalized(est, x) To apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.: imgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds\nstencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)\nest = SpatialDispersion(stencil, first(imgs))\nh_vs_t = information_normalized.(Ref(est), imgs) Computing generalized spatiotemporal dispersion entropy is trivial, e.g. with  Renyi : x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialDispersion(stencil, x)\ninformation(Renyi(q = 2), est, x) See also:  SpatialOrdinalPatterns ,  GaussianCDFEncoding ,  codify . source"},{"id":741,"pagetitle":"Probabilities","title":"ComplexityMeasures.SpatialBubbleSortSwaps","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.SpatialBubbleSortSwaps","content":" ComplexityMeasures.SpatialBubbleSortSwaps  —  Type SpatialBubbleSortSwaps <: SpatialOutcomeSpace\nSpatialBubbleSortSwaps(stencil, x; periodic = true) SpatialBubbleSortSwaps  generalizes  BubbleSortSwaps  to high-dimensional arrays by encoding pixel/voxel/hypervoxel windows in terms of how many swap  operations the bubble sort algorithm requires to sort them. What does this mean? For  BubbleSortSwaps  the input data is embedded using embedding dimension  m  and the number of swaps required are computed for each embedding vector. For   SpatialBubbleSortSwaps , the \"embedding dimension\"  m  for  is inferred from the number of elements in the  stencil , and the \"embedding vectors\" are the  hypervoxels selected by the  stencil .  Outcome space The outcome space  Ω  for  SpatialBubbleSortSwaps  is the range of integers   0:(n*(n-1)÷2) , corresponding to the number of swaps required by the bubble sort algorithm to sort a particular pixel/voxel/hypervoxel window. Arguments stencil . Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). See     SpatialOrdinalPatterns  and  SpatialDispersion  for   more information about stencils and examples of how to specify them. x::AbstractArray . The input data. Must be provided because we need to know its size   for optimization and bound checking. Keyword arguments periodic::Bool . If  periodic == true , then the stencil should wrap around at the   end of the array. If  periodic = false , then pixels whose stencil exceeds the array   bounds are skipped. Example using ComplexityMeasures\nusing Random; rng = MersenneTwister(1234)\n\nx = rand(rng, 100, 100, 100) # some 3D image\nstencil = zeros(Int,2,2,2) # 3D stencil\nstencil[:, :, 1] = [1 0; 1 1]\nstencil[:, :, 2] = [0 1; 1 0]\no = SpatialBubbleSortSwaps(stencil, x)\n\n# Distribution of \"bubble sorting complexity\" among voxel windows\ncounts_and_outcomes(o, x)\n\n# \"Spatial bubble Kaniadakis entropy\", with shrinkage-adjusted probabilities\ninformation(Kaniadakis(), Shrinkage(), o, x) source"},{"id":742,"pagetitle":"Probabilities","title":"Probabilities  and related functions","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Probabilities-and-related-functions","content":" Probabilities  and related functions"},{"id":743,"pagetitle":"Probabilities","title":"ComplexityMeasures.Probabilities","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Probabilities","content":" ComplexityMeasures.Probabilities  —  Type Probabilities <: Array{<:AbstractFloat, N}\nProbabilities(probs::Array [, outcomes [, dimlabels]]) → p\nProbabilities(counts::Counts [, outcomes [, dimlabels]]) → p Probabilities  stores an  N -dimensional array of probabilities, while ensuring that the array sums to 1 (normalized probability mass). In most cases the array is a standard vector.  p  itself can be manipulated and iterated over, just like its stored array. The probabilities correspond to  outcomes  that describe the axes of the array. If  p isa Probabilities , then  p.outcomes[i]  is an an abstract vector containing the outcomes along the  i -th dimension. The outcomes have the same ordering as the probabilities, so that  p[i][j]  is the probability for outcome  p.outcomes[i][j] . The dimensions of the array are named, and can be accessed by  p.dimlabels , where  p.dimlabels[i]  is the label of the  i -th dimension. Both  outcomes  and  dimlabels  are assigned automatically if not given. If the input is a set of  Counts , and  outcomes  and  dimlabels  are not given, then the labels and outcomes are inherited from the counts. Examples julia> probs = [0.2, 0.2, 0.2, 0.2]; Probabilities(probs) # will be normalized to sum to 1\n Probabilities{Float64,1} over 4 outcomes\n Outcome(1)  0.25\n Outcome(2)  0.25\n Outcome(3)  0.25\n Outcome(4)  0.25 julia> c = Counts([12, 16, 12], [\"out1\", \"out2\", \"out3\"]); Probabilities(c)\n Probabilities{Float64,1} over 3 outcomes\n \"out1\"  0.3\n \"out2\"  0.4\n \"out3\"  0.3 source"},{"id":744,"pagetitle":"Probabilities","title":"ComplexityMeasures.probabilities","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.probabilities","content":" ComplexityMeasures.probabilities  —  Function probabilities(\n    [est::ProbabilitiesEstimator], o::OutcomeSpace, x::Array_or_SSSet\n) → p::Probabilities Compute the same probabilities as in the  probabilities_and_outcomes  function, with two differences: Do not explicitly return the outcomes. If the outcomes are not estimated for free while estimating the counts, a special integer type is used to enumerate the outcomes, to avoid the computational cost of estimating the outcomes. probabilities([est::ProbabilitiesEstimator], counts::Counts) → (p::Probabilities, Ω) The same as above, but estimate the probability directly from a set of  Counts . source"},{"id":745,"pagetitle":"Probabilities","title":"ComplexityMeasures.probabilities_and_outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.probabilities_and_outcomes","content":" ComplexityMeasures.probabilities_and_outcomes  —  Function probabilities_and_outcomes(\n    [est::ProbabilitiesEstimator], o::OutcomeSpace, x::Array_or_SSSet\n) → (p::Probabilities, Ω) Estimate a probability distribution over the set of possible outcomes  Ω  defined by the  OutcomeSpace o , given input data  x . Probabilities are estimated according to the given probabilities estimator  est , which defaults to  RelativeAmount . The input data is typically an  Array  or a  StateSpaceSet  (or  SSSet  for short); see  Input data for ComplexityMeasures.jl . Configuration options are always given as arguments to the chosen outcome space and probabilities estimator. Return a tuple where the first element is a  Probabilities  instance, which is vector-like and contains the probabilities, and where the second element  Ω  are the outcomes corresponding to the probabilities, such that  p[i]  is the probability for the outcome  Ω[i] . The outcomes are actually included in  p , and you can use the  outcomes  function on the  p  to get them.  probabilities_and_outcomes  returns both for backwards compatibility. probabilities_and_outcomes(\n    [est::ProbabilitiesEstimator], counts::Counts\n) → (p::Probabilities, Ω) Estimate probabilities from the pre-computed  counts  using the given  ProbabilitiesEstimator est . Description Probabilities are computed by: Discretizing/encoding  x  into a finite set of outcomes  Ω  specified by the provided   OutcomeSpace o . Assigning to each outcome  Ωᵢ ∈ Ω  either a count (how often it appears among the  discretized data points), or a pseudo-count (some pre-normalized probability such  that  sum(Ωᵢ for Ωᵢ in Ω) == 1 ). For outcome spaces that result in pseudo counts, such as  PowerSpectrum , these pseudo counts are simply treated as probabilities and returned directly (that is,  est  is ignored). For counting-based outcome spaces (see  OutcomeSpace  docstring), probabilities are estimated from the counts using some  ProbabilitiesEstimator  (first signature). Observed vs all probabilities Due to performance optimizations, whether the returned probabilities contain  0 s as entries or not depends on the outcome space. E.g., in  ValueBinning 0 s are skipped, while in  PowerSpectrum 0  are not skipped, because we get them for free. Use  allprobabilities_and_outcomes  to guarantee that zero probabilities are also returned (may be slower). source"},{"id":746,"pagetitle":"Probabilities","title":"ComplexityMeasures.allprobabilities_and_outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.allprobabilities_and_outcomes","content":" ComplexityMeasures.allprobabilities_and_outcomes  —  Function allprobabilities_and_outcomes(est::ProbabilitiesEstimator, x::Array_or_SSSet) → (p::Probabilities, outs)\nallprobabilities_and_outcomes(o::OutcomeSpace, x::Array_or_SSSet) → (p::Probabilities, outs) The same as  probabilities_and_outcomes , but ensures that outcomes with  0  probability are explicitly added in the returned vector. This means that  p[i]  is the probability of  ospace[i] , with  ospace = outcome_space (est, x) . This function is useful in cases where one wants to compare the probability mass functions of two different input data  x, y  under the same estimator. E.g., to compute the KL-divergence of the two PMFs assumes that the obey the same indexing. This is not true for  probabilities  even with the same  est , due to the skipping of 0 entries, but it is true for  allprobabilities_and_outcomes . source"},{"id":747,"pagetitle":"Probabilities","title":"ComplexityMeasures.probabilities!","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.probabilities!","content":" ComplexityMeasures.probabilities!  —  Function probabilities!(s, args...) Similar to  probabilities(args...) , but allows pre-allocation of temporarily used containers  s . Only works for certain estimators. See for example  OrdinalPatterns . source"},{"id":748,"pagetitle":"Probabilities","title":"ComplexityMeasures.missing_probabilities","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.missing_probabilities","content":" ComplexityMeasures.missing_probabilities  —  Function missing_probabilities([est::ProbabilitiesEstimator], o::OutcomeSpace, x) Same as  missing_outcomes , but defines a \"missing outcome\" as an outcome having 0 probability according to  est . source"},{"id":749,"pagetitle":"Probabilities","title":"Counts","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Counts","content":" Counts"},{"id":750,"pagetitle":"Probabilities","title":"ComplexityMeasures.Counts","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Counts","content":" ComplexityMeasures.Counts  —  Type Counts <: Array{<:Integer, N}\nCounts(counts [, outcomes [, dimlabels]]) → c Counts  stores an  N -dimensional array of integer  counts  corresponding to a set of  outcomes . This is typically called a \"frequency table\" or  \"contingency table\" . If  c isa Counts , then  c.outcomes[i]  is an abstract vector containing the outcomes along the  i -th dimension, where  c[i][j]  is the count corresponding to the outcome  c.outcomes[i][j] , and  c.dimlabels[i]  is the label of the  i -th dimension. Both labels and outcomes are assigned automatically if not given.  c  itself can be manipulated and iterated over like its stored array. source"},{"id":751,"pagetitle":"Probabilities","title":"ComplexityMeasures.counts_and_outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.counts_and_outcomes","content":" ComplexityMeasures.counts_and_outcomes  —  Function counts_and_outcomes(o::OutcomeSpace, x) → (cts::Counts, Ω) Discretize/encode  x  (which must be sortable) into a finite set of outcomes  Ω  specified by the provided  OutcomeSpace o , and then count how often each outcome  Ωᵢ ∈ Ω  (i.e. each \"discretized value\", or \"encoded symbol\") appears. Return a tuple where the first element is a  Counts  instance, which is vector-like and contains the counts, and where the second element  Ω  are the outcomes corresponding to the counts, such that  cts[i]  is the count for the outcome  Ω[i] . The outcomes are actually included in  cts , and you can use the  outcomes  function on the  cts  to get them.  counts_and_outcomes  returns both for backwards compatibility. counts_and_outcomes(x) → cts::Counts If no  OutcomeSpace  is specified, then  UniqueElements  is used as the outcome space. Description For  OutcomeSpace s that uses  encode  to discretize, it is possible to count how often each outcome  $\\omega_i \\in \\Omega$ , where  $\\Omega$  is the set of possible outcomes, is observed in the discretized/encoded input data. Thus, we can assign to each outcome  $\\omega_i$  a count  $f(\\omega_i)$ , such that  $\\sum_{i=1}^N f(\\omega_i) = N$ , where  $N$  is the number of observations in the (encoded) input data.  counts  returns the counts  $f(\\omega_i)_{obs}$  and outcomes only for the  observed  outcomes  $\\omega_i^{obs}$  (those outcomes that actually appear in the input data). If you need the counts for  unobserved  outcomes as well, use  allcounts_and_outcomes . source"},{"id":752,"pagetitle":"Probabilities","title":"ComplexityMeasures.counts","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.counts","content":" ComplexityMeasures.counts  —  Function counts(o::OutcomeSpace, x) → cts::Counts Compute the same counts as in the  counts_and_outcomes  function, with two differences: Do not explicitly return the outcomes. If the outcomes are not estimated for free while estimating the counts, a special integer type is used to enumerate the outcomes, to avoid the computational cost of estimating the outcomes. source"},{"id":753,"pagetitle":"Probabilities","title":"ComplexityMeasures.allcounts_and_outcomes","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.allcounts_and_outcomes","content":" ComplexityMeasures.allcounts_and_outcomes  —  Function allcounts_and_outcomes(o::OutcomeSpace, x::Array_or_SSSet) → (cts::Counts{<:Integer, 1}, Ω) Like  counts_and_outcomes , but ensures that  all  outcomes  Ωᵢ ∈ Ω , where  Ω = outcome_space(o, x) ), are included. Outcomes that do not occur in the data  x  get a 0 count. source"},{"id":754,"pagetitle":"Probabilities","title":"ComplexityMeasures.is_counting_based","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.is_counting_based","content":" ComplexityMeasures.is_counting_based  —  Function is_counting_based(o::OutcomeSpace) Return  true  if the  OutcomeSpace o  is counting-based, and  false  otherwise. source"},{"id":755,"pagetitle":"Probabilities","title":"Probability estimators","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#probability_estimators","content":" Probability estimators"},{"id":756,"pagetitle":"Probabilities","title":"ComplexityMeasures.ProbabilitiesEstimator","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator","content":" ComplexityMeasures.ProbabilitiesEstimator  —  Type ProbabilitiesEstimator The supertype for all probabilities estimators. The role of the probabilities estimator is to convert (pseudo-)counts to probabilities. Currently, the implementation of all probabilities estimators assume  finite  outcome space with known cardinality. Therefore,  ProbabilitiesEstimator  accept an  OutcomeSpace  as the first argument, which specifies the set of possible outcomes. Probabilities estimators are used with  probabilities  and  allprobabilities_and_outcomes . Implementations The default probabilities estimator is  RelativeAmount , which is compatible with any  OutcomeSpace . The following estimators only support counting-based outcomes. Shrinkage . BayesianRegularization . AddConstant . Description In ComplexityMeasures.jl, probability mass functions are estimated from data by defining a set of possible outcomes  $\\Omega = \\{\\omega_1, \\omega_2, \\ldots, \\omega_L \\}$  (by specifying an  OutcomeSpace ), and assigning to each outcome  $\\omega_i$  a probability  $p(\\omega_i)$ , such that  $\\sum_{i=1}^N p(\\omega_i) = 1$  (by specifying a  ProbabilitiesEstimator ). source"},{"id":757,"pagetitle":"Probabilities","title":"ComplexityMeasures.RelativeAmount","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount","content":" ComplexityMeasures.RelativeAmount  —  Type RelativeAmount <: ProbabilitiesEstimator\nRelativeAmount() The  RelativeAmount  estimator is used with  probabilities  and related functions to estimate probabilities over the given  OutcomeSpace  using maximum likelihood estimation (MLE), also called plug-in estimation. See  ProbabilitiesEstimator  for usage. Description Consider a length- m  outcome space  $\\Omega$  and random sample of length  N . The maximum likelihood estimate of the probability of the  k -th outcome  $\\omega_k$  is \\[p(\\omega_k) = \\dfrac{n_k}{N},\\] where  $n_k$  is the number of times the  k -th outcome was observed in the (encoded) sample. This estimation is known as  maximum likelihood estimation . However,  RelativeAmount  also serves as the fall-back probabilities estimator for  OutcomeSpace s that are not count-based and only yield \"pseudo-counts\", for example  WaveletOverlap  or  PowerSpectrum . These outcome spaces do not yield counts, but pre-normalized numbers that can be treated as \"relative frequencies\" or \"relative power\". Hence, this estimator is called  RelativeAmount . Examples using ComplexityMeasures\nx = cumsum(randn(100))\nps = probabilities(OrdinalPatterns{3}(), x) # `RelativeAmount` is the default estimator\nps_mle = probabilities(RelativeAmount(), OrdinalPatterns{3}(), x) # equivalent\nps == ps_mle # true See also:  BayesianRegularization ,  Shrinkage . source"},{"id":758,"pagetitle":"Probabilities","title":"ComplexityMeasures.BayesianRegularization","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.BayesianRegularization","content":" ComplexityMeasures.BayesianRegularization  —  Type BayesianRegularization <: ProbabilitiesEstimator\nBayesianRegularization(; a = 1.0) The  BayesianRegularization  estimator is used with  probabilities  and related functions to estimate probabilities an  m -element counting-based  OutcomeSpace  using Bayesian regularization of cell counts ( Hausser and Strimmer, 2009 ). See  ProbabilitiesEstimator  for usage. Outcome space requirements This estimator only works with counting-compatible outcome spaces. Description The  BayesianRegularization  estimator estimates the probability of the  $k$ -th outcome  $\\omega_{k}$  is \\[\\omega_{k}^{\\text{BayesianRegularization}} = \\dfrac{n_k + a_k}{n + A},\\] where  $n$  is the number of samples in the input data,  $n_k$  is the observed counts for the outcome  $\\omega_{k}$ , and  $A = \\sum_{i=1}^k a_k$ . Picking  a There are many common choices of priors, some of which are listed in  Hausser and Strimmer (2009) . They include a == 0 , which is equivalent to the  RelativeAmount  estimator. a == 0.5  (Jeffrey's prior) a == 1  (Bayes-Laplace uniform prior) a  can also be chosen as a vector of real numbers. Then, if used with  allprobabilities_and_outcomes , it is required that   length(a) == total_outcomes(o, x) , where  x  is the input data and  o  is the  OutcomeSpace . If used with  probabilities , then  length(a)  must match the number of  observed  outcomes (you can check this using  probabilities_and_outcomes ). The choice of  a  can severely impact the estimation errors of the probabilities, and the errors depend both on the choice of  a  and on the sampling scenario ( Hausser and Strimmer, 2009 ). Assumptions The  BayesianRegularization  estimator assumes a fixed and known  m . Thus, using it with  probabilities_and_outcomes  and  allprobabilities_and_outcomes  will  yield different results, depending on whether all outcomes are observed in the input data or not. For  probabilities_and_outcomes ,  m  is the number of  observed  outcomes. For  allprobabilities_and_outcomes ,  m = total_outcomes(o, x) , where  o  is the  OutcomeSpace  and  x  is the input data. Note If used with  allprobabilities_and_outcomes , then outcomes which have not been observed may be assigned non-zero probabilities. This might affect your results if using e.g.  missing_outcomes . Examples using ComplexityMeasures\nx = cumsum(randn(100))\nps_bayes = probabilities(BayesianRegularization(a = 0.5), OrdinalPatterns{3}(), x) See also:  RelativeAmount ,  Shrinkage . source"},{"id":759,"pagetitle":"Probabilities","title":"ComplexityMeasures.Shrinkage","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Shrinkage","content":" ComplexityMeasures.Shrinkage  —  Type Shrinkage{<:OutcomeSpace} <: ProbabilitiesEstimator\nShrinkage(; t = nothing, λ = nothing) The  Shrinkage  estimator is used with  probabilities  and related functions to estimate probabilities over the given  m -element counting-based  OutcomeSpace  using James-Stein-type shrinkage ( James and Stein, 1992 ), as presented in  Hausser and Strimmer (2009) . Description The  Shrinkage  estimator estimates a cell probability  $\\theta_{k}^{\\text{Shrink}}$  as \\[\\theta_{k}^{\\text{Shrink}} = \\lambda t_k + (1-\\lambda) \\hat{\\theta}_k^{RelativeAmount},\\] where  $\\lambda \\in [0, 1]$  is the shrinkage intensity ( $\\lambda = 0$  means no shrinkage, and  $\\lambda = 1$  means full shrinkage), and  $t_k$  is the shrinkage target.  Hausser and Strimmer (2009)  picks  $t_k = 1/m$ , i.e. the uniform distribution. If  t == nothing , then  $t_k$  is set to  $1/m$  for all  $k$ , as in  Hausser and Strimmer (2009) . If  λ == nothing  (the default), then the shrinkage intensity is optimized according to  Hausser and Strimmer (2009) . Hence, you should probably not pick  λ  nor  t  manually, unless you know what you are doing. Assumptions The  Shrinkage  estimator assumes a fixed and known number of outcomes  m . Thus, using it with  probabilities_and_outcomes ) and   allprobabilities_and_outcomes  will yield different results, depending on whether all outcomes are observed in the input data or not. For  probabilities_and_outcomes ,  m  is the number of  observed  outcomes. For  allprobabilities_and_outcomes ,  m = total_outcomes(o, x) , where  o  is the  OutcomeSpace  and  x  is the input data. Note If used with  allprobabilities_and_outcomes , then outcomes which have not been observed may be assigned non-zero probabilities. This might affect your results if using e.g.  missing_outcomes . Examples using ComplexityMeasures\nx = cumsum(randn(100))\nps_shrink = probabilities(Shrinkage(), OrdinalPatterns{3}(), x) See also:  RelativeAmount ,  BayesianRegularization . source"},{"id":760,"pagetitle":"Probabilities","title":"ComplexityMeasures.AddConstant","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.AddConstant","content":" ComplexityMeasures.AddConstant  —  Type AddConstant <: ProbabilitiesEstimator\nAddConstant(; c = 1.0) A generic add-constant probabilities estimator for counting-based  OutcomeSpace s, where several literature estimators can be obtained tuning  c . Currently  $c$  can only be a scalar. c = 1.0  is the Laplace estimator, or the \"add-one\" estimator. Description Probabilities for the  $k$ -th outcome  $\\omega_{k}$  are estimated as \\[p(\\omega_k) = \\dfrac{(n_k + c)}{n + mc},\\] where  $m$  is the cardinality of the outcome space, and  $n$  is the number of (encoded) input data points, and  $n_k$  is the number of times the outcome  $\\omega_{k}$  is observed in the (encoded) input data points. If the  AddConstant  estimator used with  probabilities_and_outcomes , then  $m$  is set to the number of  observed  outcomes. If used with  allprobabilities_and_outcomes , then  $m$  is set to the number of  possible  outcomes. Unobserved outcomes are assigned nonzero probability! Looking at the formula above, if  $n_k = 0$ , then unobserved outcomes are assigned a non-zero probability of  $\\dfrac{c}{n + mc}$ . This means that if the estimator is used with  allprobabilities_and_outcomes , then all outcomes, even those that are not observed, are assigned non-zero probabilities. This might affect your results if using e.g.  missing_outcomes . source"},{"id":761,"pagetitle":"Probabilities","title":"Encodings/Symbolizations API","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#encodings","content":" Encodings/Symbolizations API Count-based  OutcomeSpace s first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\". Alternative names for \"encode\" in the literature is \"symbolize\" or \"codify\", and in this package we use the latter. The encodings API is defined by: Encoding encode decode codify"},{"id":762,"pagetitle":"Probabilities","title":"ComplexityMeasures.Encoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Encoding","content":" ComplexityMeasures.Encoding  —  Type Encoding The supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions  encode  and  decode . Some probability estimators utilize encodings internally. Current available encodings are: OrdinalPatternEncoding . GaussianCDFEncoding . RectangularBinEncoding . RelativeMeanEncoding . RelativeFirstDifferenceEncoding . UniqueElementsEncoding . BubbleSortSwapsEncoding . PairDistanceEncoding . CombinationEncoding , which can combine any of the above encodings. source"},{"id":763,"pagetitle":"Probabilities","title":"ComplexityMeasures.encode","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.encode","content":" ComplexityMeasures.encode  —  Function encode(c::Encoding, χ) -> i::Int Encode an element  χ ∈ x  of input data  x  (those given to  counts ) into the positive integers  i ≥ 0  using encoding  c . The special value of  i = -1  is used as a return value for inappropriate elements  χ  that cannot be encoded according to  c . source"},{"id":764,"pagetitle":"Probabilities","title":"ComplexityMeasures.decode","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.decode","content":" ComplexityMeasures.decode  —  Function decode(c::Encoding, i::Integer) -> ω Decode an encoded element  i  into the outcome  ω ∈ Ω  it corresponds to.  Ω  is the  outcome_space  that uses encoding  c . source"},{"id":765,"pagetitle":"Probabilities","title":"ComplexityMeasures.codify","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.codify","content":" ComplexityMeasures.codify  —  Function codify(o::OutcomeSpace, x::Vector) → s::Vector{Int}\ncodify(o::OutcomeSpace, x::AbstractStateSpaceSet{D}) → s::NTuple{D, Vector{Int} Codify  x  according to the outcome space  o . If  x  is a  Vector , then a  Vector{<:Integer}  is returned. If  x  is a  StateSpaceSet{D} , then symbolization is done column-wise and an  NTuple{D, Vector{<:Integer}}  is returned, where  D = dimension(x) . Description The reason this function exists is that we don't always want to  encode  the entire input  x  at once. Sometimes, it is desirable to first apply some transformation to  x  first, then apply  Encoding s in a point-wise manner in the transformed space. (the  OutcomeSpace  dictates this transformation). This is useful for encoding timeseries data. The length of the returned  s  depends on the  OutcomeSpace . Some outcome spaces preserve the input data length (e.g.  UniqueElements ), while some outcome spaces (e.g.  OrdinalPatterns ) do e.g. delay embeddings before encoding, so that  length(s) < length(x) . source"},{"id":766,"pagetitle":"Probabilities","title":"Available encodings","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#Available-encodings","content":" Available encodings"},{"id":767,"pagetitle":"Probabilities","title":"ComplexityMeasures.OrdinalPatternEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.OrdinalPatternEncoding","content":" ComplexityMeasures.OrdinalPatternEncoding  —  Type OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding{m}(lt = ComplexityMeasures.isless_rand) An encoding scheme that  encode s length- m  vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by  OrdinalPatterns  and similar estimators, see that for a description of the outcome space. The ordinal/permutation pattern of a vector  χ  is simply  sortperm(χ) , which gives the indices that would sort  χ  in ascending order. Description The Lehmer code, as implemented here, is a bijection between the set of  factorial(m)  possible permutations for a length- m  sequence, and the integers  1, 2, …, factorial(m) . The encoding step uses algorithm 1 in  Berger  et al.  (2019) , which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!). Example julia> using ComplexityMeasures\n\njulia> χ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, χ)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3 If you want to encode something that is already a permutation pattern, then you can use the non-exported  permutation_to_integer  function. source"},{"id":768,"pagetitle":"Probabilities","title":"ComplexityMeasures.GaussianCDFEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.GaussianCDFEncoding","content":" ComplexityMeasures.GaussianCDFEncoding  —  Type GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding{m}(; μ, σ, c::Int = 3) An encoding scheme that  encode s a scalar or vector  χ  into one of the integers  sᵢ ∈ [1, 2, …, c]  based on the normal cumulative distribution function (NCDF), and  decode s the  sᵢ  into subintervals of  [0, 1]  (with some loss of information). Initializing a  GaussianCDFEncoding The size of the input to be encoded must be known beforehand. One must therefore set  m = length(χ) , where  χ  is the input ( m = 1  for scalars,  m ≥ 2  for vectors). To do so, one must explicitly give  m  as a type parameter: e.g.  encoding = GaussianCDFEncoding{3}(; μ = 0.0, σ = 0.1)  to encode 3-element vectors, or  encoding = GaussianCDFEncoding{1}(; μ = 0.0, σ = 0.1)  to encode scalars. Description Encoding/decoding scalars GaussianCDFEncoding  first maps an input scalar  $χ$  to a new real number  $y_ \\in [0, 1]$  by using the normal cumulative distribution function (CDF) with the given mean  μ  and standard deviation  σ , according to the map \\[x \\to y : y = \\dfrac{1}{ \\sigma\n    \\sqrt{2 \\pi}} \\int_{-\\infty}^{x} e^{(-(x - \\mu)^2)/(2 \\sigma^2)} dx.\\] Next, the interval  [0, 1]  is equidistantly binned and enumerated  $1, 2, \\ldots, c$ ,  and  $y$  is linearly mapped to one of these integers using the linear map   $y \\to z : z = \\text{floor}(y(c-1)) + 1$ . Because of the floor operation, some information is lost, so when used with  decode , each decoded  sᵢ  is mapped to a  subinterval  of  [0, 1] . This subinterval is returned as a length- 1 Vector{SVector} . Notice that the decoding step does not yield an element of any outcome space of the estimators that use  GaussianCDFEncoding  internally, such as  Dispersion . That is because these estimators additionally delay embed the encoded data. Encoding/decoding vectors If  GaussianCDFEncoding  is used with a vector  χ , then each element of  χ  is encoded separately, resulting in a  length(χ)  sequence of integers which may be treated as a  CartesianIndex . The encoded symbol  s ∈ [1, 2, …, c]  is then just the linear index corresponding to this cartesian index (similar to how  CombinationEncoding  works). When  decode d, the integer symbol  s  is converted back into its  CartesianIndex  representation,  which is just a sequence of integers that refer to subdivisions of the  [0, 1]  interval. The relevant subintervals are then returned as a length- χ Vector{SVector} . Examples julia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6 source"},{"id":769,"pagetitle":"Probabilities","title":"ComplexityMeasures.RectangularBinEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RectangularBinEncoding","content":" ComplexityMeasures.RectangularBinEncoding  —  Type RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning) An encoding scheme that  encode s points  χ ∈ x  into their histogram bins. The first call signature simply initializes a  FixedRectangularBinning  and then calls the second call signature. See  FixedRectangularBinning  for info on mapping points to bins. source"},{"id":770,"pagetitle":"Probabilities","title":"ComplexityMeasures.RelativeMeanEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeMeanEncoding","content":" ComplexityMeasures.RelativeMeanEncoding  —  Type RelativeMeanEncoding <: Encoding\nRelativeMeanEncoding(minval::Real, maxval::Real; n = 2) RelativeMeanEncoding  encodes a vector based on the relative position the mean of the vector has with respect to a predefined minimum and maximum value ( minval  and  maxval , respectively). Description This encoding is inspired by  Azami and Escudero (2016) 's algorithm for amplitude-aware permutation entropy. They use a linear combination of amplitude information and first differences information of state vectors to correct probabilities. Here, however, we explicitly encode the amplitude-part of the correction as an a integer symbol  Λ ∈ [1, 2, …, n] . The first-difference part of the encoding is available as the  RelativeFirstDifferenceEncoding  encoding. Encoding/decoding When used with  encode , an  $m$ -element state vector  $\\bf{x} = (x_1, x_2, \\ldots, x_m)$  is encoded as  $Λ = \\dfrac{1}{N}\\sum_{i=1}^m abs(x_i)$ . The value of  $Λ$  is then normalized to lie on the interval  [0, 1] , assuming that the minimum/maximum value any single element  $x_i$  can take is  minval / maxval , respectively. Finally, the interval  [0, 1]  is discretized into  n  discrete bins, enumerated by positive integers  1, 2, …, n , and the number of the bin that the normalized  $Λ$  falls into is returned. When used with  decode , the left-edge of the bin that the normalized  $Λ$  fell into is returned. source"},{"id":771,"pagetitle":"Probabilities","title":"ComplexityMeasures.RelativeFirstDifferenceEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeFirstDifferenceEncoding","content":" ComplexityMeasures.RelativeFirstDifferenceEncoding  —  Type RelativeFirstDifferenceEncoding <: Encoding\nRelativeFirstDifferenceEncoding(minval::Real, maxval::Real; n = 2) RelativeFirstDifferenceEncoding  encodes a vector based on the relative position the average of the  first differences  of the vectors has with respect to a predefined minimum and maximum value ( minval  and  maxval , respectively). Description This encoding is inspired by  Azami and Escudero (2016) 's algorithm for amplitude-aware permutation entropy. They use a linear combination of amplitude information and first differences information of state vectors to correct probabilities. Here, however, we explicitly encode the first differences part of the correction as an a integer symbol  Λ ∈ [1, 2, …, n] . The amplitude part of the encoding is available as the  RelativeMeanEncoding  encoding. Encoding/decoding When used with  encode , an  $m$ -element state vector  $\\bf{x} = (x_1, x_2, \\ldots, x_m)$  is encoded as  $Λ = \\dfrac{1}{m - 1}\\sum_{k=2}^m |x_{k} - x_{k-1}|$ . The value of  $Λ$  is then normalized to lie on the interval  [0, 1] , assuming that the minimum/maximum value any single  $abs(x_k - x_{k-1})$  can take is  minval / maxval , respectively. Finally, the interval  [0, 1]  is discretized into  n  discrete bins, enumerated by positive integers  1, 2, …, n , and the number of the bin that the normalized  $Λ$  falls into is returned. The smaller the mean first difference of the state vector is, the smaller the bin number is. The higher the mean first difference of the state vectors is, the higher the bin number is. When used with  decode , the left-edge of the bin that the normalized  $Λ$  fell into is returned. Performance tips If you are encoding multiple input vectors, it is more efficient to construct a  RelativeFirstDifferenceEncoding  instance and re-use it: minval, maxval = 0, 1\nencoding = RelativeFirstDifferenceEncoding(minval, maxval; n = 4)\npts = [rand(3) for i = 1:1000]\n[encode(encoding, x) for x in pts] source"},{"id":772,"pagetitle":"Probabilities","title":"ComplexityMeasures.UniqueElementsEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.UniqueElementsEncoding","content":" ComplexityMeasures.UniqueElementsEncoding  —  Type UniqueElementsEncoding <: Encoding\nUniqueElementsEncoding(x) UniqueElementsEncoding  is a generic encoding that encodes each  xᵢ ∈ unique(x)  to one of the positive integers. The  xᵢ  are encoded according to the order of their first appearance in the input data. The constructor requires the input data  x , since the number of possible symbols is  length(unique(x)) . Example using ComplexityMeasures\nx = ['a', 2, 5, 2, 5, 'a']\ne = UniqueElementsEncoding(x)\nencode.(Ref(e), x) == [1, 2, 3, 2, 3, 1] # true source"},{"id":773,"pagetitle":"Probabilities","title":"ComplexityMeasures.PairDistanceEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.PairDistanceEncoding","content":" ComplexityMeasures.PairDistanceEncoding  —  Type PairDistanceEncoding <: Encoding\nPairDistanceEncoding(min_dist, max_dist; n = 2, metric = Chebyshev(), precise = false) An encoding that  encode s point pairs of the form   Tuple{<:AbstractVector, <:AbstractVector}  by first computing their distance  using the given  metric , then dividing the interval [ min_dist, max_dist]  into   n  equal-size bins, and mapping the computed distance onto one of those bins. Bins are enumerated as  1:n . When  decode -ing the bin integer, the left edge of the bin is returned. precise  has the same meaning as in  RectangularBinEncoding . Example Let's create an example where the minimum and maximum allowed distance is known. using ComplexityMeasures, Distances, StaticArrays\nm = Chebyshev()\ny = [SVector(1.0), SVector(0.5), SVector(0.25), SVector(0.64)]\npair1, pair2, pair3 = (y[1], y[2]), (y[2], y[3]), (y[3], y[4])\ndmax = m(pair1...) # dist = 0.50\ndmin = m(pair2...) # dist = 0.25\ndmid = m(pair3...) # dist = 0.39\n\n# This should give five bins with left adges at [0.25], [0.30], [0.35], [0.40] and [0.45]\nencoding = PairDistanceEncoding(dmin, dmax; n = 5, metric = m)\nc1 = encode(encoding, pair1) # 5\nc2 = encode(encoding, pair2) # 1\nc3 = encode(encoding, pair3) # 3\n\ndecode(encoding, c3) ≈ [0.35] # true source"},{"id":774,"pagetitle":"Probabilities","title":"ComplexityMeasures.BubbleSortSwapsEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.BubbleSortSwapsEncoding","content":" ComplexityMeasures.BubbleSortSwapsEncoding  —  Type BubbleSortSwapsEncoding <: Encoding\nBubbleSortSwapsEncoding{m}() BubbleSortSwapsEncoding  is used with  encode  to encode a length- m  input vector  x  into an integer in the range  ω ∈ 0:((m*(m-1)) ÷ 2) , by counting the number  of swaps required for the bubble sort algorithm to sort  x  in ascending order.  decode  is not implemented for this encoding. Example using ComplexityMeasures\nx = [1, 5, 3, 1, 2]\ne = BubbleSortSwapsEncoding{5}() # constructor type argument must match length of vector \nencode(e, x) source"},{"id":775,"pagetitle":"Probabilities","title":"ComplexityMeasures.CombinationEncoding","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.CombinationEncoding","content":" ComplexityMeasures.CombinationEncoding  —  Type CombinationEncoding <: Encoding\nCombinationEncoding(encodings) A  CombinationEncoding  takes multiple  Encoding s and creates a combined encoding that can be used to encode inputs that are compatible with the given  encodings . Encoding/decoding When used with  encode , each  Encoding  in  encodings  returns integers in the set  1, 2, …, n_e , where  n_e  is the total number of outcomes for a particular encoding. For  k  different encodings, we can thus construct the cartesian coordinate  (c₁, c₂, …, cₖ)  ( cᵢ ∈ 1, 2, …, n_i ), which can uniquely be identified by an integer. We can thus identify each unique  combined  encoding with a single integer. When used with  decode , the integer symbol is converted to its corresponding cartesian coordinate, which is used to retrieve the decoded symbols for each of the encodings, and a tuple of the decoded symbols are returned. The total number of outcomes is  prod(total_outcomes(e) for e in encodings) . Examples using ComplexityMeasures\n\n# We want to encode the vector `x`.\nx = [0.9, 0.2, 0.3]\n\n# To do so, we will use a combination of first-difference encoding, amplitude encoding,\n# and ordinal pattern encoding.\n\nencodings = (\n    RelativeFirstDifferenceEncoding(0, 1; n = 2),\n    RelativeMeanEncoding(0, 1; n = 5),\n    OrdinalPatternEncoding(3) # x is a three-element vector\n    )\nc = CombinationEncoding(encodings)\n\n# Encode `x` as integer\nω = encode(c, x)\n\n# Decode symbol (into a vector of decodings, one for each encodings `e ∈ encodings`).\n# In this particular case, the first two element will be left-bin edges, and\n# the last element will be the decoded ordinal pattern (indices that would sort `x`).\nd = decode(c, ω) source"},{"id":778,"pagetitle":"References","title":"References","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/references/#References","content":" References Alizadeh, N. H. and Arghami, N. R. (2010).  A new estimator of entropy . Journal of the Iranian Statistical Society (JIRSS). Amigó, J. M.; Balogh, S. G. and Hernández, S. (2018).  A brief review of generalized entropies . Entropy  20 , 813. Amigó, J. M.; Szczepański, J.; Wajnryb, E. and Sanchez-Vives, M. V. (2004).  Estimating the Entropy Rate of Spike Trains via Lempel-Ziv Complexity .  Neural Computation  16 , 717–736 ,  arXiv:https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf . Anteneodo, C. and Plastino, A. R. (1999).  Maximum entropy approach to stretched exponential probability distributions .  Journal of Physics A: Mathematical and General  32 , 1089 . Arora, A.; Meister, C. and Cotterell, R. (2022).  Estimating the Entropy of Linguistic Distributions , arXiv,  arXiv:2204.01469 [cs.CL] . Azami, H. and Escudero, J. (2016).  Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation .  Computer Methods and Programs in Biomedicine  128 , 40–51 . Azami, H.; Rostaghi, M.; Abásolo, D. and Escudero, J. (2017).  Refined Composite Multiscale Dispersion Entropy and its Application to Biomedical Signals .  IEEE Transactions on Biomedical Engineering  64 , 2872–2879 . Azami, H.; da Silva, L. E.; Omoto, A. C. and Humeau-Heurtier, A. (2019).  Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images .  Signal Processing: Image Communication  75 , 178–187 . Bandt, C. and Pompe, B. (2002).  Permutation Entropy: A Natural Complexity Measure for Time Series .  Phys. Rev. Lett.  88 , 174102 . Bates, J. E. and Shepard, H. K. (1993).  Measuring complexity using information fluctuation . Physics Letters A  172 , 416–425. Berger, S.; Kravtsiv, A.; Schneider, G. and Jordan, D. (2019).  Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code .  Entropy  21 . Chao, A. and Shen, T.-J. (2003).  Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample .  Environmental and Ecological Statistics  10 , 429–443 . Charzyńska, A. and Gambin, A. (2016).  Improvement of the k-nn Entropy Estimator with Applications in Systems Biology .  Entropy  18 . Correa, J. C. (1995).  A new estimator of entropy .  Communications in Statistics - Theory and Methods  24 , 2439–2449 ,  arXiv:https://doi.org/10.1080/03610929508831626 . Costa, M.; Goldberger, A. L. and Peng, C.-K. (2002).  Multiscale Entropy Analysis of Complex Physiologic Time Series .  Phys. Rev. Lett.  89 , 068102 . Costa, M. D. and Goldberger, A. L. (2015).  Generalized Multiscale Entropy Analysis: Application to Quantifying the Complex Volatility of Human Heartbeat Time Series .  Entropy  17 , 1197–1203 . Curado, E. M. and Nobre, F. D. (2004).  On the stability of analytic entropic forms .  Physica A: Statistical Mechanics and its Applications  335 , 94–106 . Datseris, G. and Parlitz, U. (2022).  Nonlinear dynamics: a concise introduction interlaced with code  ( Springer Nature ). Diego, D.; Haaga, K. A. and Hannisdal, B. (2019).  Transfer entropy computation using the Perron-Frobenius operator .  Phys. Rev. E  99 , 042212 . Ebrahimi, N.; Pflughoeft, K. and Soofi, E. S. (1994).  Two measures of sample entropy .  Statistics & Probability Letters  20 , 225–234 . Fadlallah, B.; Chen, B.; Keil, A. and Prı́ncipe, J. (2013).  Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information .  Phys. Rev. E  87 , 022911 . Gao, S.; Ver Steeg, G. and Galstyan, A. (09–12 May 2015).  Efficient Estimation of Mutual Information for Strongly Dependent Variables . In:  Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics , Vol. 38 of  Proceedings of Machine Learning Research , edited by Lebanon, G. and Vishwanathan, S. V. (PMLR, San Diego, California, USA); pp. 277–286. Goria, M. N.; Leonenko, N. N.; Mergel, V. V. and Inverardi, P. L. (2005).  A new class of random vector entropy estimators and its applications in testing statistical hypotheses .  Journal of Nonparametric Statistics  17 , 277–297 ,  arXiv:https://doi.org/10.1080/104852504200026815 . Grassberger, P. (2022).  On Generalized Schürmann Entropy Estimators .  Entropy  24 . Hausser, J. and Strimmer, K. (2009).  Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks.  Journal of Machine Learning Research  10 . He, S.; Sun, K. and Wang, H. (2016).  Multivariate permutation entropy and its application for complexity analysis of chaotic systems .  Physica A: Statistical Mechanics and its Applications  461 , 812–823 . Horvitz, D. G. and Thompson, D. J. (1952).  A Generalization of Sampling Without Replacement from a Finite Universe .  Journal of the American Statistical Association  47 , 663–685 ,  arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446 . James, W. and Stein, C. (1992).  Estimation with quadratic loss . In:  Breakthroughs in statistics: Foundations and basic theory  (Springer); pp. 443–460. Kozachenko, L. F. and Leonenko, N. N. (1987).  Sample estimate of the entropy of a random vector . Problemy Peredachi Informatsii  23 , 9–16. Kraskov, A.; Stögbauer, H. and Grassberger, P. (2004).  Estimating mutual information .  Phys. Rev. E  69 , 066138 . Lad, F.; Sanfilippo, G. and Agrò, G. (2015).  Extropy: Complementary Dual of Entropy .  Statistical Science  30 , 40–58 . Lempel, A. and Ziv, J. (1976).  On the Complexity of Finite Sequences .  IEEE Transactions on Information Theory  22 , 75–81 . Leonenko, N.; Pronzato, L. and Savani, V. (2008).  A class of Rényi information estimators for multidimensional densities .  The Annals of Statistics  36 , 2153–2182 . Li, G.; Guan, Q. and Yang, H. (2019).  Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising .  Entropy  21 . Li, P.; Liu, C.; Li, K.; Zheng, D.; Liu, C. and Hou, Y. (2015).  Assessing the complexity of short-term heartbeat interval series by distribution entropy .  Medical & biological engineering & computing  53 , 77–87 . Li, Y.; Gao, X. and Wang, L. (2019).  Reverse Dispersion Entropy: A New Complexity Measure for Sensor Signal .  Sensors  19 . Liu, J. and Xiao, F. (2023).  Renyi extropy .  Communications in Statistics, Theory and Methods  52 , 5836–5847 . Llanos, F.; Alexander, J. M.; Stilp, C. E. and Kluender, K. R. (2017).  Power spectral entropy as an information-theoretic correlate of manner of articulation in American English .  The Journal of the Acoustical Society of America  141 , EL127–EL133 . Lord, W. M.; Sun, J. and Bollt, E. M. (2018).  Geometric k-nearest neighbor estimation of entropy and mutual information .  Chaos: An Interdisciplinary Journal of Nonlinear Science  28 . Manis, G.; Aktaruzzaman, M. and Sassi, R. (2017).  Bubble entropy: An entropy almost free of parameters . IEEE Transactions on Biomedical Engineering  64 , 2711–2718. Miller, G. (1955).  Note on the bias of information estimates . Information theory in psychology: Problems and methods. Paninski, L. (2003).  Estimation of entropy and mutual information .  Neural computation  15 , 1191–1253 . Pincus, S. M. (1991).  Approximate entropy as a measure of system complexity. Proceedings of the National Academy of Sciences  88 , 2297–2301 . Prichard, D. and Theiler, J. (1995).  Generalized redundancies for time series analysis .  Physica D: Nonlinear Phenomena  84 , 476–493 . Ribeiro, H. V.; Zunino, L.; Lenzi, E. K.; Santoro, P. A. and Mendes, R. S. (2012).  Complexity-Entropy Causality Plane as a Complexity Measure for Two-Dimensional Patterns .  PLOS ONE  7 , 1–9 . Richman, J. S. and Moorman, J. R. (2000).  Physiological time-series analysis using approximate entropy and sample entropy .  American journal of physiology-heart and circulatory physiology  278 , H2039–H2049 . Rosso, O. A.; Blanco, S.; Yordanova, J.; Kolev, V.; Figliola, A.; Schürmann, M. and Başar, E. (2001).  Wavelet entropy: a new tool for analysis of short duration brain electrical signals .  Journal of Neuroscience Methods  105 , 65–75 . Rosso, O. A.; Larrondo, H.; Martin, M. T.; Plastino, A. and Fuentes, M. A. (2007).  Distinguishing noise from chaos .  Physical review letters  99 , 154102 . Rosso, O. A.; Martín, M.; Larrondo, H. A.; Kowalski, A. and Plastino, A. (2013).  Generalized statistical complexity: A new tool for dynamical systems . Concepts and recent advances in generalized information measures and statistics, 169–215. Rostaghi, M. and Azami, H. (2016).  Dispersion entropy: A measure for time-series analysis .  IEEE Signal Processing Letters  23 , 610–614 . Rényi, A. (1961).  On measures of entropy and information . In:  Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics , Vol. 4 (University of California Press); pp. 547–562. Schlemmer, A.; Berg, S.; Lilienkamp, T.; Luther, S. and Parlitz, U. (2018).  Spatiotemporal permutation entropy as a measure for complexity of cardiac arrhythmia .  Frontiers in Physics  6 , 39 . Schürmann, T. (2004).  Bias analysis in entropy estimation .  Journal of Physics A: Mathematical and General  37 , L295 . Shannon, C. E. (1948).  A mathematical theory of communication .  The Bell system technical journal  27 , 379–423 . Singh, H.; Misra, N.; Hnizdo, V.; Fedorowicz, A. and Demchuk, E. (2003).  Nearest neighbor estimates of entropy .  American journal of mathematical and management sciences  23 , 301–321 . Sippel, S.; Lange, H. and Gans, F. (2016),  statcomp: Statistical Complexity and Information measures for time series analysis . R package version. Tian, Y.; Zhang, H.; Xu, W.; Zhang, H.; Yang, L.; Zheng, S. and Shi, Y. (2017).  Spectral entropy can predict changes of working memory performance reduced by short-time training in the delayed-match-to-sample task .  Frontiers in human neuroscience  11 , 437 . Tsallis, C. (1988).  Possible generalization of Boltzmann-Gibbs statistics .  Journal of statistical physics  52 , 479–487 . Tsallis, C. (2009).  Introduction to nonextensive statistical mechanics: approaching a complex world . Vol. 1 no. 1 (Springer). Vasicek, O. (1976).  A test for normality based on sample entropy .  Journal of the Royal Statistical Society Series B: Statistical Methodology  38 , 54–59 . Wang, X.; Si, S. and Li, Y. (2020).  Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery .  IEEE Transactions on Industrial Informatics  17 , 5419–5429 . Wu, S.-D.; Wu, C.-W.; Lin, S.-G.; Wang, C.-C. and Lee, K.-Y. (2013).  Time series analysis using composite multiscale entropy .  Entropy  15 , 1069–1084 . Xue, Y. and Deng, Y. (2023).  Tsallis extropy .  Communications in Statistics-Theory and Methods  52 , 751–762 . Zahl, S. (1977).  Jackknifing an index of diversity .  Ecology  58 , 907–913 . Zhou, Q.; Shang, P. and Zhang, B. (2023).  Using missing dispersion patterns to detect determinism and nonlinearity in time series data .  Nonlinear Dynamics  111 , 439–458 . Zhu, J.; Bellanger, J.-J.; Shu, H. and Le Bouquin Jeannès, R. (2015).  Contribution to transfer entropy estimation via the k-nearest-neighbors approach .  Entropy  17 , 4173–4201 . Zunino, L.; Olivares, F.; Scholkmann, F. and Rosso, O. A. (2017).  Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions .  Physics Letters A  381 , 1883–1892 ."},{"id":781,"pagetitle":"Tutorial","title":"Tutorial","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Tutorial","content":" Tutorial The goal of this tutorial is threefold: To convey the  terminology  used by ComplexityMeasures.jl: key terms, what they mean, and how they are used within the codebase. To provide a  rough overview  of the overall features provided by ComplexityMeasures.jl. To introduce the  main API functions  of ComplexityMeasures.jl in a single, self-contained document: how these functions connect to key terms, what are their main inputs and outputs, and how they are used in realistic scientific scripting. Note The documentation and exposition of ComplexityMeasures.jl is inspired by chapter 5 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022 ( Datseris and Parlitz, 2022 ), and expanded to cover more content."},{"id":782,"pagetitle":"Tutorial","title":"First things first: \"complexity measures\"","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#First-things-first:-\"complexity-measures\"","content":" First things first: \"complexity measures\" \"Complexity measure\" is a generic, umbrella term, used extensively in the nonlinear timeseries analysis (NLTS) literature. Roughly speaking, a complexity measure is a quantity extracted from input data that quantifies some dynamical property in the data (often, complexity measures are entropy variants). These complexity measures can highlight some aspects of the dynamics more than others, or distinguish one type of dynamics from another, or classify timeseries into classes with different dynamics, among other things. Typically, more \"complex\" data have higher complexity measure value. ComplexityMeasures.jl implements hundreds such measures, and hence it is named as such. To enable this, ComplexityMeasures.jl is more than a collection \"dynamic statistics\": it is also a framework for rigorously defining probability spaces and estimating probabilities from input data. Within the codebase of ComplexityMeasures.jl we make a separation with the functions  information  (or its daughter function  entropy ) and  complexity . We use  information  for complexity measures that are  explicit functionals of probability mass or probability density functions , even though these measures might not be labelled as \"information measures\" in the literature. We use  complexity  for other complexity measures that are not explicit functionals of probabilities. We stress that the separation between  information  and  complexity  is purely pragmatic, to establish a generic and extendable software interface within ComplexityMeasures.jl."},{"id":783,"pagetitle":"Tutorial","title":"The basics: probabilities and outcome spaces","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#The-basics:-probabilities-and-outcome-spaces","content":" The basics: probabilities and outcome spaces Information measures and some other complexity measures are computed based on  probabilities  derived from input data. In order to derive probabilities from data, an  outcome space  (also called a sample space) needs to be defined: a way to transform data into elements  $\\omega$  of an outcome space  $\\omega \\in \\Omega$ , and assign probabilities to each outcome  $p(\\omega)$ , such that  $p(\\Omega)=1$ .  $\\omega$  are called  outcomes  or  events . In code, outcome spaces are subtypes of  OutcomeSpace . For example, one outcome space is the  ValueBinning , which is the most commonly known outcome space, and corresponds to discretizing data by putting the data values into bins of a specific size. using ComplexityMeasures\n\nx = randn(10_000)\nε = 0.1 # bin width\no = ValueBinning(ε)\no isa OutcomeSpace true Such outcome spaces may be given to  probabilities_and_outcomes  to estimate the probabilities and corresponding outcomes from input data: probs, outs = probabilities_and_outcomes(o, x);\nprobs  Probabilities{Float64,1} over 71 outcomes \n  [-3.861294716805725]   0.0002\n  [-3.461294716805725]   0.0003\n  [-3.361294716805725]   0.0002\n  [-3.261294716805725]   0.0003\n  [-3.161294716805725]   0.0004\n  [-3.061294716805725]   0.0003\n  [-2.961294716805725]   0.0004\n  [-2.861294716805725]   0.0003\n  [-2.761294716805725]   0.0011\n  [-2.661294716805725]   0.0017\n ⋮                     \n  [2.638705283194275]    0.0012\n  [2.7387052831942755]   0.0011\n  [2.838705283194275]    0.0005\n  [2.9387052831942757]   0.0004\n  [3.0387052831942754]   0.0003\n  [3.2387052831942755]   0.0002\n  [3.4387052831942757]   0.0001\n  [3.638705283194275]    0.0001\n  [3.7387052831942755]   0.0001 In this example the probabilities are the (normalized) heights of each bin of the histogram. The bins, which are the  elements  of the outcome space, are shown in the margin, left of the probabilities. They are also returned explicitly as  outs  above. This convenience printing syntax with outcomes and probabilities is useful for visual inspection of the probabilities data. However, don't let it worry you. Probabilities are returned as a special  Probabilities  type that behaves identically to a standard Julia numerical  Vector . You can obtain the maximum probability maximum(probs) 0.0427 or iterate over the probabilities function total(probs)\n    t = 0.0\n    for p in probs\n        t += p\n    end\n    return t\nend\n\ntotal(probs) 0.9999999999999998 Notice that if you use  probabilities  instead of  probabilities_and_outcomes , then outcomes are enumerated generically. This avoids computing outcomes explicitly, and can save some computation time in cases where you don't need the outcomes. probs2 = probabilities(o, x)  Probabilities{Float64,1} over 71 outcomes \n   Outcome(1)   0.0002\n   Outcome(2)   0.0003\n   Outcome(3)   0.0002\n   Outcome(4)   0.0003\n   Outcome(5)   0.0004\n   Outcome(6)   0.0003\n   Outcome(7)   0.0004\n   Outcome(8)   0.0003\n   Outcome(9)   0.0011\n  Outcome(10)   0.0017\n           ⋮  \n  Outcome(63)   0.0012\n  Outcome(64)   0.0011\n  Outcome(65)   0.0005\n  Outcome(66)   0.0004\n  Outcome(67)   0.0003\n  Outcome(68)   0.0002\n  Outcome(69)   0.0001\n  Outcome(70)   0.0001\n  Outcome(71)   0.0001 Of course, if the outcomes are obtained for free while estimating the probabilities, they would be included in the return value. For the  ValueBinning  example that we use, the outcomes are the left edges of each bin. This allows us to straightforwardly visualize the results. using CairoMakie\nouts = outcomes(probs);\nleft_edges = only.(outs) # convert `Vector{SVector}` into `Vector{Real}`\nbarplot(left_edges, probs; axis = (ylabel = \"probability\",)) Naturally, there are other outcome spaces one may use, and one can find the list of implemented ones in  OutcomeSpace . A prominent example used in the NLTS literature are ordinal patterns. The outcome space for it is  OrdinalPatterns , and can be particularly useful with timeseries that come from nonlinear dynamical systems. For example, let's simulate a logistic map timeseries. using DynamicalSystemsBase\n\nlogistic_rule(u, r, t) = SVector(r*u[1]*(1 - u[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], 4.0)\nY, t = trajectory(ds, 10_000; Ttr = 100)\ny = Y[:, 1]\nsummary(y) \"10001-element Vector{Float64}\" We can then estimate the probabilities corresponding to the ordinal patterns of a certain length (here we use  m = 3 ). o = OrdinalPatterns{3}()\nprobsy = probabilities(o, y)  Probabilities{Float64,1} over 5 outcomes \n  [1, 2, 3]   0.33973397339733974\n  [1, 3, 2]   0.06440644064406441\n  [2, 1, 3]   0.13051305130513052\n  [2, 3, 1]   0.1996199619961996\n  [3, 1, 2]   0.26572657265726574 Comparing these probabilities with those for the purely random timeseries  x , probsx = probabilities(o, x)  Probabilities{Float64,1} over 6 outcomes \n  [1, 2, 3]   0.16393278655731147\n  [1, 3, 2]   0.16503300660132025\n  [2, 1, 3]   0.16853370674134827\n  [2, 3, 1]   0.16463292658531706\n  [3, 1, 2]   0.16803360672134426\n  [3, 2, 1]   0.16983396679335866 you will notice that the probabilities computing from  x  has six outcomes, while the probabilities computed from the timeseries  y  has five outcomes. The reason that there are less outcomes in the  y  is because one outcome was never encountered in the  y  data. This is a common theme in ComplexityMeasures.jl: outcomes that are not in the data are skipped. This can save memory for outcome spaces with large numbers of outcomes. To explicitly obtain all outcomes, by assigning 0 probability to not encountered outcomes, use  allprobabilities_and_outcomes . For  OrdinalPatterns  the outcome space does not depend on input data and is always the same. Hence, the corresponding outcomes matching to  allprobabilities_and_outcomes , coincide for  x  and  y , and also coincide with the output of the function  outcome_space : o = OrdinalPatterns()\nprobsx = allprobabilities(o, x)\nprobsy = allprobabilities(o, y)\noutsx = outsy = outcome_space(o)\n# display all quantities as parallel columns\nhcat(outsx, probsx, probsy) 6×3 Matrix{Any}:\n [1, 2, 3]  0.163933  0.339734\n [1, 3, 2]  0.165033  0.0644064\n [2, 1, 3]  0.168534  0.130513\n [2, 3, 1]  0.164633  0.19962\n [3, 1, 2]  0.168034  0.265727\n [3, 2, 1]  0.169834  0.0 The number of  possible  outcomes, i.e., the cardinality of the outcome space, can always be found using  total_outcomes : total_outcomes(o) 6"},{"id":784,"pagetitle":"Tutorial","title":"Beyond the basics: probabilities  estimators","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Beyond-the-basics:-probabilities-*estimators*","content":" Beyond the basics: probabilities  estimators So far we have been estimating probabilities by counting the amount of times each possible outcome was encountered in the data, then normalizing. This is called \"maximum likelihood estimation\" of probabilities. To get the counts themselves, use the  counts  or  counts_and_outcomes  function. countsy = counts(o, y)  Counts{Int64,1} over 5 outcomes \n  Outcome(1)   3397\n  Outcome(2)    644\n  Outcome(3)   1305\n  Outcome(4)   1996\n  Outcome(5)   2657 Counts are printed like  Probabilities : they display the outcomes they match to on the left marginal, but otherwise can be used as standard Julia numerical  Vector s. To go from outcomes to probabilities, we divide with the total: probsy = probabilities(o, y)\noutsy = outcomes(probsy)\nhcat(outsy, countsy, countsy ./ sum(countsy), probsy) 5×4 Matrix{Any}:\n [1, 2, 3]  3397  0.339734   0.339734\n [1, 3, 2]   644  0.0644064  0.0644064\n [2, 1, 3]  1305  0.130513   0.130513\n [2, 3, 1]  1996  0.19962    0.19962\n [3, 1, 2]  2657  0.265727   0.265727 By definition, columns 3 and 4 are identical. However, there are other ways to estimate probabilities that may account for biases in counting outcomes from finite data. Alternative estimators for probabilities are subtypes of  ProbabilitiesEstimator .  ProbabilitiesEstimator s  dictate alternative ways to estimate probabilities, given some outcome space and unput data. For example, one could use  BayesianRegularization . probsy_bayes = probabilities(BayesianRegularization(), o, y)\n\nprobsy_bayes .- probsy 5-element Vector{Float64}:\n -6.98390510782132e-5\n  6.776967180924243e-5\n  3.472958251440894e-5\n  1.8994302469765856e-7\n -3.285014627013583e-5 While the corrections of  BayesianRegularization  are small in this case, they are nevertheless measurable. When calling  probabilities  only with an outcome space instance and some input data (skipping the  ProbabilitiesEstimator ), then by default, the  RelativeAmount  probabilities estimator is used to extract the probabilities."},{"id":785,"pagetitle":"Tutorial","title":"Entropies","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Entropies","content":" Entropies Many compexity measures are a straightforward estimation of Shannon entropy, computed over probabilities estimated from data over some particular outcome space. For example, the well known  permutation entropy  ( Bandt and Pompe, 2002 ) is exactly the Shannon entropy of the probabilities  probsy  we computed above based on ordinal patterns. To compute it, we use the  entropy  function. perm_ent_x = entropy(OrdinalPatterns(), x)\nperm_ent_y = entropy(OrdinalPatterns(), y)\n(perm_ent_x, perm_ent_y) (2.58483416818717, 2.139506127185978) As expected, the permutation entropy of the  x  signal is higher, because the signal is \"more random\". Moreover, since we have estimated the probabilities already, we could have passed these to the entropy function directly instead of recomputing them as above perm_ent_y_2 = entropy(probsy) 2.139506127185978 We crucially realize here that many quantities in the NLTS literature that are named as entropies, such as \"permutation entropy\", are  not really new entropies . They are the good old Shannon entropy ( Shannon ), but calculated with  new outcome spaces  that smartly quantify some dynamic property in the data. Nevertheless, we acknowledge that names such as \"permutation entropy\" are commonplace, so in ComplexityMeasures.jl we provide convenience functions like  entropy_permutation . More convenience functions can be found in the  convenience  documentation page."},{"id":786,"pagetitle":"Tutorial","title":"Beyond Shannon: more entropies","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Beyond-Shannon:-more-entropies","content":" Beyond Shannon: more entropies Just like there are different outcome spaces, the same concept applies to entropy. There are many  fundamentally different  entropies. Shannon entropy is not the only one, just the one used most often. Each entropy is a subtype of  InformationMeasure . Another commonly used entropy is the  Renyi  or generalized entropy. We can use  Renyi  as an additional first argument to the  entropy  function to estimate the generalized entropy: perm_ent_y_q2 = entropy(Renyi(q = 2.0), OrdinalPatterns(), y)\n(perm_ent_y_q2, perm_ent_y) (2.0170680478970375, 2.139506127185978) In fact, when we called  entropy(OrdinalPatterns(), y) , this dispatched to the default call of  entropy(Shannon(), OrdinalPatterns(), y) ."},{"id":787,"pagetitle":"Tutorial","title":"Beyond entropies: discrete estimators","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Beyond-entropies:-discrete-estimators","content":" Beyond entropies: discrete estimators The estimation of an entropy truly parallelizes the estimation of probabilities: in the latter, we could decide an outcome space  and  an  estimator  to estimate probabilities. The same happens for entropy: we can decide an entropy definition and an  estimator  of how to estimate the entropy. For example, instead of the default  PlugIn  estimator that we used above implicitly, we could use the  Jackknife  estimator. ospace = OrdinalPatterns()\nentdef = Renyi(q = 2.0)\nentest = Jackknife(entdef)\nperm_ent_y_q2_jack = entropy(entest, ospace, y)\n\n(perm_ent_y_q2, perm_ent_y_q2_jack) (2.0170680478970375, 1.2173088128474774) Entropy estimators always reference an entropy definition, even if they only apply to one type of entropy (typically the Shannon one). From here, it is up to the researcher to read the documentation of the plethora of estimators implemented and decide what is most suitable for their data at hand. They all can be found in  DiscreteInfoEstimator ."},{"id":788,"pagetitle":"Tutorial","title":"Beyond entropies: other information measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Beyond-entropies:-other-information-measures","content":" Beyond entropies: other information measures Recall that at the very beginning of this notebook we mentioned a code separation of  information  and  complexity . We did this because there are other measures, besides entropy, that are explicit functionals of some probability mass function. One example is the Shannon  extropy ShannonExtropy , the complementary dual of  entropy , which could be computed as follows. extdef = ShannonExtropy()\nperm_ext_y = information(extdef, ospace, y) 1.2450281736700084 Just like the Shannon  entropy , the extropy could also be estimated with a different estimator such as  Jackknife . perm_ext_y_jack = information(Jackknife(extdef), ospace, y) 1.126229249906828 In truth, when we called  entropy(e, o, y)  it actually calls  information(e, o, y) , as all \"information measures\" are part of the same function interface. And entropy is an information measure."},{"id":789,"pagetitle":"Tutorial","title":"Beyond discrete: differential or continuous","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Beyond-discrete:-differential-or-continuous","content":" Beyond discrete: differential or continuous Discrete information measures are functions of probability mass functions. It is also possible to compute information measures of probability density functions. In ComplexityMeasures.jl, this is done by calling  entropy  (or the more general  information ) with a differential information estimator, a subtype of  DifferentialInfoEstimator . These estimators are given directly to  information  without assigning an outcome space, because the probability density is approximated implicitly, not explicitly. For example, the  Correa  estimator approximates the differential Shannon entropy by utilizing order statistics of the timeseries data: diffest = Correa()\ndiffent = entropy(diffest, x) 1.7816210045583265"},{"id":790,"pagetitle":"Tutorial","title":"Beyond  information : other complexity measures","ref":"/DynamicalSystemsDocs.jl/complexitymeasures/stable/tutorial/#Beyond-information:-other-complexity-measures","content":" Beyond  information : other complexity measures As discussed at the very beginning of this tutorial, there are some complexity measures that are not explicit functionals of probabilities, and hence cannot be straightforwardly related to an outcome space, in the sense of providing an instance of  OutcomeSpace  to the estimation function. These are estimated with the  complexity  function, by providing it a subtype of  ComplexityEstimator . An example here is the well-known  sample entropy  (which isn't actually an entropy in the formal mathematical sense). It can be computed as follows. complest = SampleEntropy(r = 0.1)\nsampent = complexity(complest, y) 0.612355215298896"},{"id":793,"pagetitle":"Documentation","title":"TimeseriesSurrogates.jl","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.jl","content":" TimeseriesSurrogates.jl TimeseriesSurrogates  is a Julia package for generating surrogate timeseries. It is part of  JuliaDynamics , a GitHub organization dedicated to creating high quality scientific software. If you are new to this method of surrogate timeseries, feel free to read the  Crash-course in timeseries surrogate testing  page. Please note that timeseries surrogates should not be confused with  surrogate models , such as those provided by  Surrogates.jl ."},{"id":794,"pagetitle":"Documentation","title":"Installation","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Installation","content":" Installation TimeseriesSurrogates.jl is a registered Julia package. To install the latest version, run the following code: import Pkg; Pkg.add(\"TimeseriesSurrogates\")"},{"id":795,"pagetitle":"Documentation","title":"API","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#API","content":" API TimeseriesSurrogates.jl API is composed by four names:  surrogate ,  surrogenerator ,  SurrogateTest , and  pvalue . They dispatch on the method to generate surrogates, which is a subtype of  Surrogate . It is recommended to standardize the signal before using these functions, i.e. subtract mean and divide by standard deviation. The function  standardize  does this."},{"id":796,"pagetitle":"Documentation","title":"Generating surrogates","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Generating-surrogates","content":" Generating surrogates"},{"id":797,"pagetitle":"Documentation","title":"TimeseriesSurrogates.surrogate","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.surrogate","content":" TimeseriesSurrogates.surrogate  —  Function surrogate(x, method::Surrogate [, rng]) → s Create a single surrogate timeseries  s  from  x  based on the given  method . If you want to generate multiple surrogates from  x , you should use  surrogenerator  for better performance. source"},{"id":798,"pagetitle":"Documentation","title":"TimeseriesSurrogates.surrogenerator","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.surrogenerator","content":" TimeseriesSurrogates.surrogenerator  —  Function surrogenerator(x, method::Surrogate [, rng]) → sgen::SurrogateGenerator Initialize a generator that creates surrogates of  x  on demand, based on the given  method . This is more efficient than  surrogate , because for most methods some things can be initialized and reused for every surrogate. Optionally you can provide an  rng::AbstractRNG  object that will control the random number generation and hence establish reproducibility of the generated surrogates. By default  Random.default_rng()  is used. The generated surrogates overwrite, in-place, a common vector container. Use  copy  if you need to actually store multiple surrogates. To generate a surrogate, call  sgen  as a function with no arguments, e.g.: sgen = surrogenerator(x, method)\ns = sgen() You can use the generator syntax of Julia to map over surrogates generated by  sg . For example, let  q  be a function returning a discriminatory statistic. To test some null hypothesis with TimeseriesSurrogates.jl you'd do using TimeseriesSurrogates\nq, x # inputs\nmethod = RandomFourier() # some example method\nsgen = surrogenerator(x, method)\nsiter = (sgen() for _ in 1:1000)\nqx = q(x)\nqs = map(q, siter)\n# compare `qx` with quantiles\nusing Statistics: quantile\nq01, q99 = quantile(qs, [0.01, 0.99])\nq01 ≤ qx ≤ q99 # if false, hypothesis can be rejected! source"},{"id":799,"pagetitle":"Documentation","title":"Hypothesis testing","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Hypothesis-testing","content":" Hypothesis testing"},{"id":800,"pagetitle":"Documentation","title":"TimeseriesSurrogates.SurrogateTest","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.SurrogateTest","content":" TimeseriesSurrogates.SurrogateTest  —  Type SurrogateTest(f::Function, x, method::Surrogate; kwargs...) → test Initialize a surrogate test for input data  x , which can be used in  pvalue . The tests requires as input a function  f  that given a timeseries (like  x ) it outputs a real number, and a method of how to generate surrogates.  f  is the function that computes the discriminatory statistic. Once called with  pvalue , the  test  estimates and then stores the real value  rval  and surrogate values  vals  of the discriminatory statistic in the fields  rval, vals  respectively. Alternatively, you can use  fill_surrogate_test!  directly if you don't care about the p-value. SurrogateTest  automates the process described in the documentation page  Performing surrogate hypothesis tests . SurrogateTest  subtypes  HypothesisTest  and is part of the StatsAPI.jl interface. Keywords rng = Random.default_rng() : a random number generator. n::Int = 10_000 : how many surrogates to generate and compute  f  on. threaded = true : Whether to parallelize looping over surrogate computations in  to the available threads ( Threads.nthreads() ). source"},{"id":801,"pagetitle":"Documentation","title":"TimeseriesSurrogates.fill_surrogate_test!","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.fill_surrogate_test!","content":" TimeseriesSurrogates.fill_surrogate_test!  —  Function fill_surrogate_test!(test::SurrgateTest) → rval, vals Perform the computations foreseen by  test  and return the value of the discriminatory statistic for the real data  rval  and the distribution of values for the surrogates  vals . This function is called by  pvalue . source"},{"id":802,"pagetitle":"Documentation","title":"StatsAPI.pvalue","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#StatsAPI.pvalue-Tuple{SurrogateTest}","content":" StatsAPI.pvalue  —  Method pvalue(test::SurrogateTest; tail = :left) Return the  p-value  corresponding to the given  SurrogateTest , optionally specifying what kind of tail test to do (one of  :left, :right, :both ). For  SurrogateTest , the p-value is simply the proportion of surrogate statistics that exceed (for  tail = :right ) or subseed ( tail = :left ) the discriminatory statistic computed from the input data. The default value of  tail  assumes that the surrogate data are expected to have higher discriminatory statistic values. This is the case for statistics that quantify entropy. For statistics that quantify autocorrelation, use  tail = :right  instead. source"},{"id":803,"pagetitle":"Documentation","title":"Surrogate methods","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Surrogate-methods","content":" Surrogate methods"},{"id":804,"pagetitle":"Documentation","title":"TimeseriesSurrogates.Surrogate","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.Surrogate","content":" TimeseriesSurrogates.Surrogate  —  Type Supertype of all surrogate methods. source TimeseriesSurrogates.AAFT TimeseriesSurrogates.AutoRegressive TimeseriesSurrogates.BlockShuffle TimeseriesSurrogates.CircShift TimeseriesSurrogates.CycleShuffle TimeseriesSurrogates.IAAFT TimeseriesSurrogates.IrregularLombScargle TimeseriesSurrogates.PartialRandomization TimeseriesSurrogates.PartialRandomizationAAFT TimeseriesSurrogates.PseudoPeriodic TimeseriesSurrogates.PseudoPeriodicTwin TimeseriesSurrogates.RandomCascade TimeseriesSurrogates.RandomFourier TimeseriesSurrogates.RandomShuffle TimeseriesSurrogates.RelativePartialRandomization TimeseriesSurrogates.RelativePartialRandomizationAAFT TimeseriesSurrogates.ShuffleDimensions TimeseriesSurrogates.SpectralPartialRandomization TimeseriesSurrogates.SpectralPartialRandomizationAAFT TimeseriesSurrogates.Surrogate TimeseriesSurrogates.SurrogateTest TimeseriesSurrogates.TAAFT TimeseriesSurrogates.TFTD TimeseriesSurrogates.TFTDAAFT TimeseriesSurrogates.TFTDIAAFT TimeseriesSurrogates.TFTDRandomFourier TimeseriesSurrogates.TFTS TimeseriesSurrogates.WLS"},{"id":805,"pagetitle":"Documentation","title":"Shuffle-based","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Shuffle-based","content":" Shuffle-based"},{"id":806,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RandomShuffle","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.RandomShuffle","content":" TimeseriesSurrogates.RandomShuffle  —  Type RandomShuffle() <: Surrogate A random constrained surrogate, generated by shifting values around. Random shuffle surrogates preserve the mean, variance and amplitude  distribution of the original signal. Properties not preserved are  any  temporal information , such as the power spectrum and hence linear  correlations.  The null hypothesis this method can test for is whether the data  are uncorrelated noise, possibly measured via a nonlinear function. Specifically, random shuffle surrogate can test  the null hypothesis that the original signal is produced by independent and  identically distributed random variables[^Theiler1991, ^Lancaster2018].  Beware: random shuffle surrogates do not cover the case of correlated noise [Lancaster2018] .  source"},{"id":807,"pagetitle":"Documentation","title":"TimeseriesSurrogates.BlockShuffle","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.BlockShuffle","content":" TimeseriesSurrogates.BlockShuffle  —  Type BlockShuffle(n::Int; shift = false) A block shuffle surrogate constructed by dividing the time series into  n  blocks of roughly equal width at random indices (end blocks are wrapped around to the start of the time series). If  shift  is  true , then the input signal is circularly shifted by a  random number of steps prior to picking blocks. Block shuffle surrogates roughly preserve short-range temporal properties in the time series (e.g. correlations at lags less than the block length), but break any long-term dynamical information (e.g. correlations beyond the block length). Hence, these surrogates can be used to test any null hypothesis aimed at comparing short-range dynamical properties versus long-range dynamical properties of the signal. source"},{"id":808,"pagetitle":"Documentation","title":"TimeseriesSurrogates.CycleShuffle","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.CycleShuffle","content":" TimeseriesSurrogates.CycleShuffle  —  Type CycleShuffle(n::Int = 7, σ = 0.5) Cycle shuffled surrogates [Theiler1994]  that identify successive local peaks in the data and shuffle the cycles in-between the peaks. Similar to  BlockShuffle , but here the \"blocks\" are defined as follows: The timeseries is smoothened via convolution with a Gaussian ( DSP.gaussian(n, σ) ). Local maxima of the smoothened signal define the peaks, and thus the blocks in between them. The first and last index of timeseries can never be peaks and thus signals that should have peaks very close to start or end of the timeseries may not perform well. In addition, points before the first or after the last peak are never shuffled. The defined blocks are randomly shuffled as in  BlockShuffle . CSS are used to test the null hypothesis that the signal is generated by a periodic oscillator with no dynamical correlation between cycles, i.e. the evolution of cycles is not deterministic. See also  PseudoPeriodic . source"},{"id":809,"pagetitle":"Documentation","title":"TimeseriesSurrogates.CircShift","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.CircShift","content":" TimeseriesSurrogates.CircShift  —  Type CircShift(n) Surrogates that are circularly shifted versions of the original timeseries. n  can be an integer (the surrogate is the original time series shifted  by  n  indices), or any vector of integers, which which means that each  surrogate is shifted by an integer selected randomly among the entries in  n . source"},{"id":810,"pagetitle":"Documentation","title":"Fourier-based","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Fourier-based","content":" Fourier-based"},{"id":811,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RandomFourier","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.RandomFourier","content":" TimeseriesSurrogates.RandomFourier  —  Type RandomFourier(phases = true) A surrogate that randomizes the Fourier components of the signal in some manner. If  phases==true , the phases are randomized, otherwise the amplitudes are randomized.  FT  is an alias for  RandomFourier . Random Fourier phase surrogates [Theiler1991]  preserve the autocorrelation function, or power spectrum, of the original signal. Random Fourier amplitude surrogates preserve the mean and autocorrelation function but do not preserve the variance of the original. Random amplitude surrogates are not common in the literature, but are provided for convenience. Random phase surrogates can be used to test the null hypothesis that the original signal was produced by a linear Gaussian process  [Theiler1991] . source"},{"id":812,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTDRandomFourier","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTDRandomFourier","content":" TimeseriesSurrogates.TFTDRandomFourier  —  Type TFTD(phases::Bool = true, fϵ = 0.05) The  TFTDRandomFourier  (or just  TFTD  for short) surrogate was proposed by Lucio et al. (2012) [Lucio2012]  as a combination of truncated Fourier surrogates [Nakamura2006]  ( TFTS ) and detrend-retrend surrogates. The  TFTD  part of the name comes from the fact that it uses a combination of truncated Fourier transforms (TFT) and de-trending and re-trending (D) the time series before and after surrogate generation. Hence, it can be used to generate surrogates also from (strongly) nonstationary time series. Implementation details Here, a best-fit linear trend is removed/added from the signal prior to and after generating the random Fourier signal. In principle, any trend can be removed, but so far, we only provide the linear option. See also:  TFTDAAFT ,  TFTDIAAFT . source"},{"id":813,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PartialRandomization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.PartialRandomization","content":" TimeseriesSurrogates.PartialRandomization  —  Type PartialRandomization(α = 0.5) PartialRandomization  surrogates [Ortega1998]  are similar to  RandomFourier  phase surrogates, but during the phase randomization step, instead of drawing phases from  [0, 2π] , phases are drawn from  [0, 2π]*α , where  α ∈ [0, 1] . The authors refers to  α  as the \"degree\" of phase randomization, where  α = 0  means  0 %  randomization and  α = 1  means  100 %  randomization. See  RelativePartialRandomization  and  SpectralPartialRandomization  for alternative partial-randomization algorithms source"},{"id":814,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PartialRandomizationAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.PartialRandomizationAAFT","content":" TimeseriesSurrogates.PartialRandomizationAAFT  —  Type PartialRandomizationAAFT(α = 0.5) PartialRandomizationAAFF  surrogates are similar to  PartialRandomization  surrogates [Ortega1998] , but adds a rescaling step, so that the surrogate has the same values as the original time series (analogous to the rescaling done for  AAFT  surrogates). Partial randomization surrogates have, to the package authors' knowledge, not been published in scientific literature. source"},{"id":815,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RelativePartialRandomization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.RelativePartialRandomization","content":" TimeseriesSurrogates.RelativePartialRandomization  —  Type RelativePartialRandomization(α = 0.5) RelativePartialRandomization  surrogates are similar to  PartialRandomization  phase surrogates, but instead of drawing phases uniformly from  [0, 2π] , phases are drawn from  ϕ + [0, 2π]*α , where  α ∈ [0, 1]  and  ϕ  is the original Fourier phase. See the documentation for a detailed comparison between partial randomization algorithms. source"},{"id":816,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RelativePartialRandomizationAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.RelativePartialRandomizationAAFT","content":" TimeseriesSurrogates.RelativePartialRandomizationAAFT  —  Type RelativePartialRandomizationAAFT(α = 0.5) RelativePartialRandomizationAAFT  surrogates are similar to  RelativePartialRandomization  surrogates, but add a rescaling step, so that the surrogate has the same values as the original time series (analogous to the rescaling done for  AAFT  surrogates). source"},{"id":817,"pagetitle":"Documentation","title":"TimeseriesSurrogates.SpectralPartialRandomization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.SpectralPartialRandomization","content":" TimeseriesSurrogates.SpectralPartialRandomization  —  Type SpectralSpectralPartialRandomization(α = 0.5) SpectralPartialRandomization  surrogates are similar to  PartialRandomization  phase surrogates, but instead of drawing phases uniformly from  [0, 2π] , phases of the highest frequency components responsible for a proportion  α  of power are replaced by random phases drawn from  [0, 2π] See the documentation for a detailed comparison between partial randomization algorithms. source"},{"id":818,"pagetitle":"Documentation","title":"TimeseriesSurrogates.SpectralPartialRandomizationAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.SpectralPartialRandomizationAAFT","content":" TimeseriesSurrogates.SpectralPartialRandomizationAAFT  —  Type SpectralPartialRandomizationAAFT(α = 0.5) SpectralPartialRandomizationAAFT  surrogates are similar to  PartialRandomization  surrogates, but add a rescaling step, so that the surrogate has the same values as the original time series (analogous to the rescaling done for  AAFT  surrogates). source"},{"id":819,"pagetitle":"Documentation","title":"TimeseriesSurrogates.AAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.AAFT","content":" TimeseriesSurrogates.AAFT  —  Type AAFT() An amplitude-adjusted-fourier-transform (AAFT) surrogate [Theiler1991] . AAFT surrogates have the same linear correlation, or periodogram, and also preserves the amplitude distribution of the original data. AAFT surrogates can be used to test the null hypothesis that the data come from a monotonic nonlinear transformation of a linear Gaussian process (also called integrated white noise) [Theiler1991] . source"},{"id":820,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.TAAFT","content":" TimeseriesSurrogates.TAAFT  —  Type TAAFT(fϵ) An truncated version of the amplitude-adjusted-fourier-transform surrogate [Theiler1991] [Nakamura2006] . The truncation parameter and phase randomization procedure is identical to  TFTS , but here an additional step of rescaling back to the original data is performed. This preserves the amplitude distribution of the original data. source"},{"id":821,"pagetitle":"Documentation","title":"TimeseriesSurrogates.IAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.IAAFT","content":" TimeseriesSurrogates.IAAFT  —  Type IAAFT(M = 100, tol = 1e-6, W = 75) An iteratively adjusted amplitude-adjusted-fourier-transform surrogate [SchreiberSchmitz1996] . IAAFT surrogates have the same linear correlation, or periodogram, and also preserves the amplitude distribution of the original data, but are improved relative to AAFT through iterative adjustment (which runs for a maximum of  M  steps). During the iterative adjustment, the periodograms of the original signal and the surrogate are coarse-grained and the powers are averaged over  W  equal-width frequency bins. The iteration procedure ends when the relative deviation between the periodograms is less than  tol  (or when  M  is reached). IAAFT, just as AAFT, can be used to test the null hypothesis that the data come from a monotonic nonlinear transformation of a linear Gaussian process. source"},{"id":822,"pagetitle":"Documentation","title":"Non-stationary","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Non-stationary","content":" Non-stationary"},{"id":823,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTS","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTS","content":" TimeseriesSurrogates.TFTS  —  Type TFTS(fϵ::Real) A truncated Fourier transform surrogate [Nakamura2006]  (TFTS). TFTS surrogates are generated by leaving some frequencies untouched when performing the phase shuffling step (as opposed to randomizing all frequencies, like for  RandomFourier  surrogates). These surrogates were designed to deal with data with irregular fluctuations superimposed over long term trends (by preserving low frequencies) [Nakamura2006] . Hence, TFTS surrogates can be used to test the null hypothesis that the signal is a stationary linear system generated the irregular fluctuations part of the signal [Nakamura2006] . Controlling the truncation of the spectrum The truncation parameter  fϵ ∈ [-1, 0) ∪ (0, 1]  controls which parts of the spectrum are preserved. If  fϵ > 0 , then  fϵ  indicates the ratio of high frequency domain to the entire frequency domain.   For example,  fϵ = 0.5  preserves 50% of the frequency domain (randomizing the higher   frequencies, leaving low frequencies intact). If  fϵ < 0 , then  fϵ  indicates ratio of low frequency domain to the entire frequency domain.   For example,  fϵ = -0.2  preserves 20% of the frequency domain (leaving higher frequencies intact,   randomizing the lower frequencies). If  fϵ ± 1 , then all frequencies are randomized. The method is then equivalent to    RandomFourier . The appropriate value of  fϵ  strongly depends on the data and time series length, and must be manually determined [Nakamura2006] , for example by comparing periodograms for the time series and the surrogates. source"},{"id":824,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTD","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTD","content":" TimeseriesSurrogates.TFTD  —  Type TFTD(phases::Bool = true, fϵ = 0.05) The  TFTDRandomFourier  (or just  TFTD  for short) surrogate was proposed by Lucio et al. (2012) [Lucio2012]  as a combination of truncated Fourier surrogates [Nakamura2006]  ( TFTS ) and detrend-retrend surrogates. The  TFTD  part of the name comes from the fact that it uses a combination of truncated Fourier transforms (TFT) and de-trending and re-trending (D) the time series before and after surrogate generation. Hence, it can be used to generate surrogates also from (strongly) nonstationary time series. Implementation details Here, a best-fit linear trend is removed/added from the signal prior to and after generating the random Fourier signal. In principle, any trend can be removed, but so far, we only provide the linear option. See also:  TFTDAAFT ,  TFTDIAAFT . source"},{"id":825,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTDAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTDAAFT","content":" TimeseriesSurrogates.TFTDAAFT  —  Type TFTDAAFT(fϵ = 0.05) TFTDAAFT [Lucio2012]  are similar to  TFTD  surrogates, but also re-scales back to the original values of the time series.  fϵ ∈ (0, 1]  is the fraction of the powerspectrum corresponding to the lowermost frequencies to be preserved. See also:  TFTD ,  TFTDIAAFT . source"},{"id":826,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTDIAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTDIAAFT","content":" TimeseriesSurrogates.TFTDIAAFT  —  Type TFTDIAAFT(fϵ = 0.05; M::Int = 100, tol::Real = 1e-6, W::Int = 75) TFTDIAAFT [Lucio2012]  are similar to  TFTDAAFT , but adds an iterative procedure to better match the periodograms of the surrogate and the original time series, analogously to how  IAAFT  improves upon  AAFT . fϵ ∈ (0, 1]  is the fraction of the powerspectrum corresponding to the lowermost frequencies to be preserved.  M  is the maximum number of iterations.  tol  is the desired maximum relative tolerance between power spectra.  W  is the number of bins into which the periodograms are binned when comparing across iterations. See also:  TFTD ,  TFTDAAFT . source"},{"id":827,"pagetitle":"Documentation","title":"Pseudo-periodic","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Pseudo-periodic","content":" Pseudo-periodic"},{"id":828,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PseudoPeriodic","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.PseudoPeriodic","content":" TimeseriesSurrogates.PseudoPeriodic  —  Type PseudoPeriodic(d, τ, ρ, shift = true) Create surrogates suitable for pseudo-periodic signals. They retain the periodic structure of the signal, while inter-cycle dynamics that are either deterministic or correlated noise are destroyed (for appropriate  ρ  choice). Therefore these surrogates are suitable to test the null hypothesis that the signal is a periodic orbit with uncorrelated noise [Small2001] . Arguments  d, τ, ρ  are as in the paper, the embedding dimension, delay time and noise radius. The method works by performing a delay coordinates embedding from DelayEmbeddings.jl (see that docs for choosing appropriate  d, τ ). For  ρ , we have implemented the method proposed in the paper in the function  noiseradius . The argument  shift  is not discussed in the paper. If  shift=false  we adjust the algorithm so that there is little phase shift between the periodic component of the original and surrogate data. See also  CycleShuffle . source"},{"id":829,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PseudoPeriodicTwin","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.PseudoPeriodicTwin","content":" TimeseriesSurrogates.PseudoPeriodicTwin  —  Type PseudoPeriodicTwin(d::Int, τ::Int, δ = 0.2, ρ = 0.1, metric = Euclidean())\nPseudoPeriodicTwin(δ = 0.2, ρ = 0.1, metric = Euclidean()) A pseudoperiodic twin surrogate [Miralles2015] , which is a fusion of the twin surrogate [Thiel2006]  and the pseudo-periodic surrogate [Small2001] . Input parameters A delay reconstruction of the input timeseries is constructed using embedding dimension  d  and embedding delay  τ . The threshold  δ ∈ (0, 1]  determines which points are \"close\" (neighbors) or not, and is expressed as a fraction of the attractor diameter, as determined by the input data. The authors of the original twin surrogate paper recommend  0.05 ≤ δ ≤ 0.2 [Thiel2006] . If you have pre-embedded your timeseries, and timeseries is already a  ::StateSpaceSet , use the three-argument constructor (so that no delay reconstruction is performed). If you want a surrogate for a scalar-valued timeseries, use the five-argument constructor to also provide the embedding delay  τ  and embedding dimension  d . Null hypothesis Pseudo-periodic twin surrogates generate signals similar to the original data if the original signal is (quasi-)periodic. If the original signal is not (quasi-)periodic, then these surrogates will have different recurrence plots than the original signal, but preserve the overall shape of the attractor. Thus,  PseudoPeriodicTwin  surrogates can be used to test null hypothesis that the observed timeseries (or orbit) is consistent with a quasi-periodic orbit [Miralles2015] . Returns A  d -dimensional surrogate orbit (a  StateSpaceSet ) is returned. Sample the first column of this dataset if a scalar-valued surrogate is desired. source"},{"id":830,"pagetitle":"Documentation","title":"Wavelet-based","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Wavelet-based","content":" Wavelet-based"},{"id":831,"pagetitle":"Documentation","title":"TimeseriesSurrogates.WLS","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.WLS","content":" TimeseriesSurrogates.WLS  —  Type WLS(shufflemethod::Surrogate = IAAFT();\n    f::Union{Nothing, Function} = Statistics.cor,\n    rescale::Bool = true,\n    wt::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{16}()) A wavelet surrogate generated by the following procedure: Compute the wavelet transform of the signal. This results in a set of   detail coefficients over a set of dyadic scales. As in Keylock (2006),   we here use the maximal overlap discrete wavelet transform, or MODWT,  so that the number of coefficients at each scale are the same. Shuffle the detail coefficients at each dyadic scale using the   provided  shufflemethod . See \"Shuffling methods\" below for alternatives. Apply the inverse wavelet transform to the shuffled detail coefficients   to obtain a surrogate time series. Shuffling methods You may choose to use any surrogate from this package to perform the  randomization of the detail coefficients at each dyadic scale. The following methods have been discussed in the literature (more may exist):  Random permutations of wavelet coefficients within each scale (Breakspear et al., 2003). To get this behaviour, use  WLS(x, RandomShuffle(), rescale = false, f = nothing) . Cyclic rotation of wavelet coefficients within each scale (Breakspear et al., 2003).  To get this behaviour, use  WLS(x, Circshift(1:length(x)), rescale = false, f = nothing) . Block resampling of wavelet coefficients within each scale (Breakspear et al., 2003). To get this behaviour, use  WLS(x, BlockShuffle(nblocks, randomize = true), rescale = false, f = nothing) . IAAFT resampling of wavelet coefficients within each scale (Keylock, 2006). To get this behaviour, use  WLS(x, IAAFT(), rescale = true, f = Statistics.cor) .   This method preserves the local mean and variance structure of the signal, but    randomises nonlinear properties of the signal (i.e. Hurst exponents) [Keylock2006] .    These surrogates can therefore be used to test for changes in nonlinear properties    of the original signal. In contrast to IAAFT surrogates, the IAAFT-wavelet surrogates    also preserves nonstationarity. Using other  shufflemethod s does not necessarily   preserve nonstationarity. To deal with nonstationary signals, Keylock (2006) recommends    using a wavelet with a high number of vanishing moments. Thus, our default is to   use a Daubechies wavelet with 16 vanishing moments.  Note: The iterative procedure after    the rank ordering step (step [v] in  [Keylock2006] ) is not performed in    this implementation. The default method and parameters replicate the behaviour of Keylock (2006)'s IAAFT  wavelet surrogates. Error minimization For the  IAAFT  approach introduced in Keylock (2006), detail coefficients  at each level are circularly rotated to minimize an error function. The methods  introduced in Breakspear et al. (2003) do not apply this error minimization. In our implementation, you can turn this option on/off using the  f  parameter of  the  WLS  constructor. If  f = nothing  turns off error minization. If  f  is set  to a two-argument function that computes some statistic, for example   f = Statistics.cor , then detail coefficients at each scale are circularly  rotated until that function is maximized (and hence the \"error\" minimized).  If you want to  minimize  some error function, then instead provide an appropriate  transform of your function. For example, if using the root mean squared deviation, define  rmsd_inv(x, y) = 1 - StatsBase.rmsd(x, y)  and set  f = rmsd_inv . Rescaling If  rescale == true , then surrogate values are mapped onto the  values of the original time series, as in the  AAFT  algorithm. If  rescale == false , surrogate values are not constrained to the  original time series values. If  AAFT  or  IAAFT  shuffling  is used,  rescale  should be set to  true . For other methods, it does not  necessarily need to be. source"},{"id":832,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RandomCascade","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.RandomCascade","content":" TimeseriesSurrogates.RandomCascade  —  Type RandomCascade(paddingmode::String = \"zeros\") A random cascade multifractal wavelet surrogate (Paluš, 2008) [Paluš2008] . If the input signal length is not a power of 2, the signal must be  padded before the surrogate is constructed.  paddingmode  determines  how the signal is padded. Currently supported padding modes:  \"zeros\" . The final surrogate (constructed from the padded signal) is subset to match the length of the original signal. Random cascade surrogate preserve multifractal properties of the input  time series, that is, interactions among dyadic scales and nonlinear  dependencies [Paluš2008] . source"},{"id":833,"pagetitle":"Documentation","title":"Other","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Other","content":" Other"},{"id":834,"pagetitle":"Documentation","title":"TimeseriesSurrogates.AutoRegressive","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.AutoRegressive","content":" TimeseriesSurrogates.AutoRegressive  —  Type AutoRegressive(n, method = LPCLevinson()) Autoregressive surrogates of order- n . The autoregressive coefficients  φ  are estimated using  DSP.lpc(x, n, method) , and thus see the documentation of DSP.jl for possible  method s. While these surrogates are obviously suited to test the null hypothesis whether the data are coming from a autoregressive process, the Fourier Transform-based surrogates are probably a better option. The current method is more like an explicit way to produce surrogates for the same hypothesis by fitting a model. It can be used as a convenient way to estimate autoregressive coefficients and automatically generate surrogates based on them. The coefficients φ of the autoregressive fit can be found by doing sg = surrogenerator(x, AutoRegressive(n))\nφ = sg.init.φ source"},{"id":835,"pagetitle":"Documentation","title":"TimeseriesSurrogates.ShuffleDimensions","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.ShuffleDimensions","content":" TimeseriesSurrogates.ShuffleDimensions  —  Type ShuffleDimensions() Multidimensional surrogates of input  StateSpaceSet s from StateSpaceSets.jl. Each point in the set is individually shuffled, but the points themselves are not shuffled. These surrogates destroy the state space structure of the dataset and are thus suited to distinguish deterministic datasets from high dimensional noise. source"},{"id":836,"pagetitle":"Documentation","title":"TimeseriesSurrogates.IrregularLombScargle","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.IrregularLombScargle","content":" TimeseriesSurrogates.IrregularLombScargle  —  Type IrregularLombScargle(t; tol = 1, n_total = 100000, n_acc = 50000, q = 1) IrregularLombScargle  surrogates for unevenly sampled time series  with supporting time steps  t , generated using the simulated annealing algorithm  described in  [SchreiberSchmitz1999] . IrregularLombScargle  surrogates (given enough iterations and a low enough  tolerance) preserve the periodogram and the amplitude  distribution of the original signal. For time series with equidistant time steps,  surrogates generated by this method result in surrogates similar to those produced  by the  IAAFT  method. This algorithm starts with a random permutation of the original data. Then it iteratively  approaches the power spectrum of the original data by swapping two randomly selected values  in the surrogate data if the Minkowski distance of order  q  between the power spectrum of  the surrogate data and the original data is less than before. The iteration procedure ends  when the relative deviation between the periodograms is less than  tol  or when  n_total   number of tries or  n_acc  number of actual swaps is reached. source"},{"id":837,"pagetitle":"Documentation","title":"Utilities","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Utilities","content":" Utilities"},{"id":838,"pagetitle":"Documentation","title":"TimeseriesSurrogates.noiseradius","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#TimeseriesSurrogates.noiseradius","content":" TimeseriesSurrogates.noiseradius  —  Function noiseradius(x::AbstractVector, d::Int, τ, ρs, n = 1) → ρ Use the proposed* algorithm of [Small2001]  to estimate optimal  ρ  value for  PseudoPeriodic  surrogates, where  ρs  is a vector of possible  ρ  values. *The paper is ambiguous about exactly what to calculate. Here we count how many times we have pairs of length-2 that are identical in  x  and its surrogate, but  are not  also part of pairs of length-3. This function directly returns the arg-maximum of the evaluated distribution of these counts versus  ρ , use  TimeseriesSurrogates._noiseradius  with same arguments to get the actual distribution.  n  means to repeat τhe evaluation  n  times, which increases accuracy. source"},{"id":839,"pagetitle":"Documentation","title":"Visualization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Visualization","content":" Visualization TimeseriesSurrogates.jl has defined a simple function  surroplot(x, s) . This comes into scope when  using Makie  (you also need a plotting backend). This functionality requires you to be using Julia 1.9 or later versions. Example: using TimeseriesSurrogates\nusing CairoMakie\nx = AR1() # create a realization of a random AR(1) process\nfig = surroplot(x, AAFT()) CairoMakie.Screen{IMAGE}\n"},{"id":840,"pagetitle":"Documentation","title":"Citing","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/#Citing","content":" Citing Please use the following BiBTeX entry, or DOI, to cite TimeseriesSurrogates.jl: DOI: https://doi.org/10.21105/joss.04414 BiBTeX: @article{TimeseriesSurrogates.jl,\n    doi = {10.21105/joss.04414},\n    url = {https://doi.org/10.21105/joss.04414},\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {77},\n    pages = {4414},\n    author = {Kristian Agasøster Haaga and George Datseris},\n    title = {TimeseriesSurrogates.jl: a Julia package for generating surrogate data},\n    journal = {Journal of Open Source Software}\n} Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Theiler1994 J. Theiler, On the evidence for low-dimensional chaos in an epileptic electroencephalogram,  Phys. Lett. A 196 Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Ortega1998 Ortega, Guillermo J.; Louis, Enrique (1998). Smoothness Implies Determinism in Time Series: A Measure Based Approach. Physical Review Letters, 81(20), 4345–4348. doi:10.1103/PhysRevLett.81.4345 Ortega1998 Ortega, Guillermo J.; Louis, Enrique (1998). Smoothness Implies Determinism in Time Series: A Measure Based Approach. Physical Review Letters, 81(20), 4345–4348. doi:10.1103/PhysRevLett.81.4345 Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. SchreiberSchmitz1996 T. Schreiber; A. Schmitz (1996). \"Improved Surrogate Data for Nonlinearity Tests\".  Phys. Rev. Lett. 77 (4) Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Small2001 Small et al., Surrogate test for pseudoperiodic time series data,  Physical Review Letters, 87(18) Small2001 Small et al., Surrogate test for pseudoperiodic timeseries data,  Physical Review Letters, 87(18) Thiel2006 Thiel, Marco, et al. \"Twin surrogates to test for complex synchronisation.\" EPL (Europhysics Letters) 75.4 (2006): 535. Miralles2015 Miralles, R., et al. \"Characterization of the complexity in short oscillating timeseries: An application to seismic airgun detonations.\" The Journal of the Acoustical Society of America 138.3 (2015): 1595-1603. Breakspear2003 Breakspear, M., Brammer, M., & Robinson, P. A. (2003). Construction of multivariate surrogate sets from nonlinear data using the wavelet transform. Physica D: Nonlinear Phenomena, 182(1-2), 1-22. Keylock2006 C.J. Keylock (2006). \"Constrained surrogate time series with preservation of the mean and variance structure\". Phys. Rev. E. 73: 036707. doi:10.1103/PhysRevE.73.036707. Paluš2008 Paluš, Milan (2008). Bootstrapping Multifractals: Surrogate Data from Random Cascades on Wavelet Dyadic Trees. Physical Review Letters, 101(13), 134101–. doi:10.1103/PhysRevLett.101.134101 SchmitzSchreiber1999 A.Schmitz T.Schreiber (1999). \"Testing for nonlinearity in unevenly sampled time series\"  Phys. Rev E Small2001 Small et al., Surrogate test for pseudoperiodic time series data,  Physical Review Letters, 87(18)"},{"id":843,"pagetitle":"Surrogates for irregular timeseries","title":"Surrogates for unevenly sampled time series","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/irregular_surrogates/#Surrogates-for-unevenly-sampled-time-series","content":" Surrogates for unevenly sampled time series To derive a surrogate for unevenly sampled time series, we can use surrogate methods which which does not explicitly use the time axis like  RandomShuffle  or  BlockShuffle , or we need to use algorithms that take the irregularity of the time axis into account."},{"id":844,"pagetitle":"Surrogates for irregular timeseries","title":"Lomb-Scargle based surrogate","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/irregular_surrogates/#Lomb-Scargle-based-surrogate","content":" Lomb-Scargle based surrogate The  IrregularLombScargle  surrogate is a form of a constrained surrogate which takes the Lomb-Scargle periodogram, which works on irregularly spaced data, to derive surrogates with similar phase distribution as the original time series. This function uses the simulated annealing algorithm [SchmitzSchreiber1999]  to minimize the Minkowski distance between the original periodogram and the surrogate periodogram. using TimeseriesSurrogates, CairoMakie, Random\n\n# Example data: random AR1 process with a time axis with unevenly\n# spaced time steps\nrng = Random.MersenneTwister(1234)\nx = AR1(n_steps = 300)\nN = length(x)\nt = (1:N) - rand(N)\n\n# Use simulated annealing based on convergence of Lomb-Scargle periodograms\n# The time series is relatively long, so set tolerance a bit higher than default.\nls = IrregularLombScargle(t, n_total = 100000, n_acc = 50000, tol = 5.0)\ns = surrogate(x, ls, rng)\n\nfig, ax = lines(t, x; label = \"original\")\nlines!(ax, t, s; label = \"surrogate\")\naxislegend(ax)\nfig SchmitzSchreiber1999 A.Schmitz T.Schreiber (1999). \"Testing for nonlinearity in unevenly sampled time series\"  Phys. Rev E"},{"id":847,"pagetitle":"Surrogates for nonstationary timeseries","title":"Surrogates for nonstationary time series","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#Surrogates-for-nonstationary-time-series","content":" Surrogates for nonstationary time series Several of the methods provided by TimeseriesSurrogates.jl can be used to  construct surrogates for nonstationary time series, which the following examples illustrate."},{"id":848,"pagetitle":"Surrogates for nonstationary timeseries","title":"Truncated Fourier surrogates","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#Truncated-Fourier-surrogates","content":" Truncated Fourier surrogates"},{"id":849,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTS","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTS](@ref)","content":" TFTS By retaining the lowermost frequencies of the frequency spectrum,  ( TFTS ) surrogates preserve long-term trends in the signals. using TimeseriesSurrogates\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Preserve 5 % lowermost frequencies.\nsurroplot(x, surrogate(x, TFTS(0.05)))"},{"id":850,"pagetitle":"Surrogates for nonstationary timeseries","title":"TAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TAAFT](@ref)","content":" TAAFT Truncated AAFT surrogates ( TAAFT ) are similar to TFTS surrogates, but also rescales back to the original values of the signal, so that the original signal and the surrogates consists of the same values. This, however, may introduce some bias, as demonstrated below. using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Preserve 5% of the power spectrum corresponding to the lowest frequencies\ns_taaft_lo = surrogate(x, TAAFT(0.05))\nsurroplot(x, s_taaft_lo) using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Preserve 20% of the power spectrum corresponding to the highest frequencies\ns_taaft_hi = surrogate(x, TAAFT(-0.2))\nsurroplot(x, s_taaft_hi)"},{"id":851,"pagetitle":"Surrogates for nonstationary timeseries","title":"Truncated FT surrogates with trend removal/addition","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#Truncated-FT-surrogates-with-trend-removal/addition","content":" Truncated FT surrogates with trend removal/addition One solution is to combine truncated Fourier surrogates with detrending/retrending.  For time series with strong trends, Lucio et al. (2012) [Lucio2012]  proposes variants  of the truncated Fourier-based surrogates wherein the trend is removed prior to surrogate generation, and then added to the surrogate again after it has been generated.  This yields surrogates quite similar to those obtained when using truncated Fourier  surrogates (e.g.  TFTS ), but reducing the effects of endpoint mismatch that  affects regular truncated Fourier transform based surrogates. In principle, any trend could be removed/added to the signal. For now, the only  option is to remove a best-fit linear trend obtained by ordinary least squares  regression."},{"id":852,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTD","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTD](@ref)","content":" TFTD The  TFTD  surrogate is a random Fourier surrogate where  the lowest frequencies are preserved during surrogate generation, and a  linear trend is removed during preprosessing and added again after the  surrogate has been generated. The  TFTD  surrogates do a decent  job at preserving long term trends. using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\ns = surrogate(x, TFTDRandomFourier(true, 0.02))\nsurroplot(x, s)"},{"id":853,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTDAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTDAAFT](@ref)","content":" TFTDAAFT The detrend-retrend extension of  TAAFT  is the  TFTDAAFT  method. The  TFTDAAFT  method adds a rescaling step to the  TFTD  method, ensuring that the surrogate and the original time series consist of the same values. Long-term trends in the data are also decently preserved by  TFTDAAFT , but like  TFTDAAFT , there is some bias. using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Keep 2 % of lowermost frequencies.\ns = surrogate(x, TFTDAAFT(0.02))\nsurroplot(x, s)"},{"id":854,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTDIAAFT","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTDIAAFT](@ref)","content":" TFTDIAAFT TFTDIAAFT [Lucio2012]  surrogates are similar to  TFTDAAFT  surrogates, but the  TFTDIAAFT [Lucio2012]  method also uses an iterative process to better match the power spectra of the original signal and the surrogate (analogous to how the  IAAFT  method improves upon the  AAFT  method). using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Keep 5% of lowermost frequences\ns = surrogate(x, TFTDIAAFT(0.05))\nsurroplot(x, s) Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202."},{"id":857,"pagetitle":"Contributing","title":"Contributing","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/contributor_guide/#Contributing","content":" Contributing"},{"id":858,"pagetitle":"Contributing","title":"Reporting issues","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/contributor_guide/#Reporting-issues","content":" Reporting issues If you are having issues with the code, find bugs or otherwise want to report something about the package, please submit an issue at our  GitHub repository . "},{"id":859,"pagetitle":"Contributing","title":"Feature requests","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/contributor_guide/#Feature-requests","content":" Feature requests If you have requests for a new method but can't implement it yourself, you can also report it as an  issue . The package developers or other volunteers might be able to help with the implementation.  Please mark method requests clearly as \"Method request: my new method...\", and provide a reference to a scientific publication that outlines the algorithm. "},{"id":860,"pagetitle":"Contributing","title":"Pull requests","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/contributor_guide/#Pull-requests","content":" Pull requests Pull requests for new surrogate methods are very welcome. Ideally, your implementation should use the same API as the existing methods:  Create a  struct  for your surrogate method, e.g.  struct MyNewSurrogateMethod <: Surrogate , that contain the parameters for the method. The docstring for the method should contain a reference to scientific publications detailing the algorithm, as well as the intended purpose of the method, and potential implementation details that differ from the original algorithm.  Implement  surrogenerator(x, method::MyNewSurrogateMethod) , where you pre-compute things for efficiency and return a  SurrogateGenerator  instance. Implement the  SurrogateGenerator{<:MyNewSurrogateMethod}  functor that produces surrogate time series on demand. This is where the precomputed things are used, and the actual algorithm is implemented. Then  surrogate(x, method::Surrogate)  will \"just work\".  If you find this approach difficult and already have a basic implementation of a new surrogate method, the package maintainers may be able to help structuring the code. Let us know in an  issue  or in a pull request!"},{"id":863,"pagetitle":"Utility systems","title":"Utility systems","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/exampleprocesses/#Utility-systems","content":" Utility systems"},{"id":864,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.SNLST","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.SNLST","content":" TimeseriesSurrogates.SNLST  —  Function SNLST(n_steps, x₀, k) Dynamically linear process transformed by a strongly nonlinear static transformation (SNLST) [1] . Equations The system is by the following map: \\[x(t) = k x(t-1) + a(t)\\] with the transformation  $s(t) = x(t)^3$ . References source"},{"id":865,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.randomwalk","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.randomwalk","content":" TimeseriesSurrogates.randomwalk  —  Function randomwalk(n_steps, x₀) Linear random walk (AR(1) process with a unit root) [1] . This is an example of a nonstationary linear process. References source"},{"id":866,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.NSAR2","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.NSAR2","content":" TimeseriesSurrogates.NSAR2  —  Function NSAR2(n_steps, x₀, x₁) Cyclostationary AR(2) process [1] . References source"},{"id":867,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.AR1","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.AR1","content":" TimeseriesSurrogates.AR1  —  Function AR1(; n_steps, x₀, k, rng) Simple AR(1) model given by the following map: \\[x(t+1) = k x(t) + a(t),\\] where  $a(t)$  is a draw from a normal distribution with zero mean and unit variance.  x₀  sets the initial condition and  k  is the tunable parameter in the map.  rng  is a random number generator source"},{"id":868,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.random_cycles","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.random_cycles","content":" TimeseriesSurrogates.random_cycles  —  Function random_cycles(; periods=10 dt=π/20, σ = 0.05, frange = (0.8, 2.0)) Make a timeseries that is composed of  period  full sine wave periods, each with a random frequency in the range given by  frange , and added noise with std  σ . The sampling time is  dt . source 1 Lucio et al., Phys. Rev. E  85 , 056202 (2012).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202 1 Lucio et al., Phys. Rev. E  85 , 056202 (2012).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202 1 Lucio et al., Phys. Rev. E  85 , 056202 (2012).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202"},{"id":871,"pagetitle":"Crash-course in timeseries surrogate testing","title":"Crash-course in timeseries surrogate testing","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/whatisasurrogate/#Crash-course-in-timeseries-surrogate-testing","content":" Crash-course in timeseries surrogate testing Note The summary here follows Sect. 7.4 from  Nonlinear Dynamics  by Datseris and Parlitz."},{"id":872,"pagetitle":"Crash-course in timeseries surrogate testing","title":"What is a surrogate timeseries?","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/whatisasurrogate/#What-is-a-surrogate-timeseries?","content":" What is a surrogate timeseries? A surrogate of a timeseries  x  is another timeseries  s  of equal length to  x . This surrogate  s  is generated from  x  so that it roughly preserves one or many pre-defined properties of  x , but is otherwise randomized. The upper panel in the figure below shows an example of a timeseries and one surrogate realization that preserves its both power spectrum and its amplitude distribution (histogram). Because of this preservation, the time series look similar. using TimeseriesSurrogates, CairoMakie\nx = LinRange(0, 20π, 300) .+ 0.05 .* rand(300)\nts = sin.(x./rand(20:30, 300) + cos.(x))\ns = surrogate(ts, IAAFT())\n\nsurroplot(ts, s)"},{"id":873,"pagetitle":"Crash-course in timeseries surrogate testing","title":"Performing surrogate hypothesis tests","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/whatisasurrogate/#Performing-surrogate-hypothesis-tests","content":" Performing surrogate hypothesis tests A surrogate test is a statistical test of whether a given timeseries satisfies or not a given hypothesis regarding its properties or origin. For example, the first surrogate methods were created to test the hypothesis, whether a given timeseries  x  that appears noisy may be the result of a linear stochastic process or not. If not, it may be a nonlinear process contaminated with observational noise. For the suitable hypothesis to test for, see the documentation strings of provided  Surrogate  methods or, even better, the review from Lancaster et al. (2018) [Lancaster2018] . To perform such a surrogate test, you need to: Decide what hypothesis to test against Pick a surrogate generating  method  that satisfies the chosen hypothesis Pick a suitable discriminatory statistic  q  with  q(x) ∈ Real . It must be a statistic that would obtain sufficiently different values for timeseries satisfying, or not, the chosen hypothesis. Compute  q(s)  for thousands of surrogate realizations  s = surrogate(x, method) Compare  q(x)  with the distribution of  q(s) . If  q(x)  is significantly outside the e.g., 5-95 confidence interval of the distribution, the hypothesis is rejected. This whole process is automated by  SurrogateTest , see the example below."},{"id":874,"pagetitle":"Crash-course in timeseries surrogate testing","title":"An educative example","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/man/whatisasurrogate/#An-educative-example","content":" An educative example Let's put everything together now to showcase how one would use this package to e.g., distinguish deterministic chaos contaminated with noise from actual stochastic timeseries, using the permutation entropy as a discriminatory statistic. First, let's visualize the timeseries using TimeseriesSurrogates # for surrogate tests\nusing DynamicalSystemsBase # to simulate logistic map\nusing ComplexityMeasures   # to compute permutation entropy\nusing Random: Xoshiro      # for reproducibility\nusing CairoMakie           # for plotting\n\n\n# AR1\nn = 400 # timeseries length\nrng = Xoshiro(1234567)\nx = TimeseriesSurrogates.AR1(; n_steps = n, k = 0.25, rng)\n# Logistic\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1 - x[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\nY, t = trajectory(ds, n-1)\ny = standardize(Y[:, 1]) .+ 0.5randn(rng, n) # 50% observational noise\n# Plot\nfig, ax1 = lines(y)\nax2, = lines(fig[2,1], x, color = Cycled(2))\nax1.title = \"deterministic + 50%noise\"\nax2.title = \"stochastic AR1\"\nfig Then, let's compute surrogate distributions for both timeseries using the permutation entropy as the discriminatory statistic and  RandomFourier  as the surrogate generation method perment(x) = entropy_normalized(SymbolicPermutation(; m = 3), x)\nmethod = RandomFourier()\n\nfig = Figure()\naxs = [Axis(fig[1, i]) for i in 1:2]\nNsurr = 1000\n\nfor (i, z) in enumerate((y, x))\n    sgen = surrogenerator(z, method)\n    qx = perment(z)\n    qs = map(perment, (sgen() for _ in 1:Nsurr))\n    hist!(axs[i], qs; label = \"pdf of q(s)\", color = Cycled(i))\n    vlines!(axs[i], qx; linewidth = 5, label = \"q(x)\", color = Cycled(3))\n    axislegend(axs[i])\nend\n\nfig we clearly see that the discriminatory value for the deterministic signal is so far out of the distribution that the null hypothesis that the timeseries is stochastic can be discarded with ease. This whole process can be easily automated with  SurrogateTest  as follows: test = SurrogateTest(perment, y, method; n = 1000, rng)\np = pvalue(test)\np < 0.001  # 99.9-th quantile confidence true Lancaster2018 Lancaster, G., Iatsenko, D., Pidde, A., Ticcinelli, V., & Stefanovska, A. (2018). Surrogate data for hypothesis testing of physical systems. Physics Reports, 748, 1–60. doi:10.1016/j.physrep.2018.06.001"},{"id":877,"pagetitle":"Fourier-based","title":"Fourier-based","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Fourier-based","content":" Fourier-based Fourier based surrogates are a form of constrained surrogates created by taking the Fourier transform of a time series, then shuffling either the phase angles or the amplitudes of the resulting complex numbers. Then, we take the inverse Fourier transform, yielding a surrogate time series."},{"id":878,"pagetitle":"Fourier-based","title":"Random phase","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Random-phase","content":" Random phase using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\nphases = true\ns = surrogate(ts, RandomFourier(phases))\n\nsurroplot(ts, s)"},{"id":879,"pagetitle":"Fourier-based","title":"Random amplitude","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Random-amplitude","content":" Random amplitude using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\nphases = false\ns = surrogate(ts, RandomFourier(phases))\n\nsurroplot(ts, s)"},{"id":880,"pagetitle":"Fourier-based","title":"Partial randomization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Partial-randomization","content":" Partial randomization"},{"id":881,"pagetitle":"Fourier-based","title":"Without rescaling","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Without-rescaling","content":" Without rescaling PartialRandomization  surrogates are similar to random phase surrogates, but allow for tuning the \"degree\" of phase randomization.  PartialRandomization  use an algorithm introduced by Ortega et al., which draws random phases as: \\[\\phi \\to \\alpha \\xi , \\quad \\xi \\sim \\mathcal{U}(0, 2\\pi),\\] where  $\\phi$  is a Fourier phase and  $\\mathcal{U}(0, 2\\pi)$  is a uniform distribution. Tuning the randomization parameter,  $\\alpha$ , produces a set of time series with varying degrees of randomness in their Fourier phases. using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\n\n# 50 % randomization of the phases\ns = surrogate(ts, PartialRandomization(0.5))\n\nsurroplot(ts, s) In addition to  PartialRandomization , we provide two other algorithms for producing partially randomized surrogates, outlined below."},{"id":882,"pagetitle":"Fourier-based","title":"Relative partial randomization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Relative-partial-randomization","content":" Relative partial randomization The  PartialRandomization  algorithm corresponds to assigning entirely new phases to the Fourier spectrum with some degree of randomness, regardless of any deterministic structure in the original phases. As such, even for  $\\alpha = 0$  the surrogate time series can differ drastically from the original time series. By contrast, the  RelativePartialRandomization  procedure draws phases as: \\[\\phi \\to \\phi + \\alpha \\xi, \\quad \\xi \\sim \\mathcal{U}(0, 2\\pi).\\] With this algorithm, phases are progressively corrupted by higher values of  $\\alpha$ : surrogates are identical to the original time series for  $\\alpha = 0$ , equivalent to random noise for  $\\alpha = 1$ , and retain some of the structure of the original time series when  $0 < \\alpha < 1$ . This procedure is particularly useful for controlling the degree of chaoticity and non-linearity in surrogates of chaotic systems."},{"id":883,"pagetitle":"Fourier-based","title":"Spectral partial randomization","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Spectral-partial-randomization","content":" Spectral partial randomization Both of the algorithms above randomize phases at all frequency components to the same degree. To assess the contribution of different frequency components to the structure of a time series, the  SpectralPartialRandomization  algorithm only randomizes phases above a frequency threshold. The threshold is chosen as the lowest frequency at which the power spectrum of the original time series drops below a fraction  $1-\\alpha$  of its maximum value (such that the power contained above the frequency threshold is a proportion  $\\alpha$  of the total power, excluding the zero frequency). See the figure below for a comparison of the three partial randomization algorithms:"},{"id":884,"pagetitle":"Fourier-based","title":"With rescaling","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#With-rescaling","content":" With rescaling PartialRandomizationAAFT  adds a rescaling step to the  PartialRandomization  surrogates to obtain surrogates that contain the same values as the original time series. AAFT versions of  RelativePartialRandomization  and  SpectralPartialRandomization  are also available. using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\n\n# 50 % randomization of the phases\ns = surrogate(ts, PartialRandomizationAAFT(0.7))\n\nsurroplot(ts, s)"},{"id":885,"pagetitle":"Fourier-based","title":"Amplitude adjusted Fourier transform (AAFT)","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Amplitude-adjusted-Fourier-transform-(AAFT)","content":" Amplitude adjusted Fourier transform (AAFT) using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\ns = surrogate(ts, AAFT())\n\nsurroplot(ts, s)"},{"id":886,"pagetitle":"Fourier-based","title":"Iterative AAFT (IAAFT)","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/fourier_surrogates/#Iterative-AAFT-(IAAFT)","content":" Iterative AAFT (IAAFT) The IAAFT surrogates add an iterative step to the AAFT algorithm to improve similarity of the power spectra of the original time series and the surrogates. using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\ns = surrogate(ts, IAAFT())\n\nsurroplot(ts, s)"},{"id":889,"pagetitle":"Multidimensional surrogates","title":"Multidimensional surrogates","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/multidim/#Multidimensional-surrogates","content":" Multidimensional surrogates Multidimensional surrogates operate typically on input  StateSpaceSet s and output the same type."},{"id":890,"pagetitle":"Multidimensional surrogates","title":"Shuffle dimensions","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/multidim/#Shuffle-dimensions","content":" Shuffle dimensions This surrogate was made to distinguish multidimensional data with  structure in the state space  from multidimensional noise. Here is a simple application that shows that the distinction is successful for a system that we know a-priori is deterministic and has structure in the state space (a chaotic attractor). using TimeseriesSurrogates\nusing DynamicalSystemsBase\nusing FractalDimensions: correlationsum\nusing CairoMakie\n\n# Create a trajectory from the towel map\nfunction towel_rule(x, p, n)\n    @inbounds x1, x2, x3 = x[1], x[2], x[3]\n    SVector( 3.8*x1*(1-x1) - 0.05*(x2+0.35)*(1-2*x3),\n    0.1*( (x2+0.35)*(1-2*x3) - 1 )*(1 - 1.9*x1),\n    3.78*x3*(1-x3)+0.2*x2 )\nend\nto = DeterministicIteratedMap(towel_rule, [0.1, -0.1, 0.1])\nX = trajectory(to, 10_000; Ttr = 100)[1]\n\ne = 10.0 .^ range(-1, 0; length = 10)\nCX = correlationsum(X, e; w = 5)\n\nle = log10.(e)\nfig, ax = lines(le, log10.(CX))\n\nsg = surrogenerator(X, ShuffleDimensions())\nfor i in 1:10\n    Z = sg()\n    CZ = correlationsum(Z, e)\n    lines!(ax, le, log.(CZ); color = (\"black\", 0.8))\nend\nax.xlabel = \"log(e)\"; ax.ylabel = \"log(C)\"\nfig"},{"id":893,"pagetitle":"Pseudo-periodic","title":"Pseudo-periodic","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/pps/#Pseudo-periodic","content":" Pseudo-periodic using TimeseriesSurrogates\nt = 0:0.05:20π\nx = @. 4 + 7cos(t) + 2cos(2t + 5π/4)\nx .+= randn(length(x))*0.2\n\n# Optimal d, τ values deduced using DelayEmbeddings.jl\nd, τ = 3, 31\n\n# For ρ you can use `noiseradius`\nρ = 0.11\n\nmethod = PseudoPeriodic(d, τ, ρ, false)\ns = surrogate(x, method)\nsurroplot(x, s)"},{"id":896,"pagetitle":"Pseudo-periodic twin","title":"Pseudo-periodic twin surrogates","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/ppts/#Pseudo-periodic-twin-surrogates","content":" Pseudo-periodic twin surrogates using TimeseriesSurrogates, CairoMakie\n\n# Example system from the original paper\nn, Δt = 500, 0.05\nf₁, f₂ = sqrt(3), sqrt(5)\nx = [8*sin(2π*f₁*t) + 4*sin(2π*f₂*t) for t = 0:Δt:Δt*n]\n\n# Embedding parameter, neighbor threshold and noise radius\nd, τ = 2, 6\nδ = 0.15\nρ = noiseradius(x, d, τ, 0.02:0.02:0.5)\nmethod = PseudoPeriodicTwin(d, τ, δ, ρ)\n\n# Generate the surrogate, which is a `d`-dimensional dataset.\nsurr_orbit = surrogate(x, method)\n\n# Get scalar surrogate time series from first and second column.\ns1, s2 = surr_orbit[:, 1], surr_orbit[:, 2]\n\n# Scalar time series versus surrogate time series\nfig = Figure()\nax_ts = Axis(fig[1,1:2]; xlabel = \"time\", ylabel = \"value\")\nlines!(ax_ts, s1; color = :red)\nlines!(ax_ts, x; color = :black)\n\n# Embedding versus surrogate embedding\nX = embed(x, d, τ)\nax2 = Axis(fig[2, 1]; xlabel = \"x(t)\", ylabel = \"x(t-$τ)\")\nlines!(ax2, X[:, 1], X[:, 2]; color = :black)\nscatter!(ax2, X[:, 1], X[:, 2]; color = :black, markersize = 4)\n\nps = Axis(fig[2,2]; xlabel=  \"s(t)\", ylabel = \"s(t-$τ)\")\nlines!(ps, s1, s2; color = :red)\nscatter!(ps, s1, s2; color = :black, markersize = 4)\n\nfig"},{"id":899,"pagetitle":"Shuffle-based","title":"Shuffle-based","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/randomshuffle/#Shuffle-based","content":" Shuffle-based"},{"id":900,"pagetitle":"Shuffle-based","title":"Random shuffle (RS)","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/randomshuffle/#Random-shuffle-(RS)","content":" Random shuffle (RS) Randomly shuffled surrogates are simply permutations of the original time series. Thus, they break any correlations in the signal. using TimeseriesSurrogates, CairoMakie\nx = AR1() # create a realization of a random AR(1) process\ns = surrogate(x, RandomShuffle())\nsurroplot(x, s)"},{"id":901,"pagetitle":"Shuffle-based","title":"Block shuffle (BS)","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/randomshuffle/#Block-shuffle-(BS)","content":" Block shuffle (BS) Randomly shuffled surrogates are generated by dividing the original signal into blocks, then permuting those blocks. Block positions are randomized, and blocks at the end of the signal gets wrapped around to the start of the time series. Thus, they keep short-term correlations within blocks, but destroy any long-term dynamical information in the signal. using TimeseriesSurrogates, CairoMakie\nx = NSAR2(n_steps = 300)\n# We want to divide the signal into 8 blocks.\ns = surrogate(x, BlockShuffle(8))\np = surroplot(x, s)"},{"id":902,"pagetitle":"Shuffle-based","title":"Cycle shuffle (CSS)","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/randomshuffle/#Cycle-shuffle-(CSS)","content":" Cycle shuffle (CSS) using TimeseriesSurrogates, CairoMakie\nx = random_cycles()\ns = surrogate(x, CycleShuffle())\np = surroplot(x, s)"},{"id":903,"pagetitle":"Shuffle-based","title":"Circular shift","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/randomshuffle/#Circular-shift","content":" Circular shift using TimeseriesSurrogates, CairoMakie\nx = random_cycles()\ns = surrogate(x, CircShift(1:length(x)))\np = surroplot(x, s)"},{"id":906,"pagetitle":"Wavelet-based","title":"Wavelet surrogates","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/wls/#Wavelet-surrogates","content":" Wavelet surrogates"},{"id":907,"pagetitle":"Wavelet-based","title":"WLS","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/wls/#WLS","content":" WLS WLS  surrogates are constructed by taking the maximal overlap  discrete wavelet transform (MODWT) of the signal, shuffling detail  coefficients across dyadic scales, then inverting the transform to  obtain the surrogate. "},{"id":908,"pagetitle":"Wavelet-based","title":"Wavelet-IAAFT (WIAAFT) surrogates","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/wls/#Wavelet-IAAFT-(WIAAFT)-surrogates","content":" Wavelet-IAAFT (WIAAFT) surrogates In  Keylock (2006) ,  IAAFT shuffling is used, yielding surrogates that preserve the local mean and  variance of the original signal, but randomizes nonlinear properties of the signal. This also preserves nonstationarities in the signal. To construct WIAAFT surrogates, rescaling must be enabled.  Note: the final iterative procedure of the WIAAFT surrogate method, after the rescaling step,  is not performed in our current implementation, so surrogates might differ a bit from results in Keylock (2006). For now, you have to do the iterative rescaling manually if desired. .  using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\n# Rescale surrogate back to original values\nmethod = WLS(IAAFT(), rescale = true)\ns = surrogate(x, method);\np = surroplot(x, s) Even without rescaling, IAAFT shuffling also yields surrogates with local properties  very similar to the original signal. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\n# Don't rescale back to original time series.\nmethod = WLS(IAAFT(), rescale = false)\ns = surrogate(x, method);\np = surroplot(x, s)"},{"id":909,"pagetitle":"Wavelet-based","title":"Other shuffling methods","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/wls/#Other-shuffling-methods","content":" Other shuffling methods The choice of coefficient shuffling method determines how well and  which properties of the original signal are retained by the surrogates.  There might be use cases where surrogates do not need to perfectly preserve the  autocorrelation of the original signal, so additional shuffling  methods are provided for convenience. Using random shuffling of the detail coefficients does not preserve the  autocorrelation structure of the original signal.  using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\nmethod = WLS(RandomShuffle(), rescale = false)\ns = surrogate(x, method);\np = surroplot(x, s) Block shuffling the detail coefficients better preserve local properties because the shuffling is not completely random, but still does not  preserve the autocorrelation of the original signal. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\ns = surrogate(x, WLS(BlockShuffle(10), rescale = false));\np = surroplot(x, s) Random Fourier phase shuffling the detail coefficients does a decent job at preserving the autocorrelation. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\ns = surrogate(x, WLS(RandomFourier(), rescale = false));\nsurroplot(x, s) To generate surrogates that preserve linear properties of the original signal, AAFT or IAAFT shuffling is required."},{"id":910,"pagetitle":"Wavelet-based","title":"RandomCascade","ref":"/DynamicalSystemsDocs.jl/timeseriessurrogates/stable/methods/wls/#RandomCascade","content":" RandomCascade RandomCascade  surrogates is another wavelet-based method that uses the regular discrete wavelet transform to generate surrogates. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n     [20*sin(2π/30*i) for i = 1:n] .+\n     [20*cos(2π/90*i) for i = 1:n] .+\n     [50*sin(2π/2*i + π) for i = 1:n] .+\n     σ .* rand(n).^2 .+\n     [0.2*t for t = 1:n];\n\ns = surrogate(x, RandomCascade());\nsurroplot(x, s)"},{"id":913,"pagetitle":"RecurrenceAnalysis.jl","title":"RecurrenceAnalysis.jl","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/#RecurrenceAnalysis.jl","content":" RecurrenceAnalysis.jl"},{"id":914,"pagetitle":"RecurrenceAnalysis.jl","title":"RecurrenceAnalysis","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/#RecurrenceAnalysis","content":" RecurrenceAnalysis  —  Module RecurrenceAnalysis.jl A Julia package that offers tools for computing Recurrence Plots and exploring them within the framework of Recurrence Quantification Analysis and Recurrence Network Analysis. It can be used as a standalone package, or as part of DynamicalSystems.jl. To install it, run  import Pkg; Pkg.add(\"RecurrenceAnalysis\") . All further information is provided in the documentation, which you can either find online or build locally by running the  docs/make.jl  file. source References Want to learn more about recurrence quantification analysis? Here are some resources: Chapter 9 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. Webber, C. L., & Marwan, N. (2015). Recurrence Quantification Analysis. (C. L. Webber, & N. Marwan, Eds.). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-07155-8 Marwan, N., Carmen Romano, M., Thiel, M., & Kurths, J. (2007). Recurrence plots for the analysis of complex systems. Physics Reports, 438(5–6), 237–329. https://doi.org/10.1016/j.physrep.2006.11.001"},{"id":917,"pagetitle":"Recurrence Networks","title":"Recurrence Networks","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/networks/#Recurrence-Networks","content":" Recurrence Networks Recurrence matrices can be reinterpreted as adjacency matrices of  complex networks  embedded in state space, such that each node or vertex of the network corresponds to a point of the timeseries, and the links of the network connect pairs of points that are mutually close the phase space. The relationship between a recurrence matrix  $R$  and its corresponding adjacency matrix  $A$  is: \\[R[i,j] = A[i,j] - \\delta[i,j]\\] i.e. there is an edge in the associated network between every two neighboring points in the phase space, excluding self-connections (points in the  Line Of Identity  or main diagonal of  $R$ ). This definition assumes that  $A$  represents an  undirected graph , so  $R$  must be a symmetric matrix as corresponding to a  RecurrenceMatrix  or a  JointRecurrenceMatrix . While RQA characterizes the properties of line structures in the recurrence plots, which consider dynamical aspects (e.g. continuity of recurrences, length of sequences, etc.), the analysis of recurrence networks does not take into account time information, since network properties are independent of the ordering of vertices. On the other hand, recurrence network analysis (RNA) provides information about geometric characteristics of the state space, like homogeneity of the connections, clustering of points, etc. More details about the theoretical framework of RNA can be found in the following papers: R.V. Donner  et al. \"Recurrence networks — a novel paradigm for nonlinear time series analysis\",  New Journal of Physics  12, 033025 (2010) R.V. Donner  et al.  \"Complex Network Analysis of Recurrences\", in: Webber, C.L. & Marwan N. (eds.)  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 101-165 (2015)."},{"id":918,"pagetitle":"Recurrence Networks","title":"Creation and visualization of Recurrence Networks","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/networks/#Creation-and-visualization-of-Recurrence-Networks","content":" Creation and visualization of Recurrence Networks The  JuliaGraphs  organization provides multiple packages for Julia to create, visualize and analyze complex networks. In particular, the package  LightGraphs  defines the type  SimpleGraph  that can be used to represent undirected networks. Such graphs can be created from symmetric recurrence matrices, as in the following example with a Hénon map: using RecurrenceAnalysis, DynamicalSystemsBase\nusing Graphs: SimpleGraph\n\n# make trajectory of Henon map\nhenon_rule(x, p, n) = SVector(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nu0 = zeros(2)\np0 = [1.4, 0.3]\nhenon = DeterministicIteratedMap(henon_rule, u0, p0)\nX, t = trajectory(henon, 200)\n# Cast it into a recurrence network\nR = RecurrenceMatrix(X, 0.25; metric = Chebyshev())\nnetwork = SimpleGraph(R) {201, 2945} undirected simple Int64 graph There are various plotting tools that can be used to visualize such graphs. For instance, the following plot made with the package  GraphMakie.jl . using GraphMakie, CairoMakie\ngraphplot(network)"},{"id":919,"pagetitle":"Recurrence Networks","title":"Recurrence Network measures","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/networks/#Recurrence-Network-measures","content":" Recurrence Network measures LightGraphs has a large set of functions to extract local measures (associated to particular vertices or edges) and global coefficients associated to the whole network. For  SimpleGraph s created from recurrence matrices, as the variable  network  in the previous example, the vertices are labelled with numeric indices following the same ordering as the rows or columns of the given matrix. So for instance  degree(network, i)  would give the  degree  of the  i -th point of the timeseries (number of connections with other points), whereas  degree(network)  would give a vector of such measures ordered as the original timeseries. As in RQA, we provide a function that computes a selection of commonly used global RNA measures, directly from the recurrence matrix:"},{"id":920,"pagetitle":"Recurrence Networks","title":"RecurrenceAnalysis.rna","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/networks/#RecurrenceAnalysis.rna","content":" RecurrenceAnalysis.rna  —  Function rna(R::AbstractRecurrenceMatrix)\nrna(args...; kwargs...) Calculate a set of Recurrence Network parameters. The input  R  can be a symmetric recurrence matrix that is interpreted as the adjacency matrix of an undirected complex network, such that linked vertices are neighboring points in the phase space. Alternatively, the inputs can be a graph object or any valid inputs to the  SimpleGraph  constructor of the  Graphs  package. Return The returned value is a dictionary that contains the following entries, with the corresponding global network properties[1, 2]: :density : edge density, approximately equivalent to the global recurrence rate in the phase space. :transitivity : network transitivity, which describes the global clustering of points following Barrat's and Weigt's formulation [3]. :averagepath : mean value of the shortest path lengths taken over all pairs of connected vertices, related to the average separation between points in the phase. :diameter : maximum value of the shortest path lengths between pairs of connected vertices, related to the phase space diameter. References [1]: R.V. Donner  et al.  \"Recurrence networks — a novel paradigm for nonlinear time series analysis\",  New Journal of Physics  12, 033025 (2010)  DOI:10.1088/1367-2630/12/3/033025 [2]: R.V. Donner  et al. , The geometry of chaotic dynamics — a complex network perspective,  Eur. Phys. J.  B 84, 653–672 (2011)  DOI:10.1140/epjb/e2011-10899-1 [3]: A. Barrat & M. Weight, \"On the properties of small-world network models\",  The European Physical Journal B  13, 547–560 (2000)  DOI:10.1007/s100510050067 source Transitivity and global clustering coefficient The concept of clustering coefficient at local level (for individual nodes of the network) is clearly defined as the fraction of connecting nodes that are also connected between them, forming \"triangles\". But at global level it is a source of confusion: the term of \"global clustering coefficient\" was originally used by Watts and Strogatz [1] , referred to the average of local clustering coefficient across all the graph's nodes. But Barrat and Weigt proposed an alternative definition [2]  that characterizes the effective global dimensionality of the system, giving equal weight to all triangles in the network [3] . This second definition is often named with the distinctive term of \"transitivity\", as in the output of  rna , whereas the corresponding function of the  LightGraphs  package is  global_clustering_coefficient . The \"global clustering coefficient\" as by Watts and Strogatz could be obtained as  mean(local_clustering_coefficient(network))  – with  network  being a graph object as in the previous example. (The function  mean  is in the Julia standard library, and can be brought into scope with the command  using Statistics .) 1 D.J. Watts & S.H. Strogatz, \"Collective dynamics of 'small-world' networks\",  Nature 393 (6684), 440–442 (1998)  DOI:10.1038%2F30918 2 A. Barrat & M. Weight, \"On the properties of small-world network models\",  The European Physical Journal B  13, 547–560 (2000)   DOI:10.1007/s100510050067 3 R.V. Donner  et al.  \"Recurrence networks — a novel paradigm for nonlinear time series analysis\",  New Journal of Physics  12, 033025 (2010)  DOI:10.1088/1367-2630/12/3/033025"},{"id":923,"pagetitle":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Recurrence-Quantification-Analysis","content":" Recurrence Quantification Analysis A  RecurrenceMatrix  can be analyzed in several ways to yield information about the dynamics of the trajectory. All these various  measures  and functions are collectively called \"Recurrence Quantification Analysis\" (RQA). To understand how each measure can be useful, we suggest to see the review articles listed in our documentation strings, namely: N. Marwan  et al. , \"Recurrence plots for the analysis of complex systems\",  Phys. Reports 438 (5-6), 237-329 (2007). N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.),  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 3-43 (2015). You can also check the wikipedia page for  Recurrence quantification analysis . The functions described in this page all accept a recurrence matrix ( x ), see  RecurrenceMatrix ."},{"id":924,"pagetitle":"Recurrence Quantification Analysis","title":"RQA Measures","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RQA-Measures","content":" RQA Measures"},{"id":925,"pagetitle":"Recurrence Quantification Analysis","title":"All-in-one Bundle","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#All-in-one-Bundle","content":" All-in-one Bundle In case you need all of the RQA-related functions (see below) and you don't want to write 10 lines of code to compute them all (since they are so many) we provide an all-in-one function that computes all of them and returns a dictionary with the results!"},{"id":926,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rqa","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.rqa","content":" RecurrenceAnalysis.rqa  —  Function rqa(R; kwargs...) Calculate all RQA parameters of a recurrence matrix  R . See the functions referred to below for the definition of the different parameters and the default values of the arguments. Using this function is much more efficient than calling all individual functions one by one. Return The returned value contains the following entries, which can be retrieved as from a dictionary (e.g.  results[:RR] , etc.): :RR : recurrence rate (see  recurrencerate ) :DET : determinsm (see  determinism ) :L : average length of diagonal structures (see  dl_average ) :Lmax : maximum length of diagonal structures (see  dl_max ) :DIV : divergence (see  divergence ) :ENTR : entropy of diagonal structures (see  dl_entropy ) :TREND : trend of recurrences (see  trend ) :LAM : laminarity (see  laminarity ) :TT : trapping time (see  trappingtime ) :Vmax : maximum length of vertical structures (see  vl_max ) :VENTR : entropy of vertical structures (see  vl_entropy ) :MRT : mean recurrence time (see  meanrecurrencetime ) :RTE  recurrence time entropy (see  rt_entropy ) :NMPRT : number of the most probable recurrence time (see  nmprt ) All the parameters returned by  rqa  are  Float64  numbers, even for parameters like  :Lmax ,  :Vmax  or  :NMPRT  which are integer values. In the case of empty histograms (e.g. no existing vertical lines less than the keyword  lminvert ) the average and maximum values ( :L ,  :Lmax ,  :TT ,  :Vmax ,  :MRT ) are returned as  0.0  but their respective entropies ( :ENTR ,  :VENTR ,  :RTE ) are returned as  NaN . Keyword Arguments Standard keyword arguments are the ones accepted by the functions listed below, i.e.  theiler ,  lmin , and  border : theiler  is used to define a \"Theiler window\" around the central diagonal or \"line of identity\" (LOI): a region of points that are excluded in the calculation of RQA parameters, in order to rule out self-recurrences and apparent recurrences for smooth or high resolution data. The LOI is excluded by default for matrices of the types  RecurrenceMatrix  or  JointRecurrenceMatrix , but it is included for matrices of the type  CrossRecurrenceMatrix .  theiler=0  means that the whole matrix is scanned for lines.  theiler=1  means that the LOI is excluded. In general,  theiler=n  means that the  n  central diagonals are excluded (at both sides of the LOI, i.e. actually  2n-1  diagonals are excluded). lmin  is used to define the minimum line length in the parameters that describe the distributions of diagonal or vertical lines (it is set as 2 by default). border  is used to avoid border effects in the calculation of  :TREND  (cf.  trend ). In addition  theilerdiag ,  lmindiag  may be used to declare specific values that override the values of  theiler  and  lmin  in the calculation of parameters related to diagonal structures. Likewise,  theilervert  and  lminvert  can be used for the calculation of parameters related to vertical structures. The keyword argument  onlydiagonal  ( false  by default) can be set to  true  in order to restrict the analysis to the recurrence rate and the parameters related to diagonal structures ( :RR ,  :DET ,  :L ,  :Lmax ,  :DIV  and  :ENTR ), which makes this function slightly faster. Transitional note on the returned type In older versions, the  rqa  function returned a  NamedTuple , and in future versions it is planned to return a  Dict  instead. In both cases, the results can be indexed with square brackets and  Symbol  keys, as  result[:RR] ,  result[:DET] , etc. However, named tuples can also be indexed with \"dot syntax\", e.g.  result.RR , whereas this will not be possible with dictionaries, and there are other differences in the indexing and iteration of those two types. In order to facilitate the transition between versions, this function currently returns a  RQA  object that essentially works as a dictionary, but can also be indexed with the dot syntax (logging a deprecation warning). The returned type can also be specified as a first argument of  rqa  in order to replicate the output of different versions: rqa(NamedTuple, R...)  to obtain the output of the older version (as in 1.3). rqa(Dict, R...)  to obtain the output of the planned future version. rqa(RQA, R...)  to obtain the default current output (same as  rqa(R...) ) rqa(DT,R...)  to obtain the output as  DT  which is a subtype of  AbstractDict  (e.g.  rqa(OrderedDict,R...)  returns an  OrderedDict ) source Return values for empty histograms It may be the case that for a given recurrence matrix some structures do not exist at all. For example there are recurrence matrices that have no vertical lengths (or no vertical lengths with length less than  lmin ). In such cases the behavior of our RQA pipeline is the following: Quantities that represent maximum or average values are  0.0 . Quantities that represent entropies are  NaN . See also the  @windowed  macro for a windowed version of  rqa ."},{"id":927,"pagetitle":"Recurrence Quantification Analysis","title":"Example","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Example","content":" Example For example, here are the RQA measures for the same example trajectory we used to make  Simple Recurrence Plots using RecurrenceAnalysis, DynamicalSystemsBase\n\n# Create trajectory of Roessler system\n@inbounds function roessler_rule(u, p, t)\n    a, b, c = p\n    du1 = -u[2]-u[3]\n    du2 = u[1] + a*u[2]\n    du3 = b + u[3]*(u[1] - c)\n    return SVector(du1, du2, du3)\nend\np0 = [0.15, 0.2, 10.0]\nu0 = ones(3)\nro = CoupledODEs(roessler_rule, u0, p0)\nN = 2000; Δt = 0.05\nX, t = trajectory(ro, N*Δt; Δt, Ttr = 10.0)\n\n# Make a recurrence matrix with fixed threshold\nR = RecurrenceMatrix(X, 5.0)\n\n# Compute RQA measures\nrqa(R) RQA parameters in Dict{Symbol, Float64} with 14 entries:\n  :DIV   => 0.00118624\n  :LAM   => 0.999173\n  :Vmax  => 57.0\n  :VENTR => 3.57043\n  :Lmax  => 843.0\n  :MRT   => 172.716\n  :NMPRT => 2467.0\n  :RR    => 0.0953458\n  :RTE   => 3.78882\n  :TT    => 17.9861\n  :L     => 133.317\n  :ENTR  => 5.49263\n  :DET   => 0.999443\n  :TREND => -0.0228398"},{"id":928,"pagetitle":"Recurrence Quantification Analysis","title":"Classical RQA Measures","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Classical-RQA-Measures","content":" Classical RQA Measures"},{"id":929,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencerate","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.recurrencerate","content":" RecurrenceAnalysis.recurrencerate  —  Function recurrencerate(R[; theiler]) Calculate the recurrence rate of the recurrence matrix  R . Description The recurrence rate is calculated as: \\[RR = \\frac{1}{S} \\sum R\\] where  $S$  is the size of  R  or the region of  R  with potential recurrent points. There is not a unique definition of that denominator, which is defined as the full size of the matrix in many sources (e.g. [1]), whereas in others it is adjusted to remove the points of the LOI when they are excluded from the count [2,3]. For matrices of type  RecurrenceMatrix  or  JointRecurrenceMatrix , where the points around the central diagonal are usually excluded, the denominator is adjusted to the size of the matrix outside the Theiler window (by default equal to the LOI, and adjustable with the keyword argument  theiler ; see  rqa  for details). For matrices of type  CrossRecurrenceMatrix , where normally all points are analyzed, the denominator is always the full size of the matrix, regardless of the Theiler window that might be defined (none by default). Hint : to reproduce the calculations done following the formulas that use the full size of the matrix in the denominator, use  CrossRecurrenceMatrix(s,s,ε)  to define the recurrence matrix, instead of  RecurrenceMatrix(s,ε) , setting  theiler=1  (or  theiler=n  in general) to explicitly exclude the LOI or other diagonals around it. References [1] : N. Marwan  et al. , \"Recurrence plots for the analysis of complex systems\",  Phys. Reports 438 (5-6), 237-329 (2007).  DOI:10.1016/j.physrep.2006.11.001 [2] : C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC, Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences, 26-94 (2005). URL: https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf [3] : N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.),  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 3-43 (2015). source"},{"id":930,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.determinism","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.determinism","content":" RecurrenceAnalysis.determinism  —  Function determinism(R[; lmin=2, theiler]) Calculate the determinism of the recurrence matrix  R : Description The determinism is calculated as: \\[DET = \\frac{\\sum_{l=lmin}{l P(l)}}{\\sum_{l=1}{l P(l)}} =\n\\frac{\\sum_{l=lmin}{l P(l)}}{\\sum R}\\] where  $l$  stands for the lengths of diagonal lines in the matrix, and  $P(l)$  is the number of lines of length equal to  $l$ . lmin  is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":931,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_average","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.dl_average","content":" RecurrenceAnalysis.dl_average  —  Function dl_average(R[; lmin=2, theiler]) Calculate the average of the diagonal lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":932,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_max","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.dl_max","content":" RecurrenceAnalysis.dl_max  —  Function dl_max(R[; lmin=2, theiler]) Calculate the longest diagonal line contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":933,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_entropy","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.dl_entropy","content":" RecurrenceAnalysis.dl_entropy  —  Function dl_entropy(R[; lmin=2, theiler]) Calculate the Shannon entropy of the diagonal lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Notes: This metric was first proposed in the paper \"Exploiting Nonlinear Recurrence  and Fractal Scaling Properties for Voice Disorder Detection\" as Recurrence Period  Density Entropy or Recurrence Probability Density Entropy (RPDE).  It is a normalized dimensionless metric in the range [0,1].  In the 2018 article \"Recurrence threshold selection for obtaining robust recurrence  characteristics in different embedding dimensions\", the indicator RPDE is explicitly  called Recurrence Time Entropy (RTE). Here RPDE and RTE are clearly the same indicator. source"},{"id":934,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.divergence","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.divergence","content":" RecurrenceAnalysis.divergence  —  Function divergence(R[; theiler]) Calculate the divergence of the recurrence matrix  R  (actually the inverse of  dl_max ). source"},{"id":935,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trend","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.trend","content":" RecurrenceAnalysis.trend  —  Function trend(R[; border=10, theiler]) Calculate the trend of recurrences in the recurrence matrix  R . Description The trend is the slope of the linear regression that relates the density of recurrent points in the diagonals parallel to the LOI and the distance between those diagonals and the LOI. It quantifies the degree of system stationarity, such that in recurrence plots where points \"fade away\" from the central diagonal, the trend will have a negative value. It is calculated as: \\[TREND = 10^3\\frac{\\sum_{d=\\tau}^{\\tilde{N}}\\delta[d]\\left(RR[d]-\\langle RR[d]\\rangle\\right)}{\\sum_{d=\\tau}^{\\tilde{N}}\\delta[d]^2}\\] where  $RR[d]$  is the local recurrence rate of the diagonal  $d$ ,  $\\delta[d]$  is a balanced measure of the distance between that diagonal and the LOI,  $\\tau$  is the Theiler window (number of central diagonals that are excluded), and  $\\tilde{N}$  is the number of the outmost diagonal that is included. This parameter is expressed in units of variation recurrence rate every 1000 data points, hence the factor  $10^3$  in the formula [1]. The 10 outermost diagonals (counting from the corners of the matrix) are excluded by default to avoid \"border effects\". Use the keyword argument  border  to define a different number of excluded lines, and  theiler  to define the size of the Theiler window (see  rqa  for details). Note : In rectangular cross-recurrence plots (i.e. when the time series that originate them are not of the same length), the limits of the formula for TREND are not clearly defined. For the sake of consistency, this function limits the calculations to the biggest square matrix that contains the LOI. References [1] C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC,  Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences , 2005, 26-94. https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf source"},{"id":936,"pagetitle":"Recurrence Quantification Analysis","title":"Extended RQA Measures","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Extended-RQA-Measures","content":" Extended RQA Measures"},{"id":937,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.laminarity","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.laminarity","content":" RecurrenceAnalysis.laminarity  —  Function laminarity(R[; lmin=2, theiler]) Calculate the laminarity of the recurrence matrix  R . Description The laminarity is calculated as: \\[LAM = \\frac{\\sum_{v=lmin}{v P(l)}}{\\sum_{v=1}{v P(v)}} =\n\\frac{\\sum_{v=lmin}{v P(l)}}{\\sum R}\\] where  $v$  stands for the lengths of vertical lines in the matrix, and  $P(v)$  is the number of lines of length equal to  $v$ . lmin  is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":938,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trappingtime","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.trappingtime","content":" RecurrenceAnalysis.trappingtime  —  Function trappingtime(R[; lmin=2, theiler]) Calculate the trapping time of the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). The trapping time is the average of the vertical line structures and thus equal to  vl_average . source"},{"id":939,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_average","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.vl_average","content":" RecurrenceAnalysis.vl_average  —  Function vl_average(R[; lmin=2, theiler]) Calculate the average of the vertical lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":940,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_max","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.vl_max","content":" RecurrenceAnalysis.vl_max  —  Function vl_max(R[; lmin=2, theiler]) Calculate the longest vertical line contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":941,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_entropy","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.vl_entropy","content":" RecurrenceAnalysis.vl_entropy  —  Function vl_entropy(R[; lmin=2, theiler]) Calculate the Shannon entropy of the vertical lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Notes: This metric was first proposed in the paper \"Exploiting Nonlinear Recurrence  and Fractal Scaling Properties for Voice Disorder Detection\" as Recurrence Period  Density Entropy or Recurrence Probability Density Entropy (RPDE).  It is a normalized dimensionless metric in the range [0,1].  In the 2018 article \"Recurrence threshold selection for obtaining robust recurrence  characteristics in different embedding dimensions\", the indicator RPDE is explicitly  called Recurrence Time Entropy (RTE). Here RPDE and RTE are clearly the same indicator. source"},{"id":942,"pagetitle":"Recurrence Quantification Analysis","title":"Recurrence Time Measures","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Recurrence-Time-Measures","content":" Recurrence Time Measures"},{"id":943,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.meanrecurrencetime","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.meanrecurrencetime","content":" RecurrenceAnalysis.meanrecurrencetime  —  Function meanrecurrencetime(R[; lmin=2, theiler]) Calculate the mean recurrence time of the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Equivalent to  rt_average . source"},{"id":944,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.nmprt","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.nmprt","content":" RecurrenceAnalysis.nmprt  —  Function nmprt(R[; lmin=2, theiler]) Calculate the number of the most probable recurrence time (NMPRT), ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). This number indicates how many times the system has recurred using the recurrence time that appears most frequently, i.e it is the maximum value of the histogram of recurrence times [1]. References [1] : E.J. Ngamga  et al.  \"Recurrence analysis of strange nonchaotic dynamics\",  Physical Review E , 75(3), 036222(1-8) (2007)  DOI:10.1103/physreve.75.036222 source"},{"id":945,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_entropy","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.rt_entropy","content":" RecurrenceAnalysis.rt_entropy  —  Function rt_entropy(R[; lmin=2, theiler]) Calculate the Shannon entropy of the recurrence times contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Notes: This metric was first proposed in the paper \"Exploiting Nonlinear Recurrence  and Fractal Scaling Properties for Voice Disorder Detection\" as Recurrence Period  Density Entropy or Recurrence Probability Density Entropy (RPDE).  It is a normalized dimensionless metric in the range [0,1].  In the 2018 article \"Recurrence threshold selection for obtaining robust recurrence  characteristics in different embedding dimensions\", the indicator RPDE is explicitly  called Recurrence Time Entropy (RTE). Here RPDE and RTE are clearly the same indicator. source"},{"id":946,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_average","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.rt_average","content":" RecurrenceAnalysis.rt_average  —  Function rt_average(R[; lmin=2, theiler]) Calculate the average of the recurrence times contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":947,"pagetitle":"Recurrence Quantification Analysis","title":"Keyword table","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Keyword-table","content":" Keyword table Since most of the above functions can be fined tuned with keyword arguments, here is a table summarizing them that could be of use: Argument Default Functions Description theiler 0 for  CrossRecurrenceMatrix , 1 otherwise. recurrencerate ,  determinism ,  *_average ,  *_max ,  *_entropy ,  divergence ,  trend ,  laminarity ,  trappingtime ,   meanrecurrencetime ,  nmprt Theiler window: number of diagonals around the LOI  excluded  from the analysis. The value  0  means that the LOI is  included  in the analysis. Use  1  to exclude the LOI. lmin 2 determinism ,  *_average ,  *_max ,  *_entropy ,  divergence ,  laminarity ,  trappingtime ,   meanrecurrencetime ,  nmprt Minimum length of the recurrent structures (diagonal or vertical) considered in the analysis. border 10 trend Number of diagonals excluded from the analysis near the border of the matrix."},{"id":948,"pagetitle":"Recurrence Quantification Analysis","title":"Recurrence Structures Histograms","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Recurrence-Structures-Histograms","content":" Recurrence Structures Histograms The functions that we list in this page internally compute histograms of some recurrence structures, like e.g. the vertical lengths. You can access these values directly with the following function:"},{"id":949,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencestructures","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.recurrencestructures","content":" RecurrenceAnalysis.recurrencestructures  —  Function recurrencestructures(x::AbstractRecurrenceMatrix;\n                         diagonal=true,\n                         vertical=true,\n                         recurrencetimes=true,\n                         kwargs...) Return a dictionary with the histograms of the recurrence structures contained in the recurrence matrix  x , with the keys  \"diagonal\" ,  \"vertical\"  or  \"recurrencetimes\" , depending on what keyword arguments are given as  true . Description Each item of the dictionary is a vector of integers, such that the  i -th element of the vector is the number of lines of length  i  contained in  x . \"diagonal\"  counts the diagonal lines, i.e. the recurrent trajectories. \"vertical\"  counts the vertical lines, i.e. the laminar states. \"recurrencetimes\"  counts the vertical distances between recurrent states,   i.e. the recurrence times. All the points of the matrix are counted by default. The keyword argument  theiler  can be passed to rule out the lines around the main diagonal. See the arguments of the function  rqa  for further details. \"Empty\" histograms are represented always as  [0] . Notice : There is not a unique operational definition of \"recurrence times\". In the analysis of recurrence plots, usually the  \"second type\" of recurrence times as defined by Gao and Cai [1] are considered, i.e. the distance between consecutive (but separated) recurrent structures in the vertical direction of the matrix. But that distance is not uniquely defined when the vertical recurrent structures are longer than one point. The recurrence times calculated here are the distance between the midpoints of consecutive lines, which is a balanced estimator of the Poincaré recurrence times [2]. References [1] J. Gao & H. Cai. \"On the structures and quantification of recurrence plots\".  Physics Letters A , 270(1-2), 75–87 (2000) . [2] N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.),  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 3-43 (2015). source"},{"id":950,"pagetitle":"Recurrence Quantification Analysis","title":"Windowed application","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#Windowed-application","content":" Windowed application"},{"id":951,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.windowed","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.windowed","content":" RecurrenceAnalysis.windowed  —  Function windowed(rmat, f, width, step = 1; kwargs...) A convenience function that applies the RQA function  f , such as  determinism , to windowed views of the given recurrence matrix  rmat  with given window  width  and  step . The  kwargs...  are propagated to the call  f(rmat_view; kwargs...) . source"},{"id":954,"pagetitle":"Recurrence Plots","title":"Recurrence Plots","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Recurrence-Plots","content":" Recurrence Plots"},{"id":955,"pagetitle":"Recurrence Plots","title":"Recurrence Matrices","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Recurrence-Matrices","content":" Recurrence Matrices A  Recurrence plot  (which refers to the plot of a recurrence matrix) is a way to quantify  recurrences  that occur in a trajectory. A recurrence happens when a trajectory visits the same neighborhood on the phase space that it was at some previous time. The central structure used in these recurrences is the (cross-) recurrence matrix: \\[R[i, j] = \\begin{cases}\n1 \\quad \\text{if}\\quad d(x[i], y[j]) \\le \\varepsilon\\\\\n0 \\quad \\text{else}\n\\end{cases}\\] where  $d(x[i], y[j])$  stands for the  distance  between trajectory  $x$  at point  $i$  and trajectory  $y$  at point  $j$ . Both  $x, y$  can be single timeseries, full trajectories or embedded timeseries (which are also trajectories). If  $x\\equiv y$  then  $R$  is called recurrence matrix, otherwise it is called cross-recurrence matrix. There is also the joint-recurrence variant, see below. With  RecurrenceAnalysis  you can use the following functions to access these matrices"},{"id":956,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceMatrix","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.RecurrenceMatrix","content":" RecurrenceAnalysis.RecurrenceMatrix  —  Type RecurrenceMatrix(x, ε; metric = Euclidean(), parallel::Bool) Create a recurrence matrix from trajectory  x  and with recurrence threshold specification  ε .  x  is either a  StateSpaceSet  for multivariate data or an  AbstractVector{<:Real}  for timeseries. If  ε::Real  is given, a  RecurrenceThreshold  is used to specify recurrences. Otherwise, any subtype of  AbstractRecurrenceType  may be given as  ε  instead. The keyword  metric , if given, must be any subtype of  Metric  from  Distances.jl  and defines the metric used to calculate distances for recurrences. By default the Euclidean metric is used, typical alternatives are  Chebyshev(), Cityblock() . The keyword  parallel  decides if the comptutation should be done in parallel using threads. Defaults to  length(x) > 500 && Threads.nthreads() > 1 . Description A (cross-)recurrence matrix is a way to quantify  recurrences  that occur in a trajectory. A recurrence happens when a trajectory visits the same neighborhood on the state space that it was at some previous time. The recurrence matrix is a numeric representation of a recurrence plot, described in detail in  [Marwan2007]  and  [Marwan2015] . It represents a a sparse square matrix of Boolean values that quantifies recurrences in the trajectory, i.e., points where the trajectory returns close to itself. Given trajectories  x, y , and asumming  ε isa Real , the matrix is defined as: R[i,j] = metric(x[i], y[i]) ≤ ε ? true : false with the  metric  being the distance function. The difference between a  RecurrenceMatrix  and a  CrossRecurrenceMatrix  is that in the first case  x === y . Objects of type  <:AbstractRecurrenceMatrix  are displayed as a  recurrenceplot . See also:  CrossRecurrenceMatrix ,  JointRecurrenceMatrix  and use  recurrenceplot  to turn the result of these functions into a plottable format. source"},{"id":957,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.CrossRecurrenceMatrix","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.CrossRecurrenceMatrix","content":" RecurrenceAnalysis.CrossRecurrenceMatrix  —  Type CrossRecurrenceMatrix(x, y, ε; kwargs...) Create a cross recurrence matrix from trajectories  x  and  y . See  RecurrenceMatrix  for possible value for  ε  and  kwargs . The cross recurrence matrix is a bivariate extension of the recurrence matrix. For the time series  x ,  y , of length  n  and  m , respectively, it is a sparse  n×m  matrix of Boolean values. Note that cross recurrence matrices are generally not symmetric irrespectively of  ε . source"},{"id":958,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.JointRecurrenceMatrix","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.JointRecurrenceMatrix","content":" RecurrenceAnalysis.JointRecurrenceMatrix  —  Type JointRecurrenceMatrix(x, y, ε; kwargs...) Create a joint recurrence matrix from trajectories  x  and  y . See  RecurrenceMatrix  for possible values for  ε  and  kwargs . The joint recurrence matrix considers the recurrences of the trajectories of  x  and  y  separately, and looks for points where both recur simultaneously. It is calculated by the element-wise multiplication of the recurrence matrices of  x  and  y . If  x  and  y  are of different length, the recurrences are only calculated until the length of the shortest one. See  RecurrenceMatrix  for details, references and keywords. source JointRecurrenceMatrix(R1::AbstractRecurrenceMatrix, R2::AbstractRecurrenceMatrix) Equivalent with  R1 .* R2 . source"},{"id":959,"pagetitle":"Recurrence Plots","title":"Advanced recurrences specification","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Advanced-recurrences-specification","content":" Advanced recurrences specification"},{"id":960,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.AbstractRecurrenceType","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.AbstractRecurrenceType","content":" RecurrenceAnalysis.AbstractRecurrenceType  —  Type AbstractRecurrenceType Supertype of all recurrence specification types. Instances of subtypes are given to  RecurrenceMatrix  and similar constructors to specify recurrences. Use  recurrence_threshold  to extract the numeric distance threshold. Possible subtypes are: RecurrenceThreshold(ε::Real) : Recurrences are defined as any point with distance  ≤ ε  from the referrence point. RecurrenceThresholdScaled(ratio::Real, scale::Function) : Here  scale  is a function of the distance matrix  dm  (see  distancematrix ) that is used to scale the value of the recurrence threshold  ε  so that  ε = ratio*scale(dm) . After the new  ε  is obtained, the method works just like the  RecurrenceThreshold . Specialized versions are employed if  scale  is  mean  or  maximum . GlobalRecurrenceRate(q::Real) : Here the number of total recurrence rate over the whole matrix is specified to be a quantile  q ∈ (0,1)  of the  distancematrix . In practice this yields (approximately) a ratio  q  of recurrences out of the total  Nx * Ny  for input trajectories  x, y . LocalRecurrenceRate(r::Real) : The recurrence threhsold here is point-dependent. It is defined so that each point of  x  has a fixed number of  k = r*N  neighbors, with ratio  r  out of the total possible  N . Equivalently, this means that each column of the recurrence matrix will have exactly  k  true entries. Notice that  LocalRecurrenceRate  does not guarantee that the resulting recurrence matrix will be symmetric. source"},{"id":961,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceThreshold","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.RecurrenceThreshold","content":" RecurrenceAnalysis.RecurrenceThreshold  —  Type RecurrenceThreshold(ε::Real) Recurrences are defined as any point with distance  ≤ ε  from the referrence point. See  AbstractRecurrenceType  for more. source"},{"id":962,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceThresholdScaled","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.RecurrenceThresholdScaled","content":" RecurrenceAnalysis.RecurrenceThresholdScaled  —  Type RecurrenceThresholdScaled(ratio::Real, scale::Function) Recurrences are defined as any point with distance  ≤ d  from the referrence point, where  d  is a scaled ratio (specified by  ratio, scale ) of the distance matrix. See  AbstractRecurrenceType  for more. source"},{"id":963,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.GlobalRecurrenceRate","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.GlobalRecurrenceRate","content":" RecurrenceAnalysis.GlobalRecurrenceRate  —  Type GlobalRecurrenceRate(rate::Real) Recurrences are defined as a constant global recurrence rate. See  AbstractRecurrenceType  for more. source"},{"id":964,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.LocalRecurrenceRate","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.LocalRecurrenceRate","content":" RecurrenceAnalysis.LocalRecurrenceRate  —  Type LocalRecurrenceRate(rate::Real) Recurrences are defined as a constant local recurrence rate. See  AbstractRecurrenceType  for more. source"},{"id":965,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.recurrence_threshold","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.recurrence_threshold","content":" RecurrenceAnalysis.recurrence_threshold  —  Function recurrence_threshold(rt::AbstractRecurrenceType, x [, y] [, metric]) → ε Return the calculated distance threshold  ε  for  rt . The output is real, unless  rt isa LocalRecurrenceRate , where  ε isa Vector . source"},{"id":966,"pagetitle":"Recurrence Plots","title":"Simple Recurrence Plots","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Simple-Recurrence-Plots","content":" Simple Recurrence Plots The recurrence matrices are internally stored as sparse matrices with Boolean values. Typically in the literature one does not sees the plots of the matrices  (hence \"Recurrence Plots\"). By default, when a Recurrence Matrix is created we \"show\" a mini plot of it which is a text-based scatterplot. Here is an example recurrence plot/matrix of a full trajectory of the Roessler system: using RecurrenceAnalysis, DynamicalSystemsBase\n\n# Create trajectory of Roessler system\n@inbounds function roessler_rule(u, p, t)\n    a, b, c = p\n    du1 = -u[2]-u[3]\n    du2 = u[1] + a*u[2]\n    du3 = b + u[3]*(u[1] - c)\n    return SVector(du1, du2, du3)\nend\np0 = [0.15, 0.2, 10.0]\nu0 = ones(3)\nro = CoupledODEs(roessler_rule, u0, p0)\nN = 2000; Δt = 0.05\nX, t = trajectory(ro, N*Δt; Δt, Ttr = 10.0)\n\n# Make a recurrence matrix with fixed threshold\nR = RecurrenceMatrix(X, 5.0)\nrecurrenceplot(R; ascii = true)     (2001, 2001) RecurrenceMatrix with 383575 recurrences of type RecurrenceThreshold. \n    +------------------------------------------------------------+ \n    |         ..:'.::'       .::'.:'..:'       ..:'.::    ..  .:'| \n    |   ..  .::'.:''   .:' .:'..''.::'   .   .::'.:'    .:'..:'  | \n    | .:':.:'.::' .  .:'.::'.::'.:'    .:'.::'.::'..  ::'.::'.:' | \n    |:'.::'.::'      :.:::.:'    ..    '.::'.::'      :.:::.:'   | \n    | ''' ''' ' .. ..:'''''   ..:'..  .:'''''  ' .. ..:' '''   ..| \n    |         .:'.::'       .::'.:'.::'       .::'.:''    .  .:''| \n    |      .:' .:'..:    ..:'..:'.:''.:'    .:'..:'..:    ..:'.::| \n    |.  ..:'.::'.::'   .::'.::' :'.::'   .::'.::'.:'' ' .::'.:'' | \n    |'.::'.::'.:'    .:::.:'..  .:''   .:::.:::.:'    .:':::'    | \n    |:::::':::'      '.::'.:''  '      ':::'.::'      '.::'..    | \n    |'::' ::'   ..  .::'::'    .. .   .::'.::'   ..  .::'::'    .| \n    |         .:'.::'       .::'.:' .:'       ..:'.::'       .::'| \n    |      .::'.::'.:     .:''.:'.::'..     .::'.:''.: .   .:':.:| \n    |     :'' :'.::'.:  ::' ::'.:':.:'..   :' ::'.::'.:' ::'.::'.| \n    |     .   .:'..:'    .:'.::'.:''.:'    .  ..:'.::'   ..:'.::'| \n    |  ..:'.::'.::'   .::'.::'..'.::'   ..:'.::'.:''   .::'.:''  | \n    |.::'.:::.:'     :::::'.::' :''    :::.:::.:'     :':::'.::  | \n    |::::':::'       .::'.::'   .      .::'.::'       .::'.:''   | \n    | ''  ''  '.:' .:'' ''    .:'..: .:'' '''   .:' .:'' ''    .:| \n    |        .:'..:'.      .::'.:''.:'..      .:'.::'.    . .::'.| \n    |     .::'.::'.:'    .:''.:'.::'.:'    .::'.:':.:'.   .:'..:'| \n    |.  .:':.:'..:'  '.::'.::'  ''.:'   ..:':::'..''  '.::'.::'  | \n    |:::'.::'.:''    :::.::'..  ::'    ::'.::'.:'     :::.:''    | \n    |::::::::'       .::'.::'          .::::::'       .::'.::    | \n    |::::::'..       ::.::'            :::::'.        ::.:'      | \n    +------------------------------------------------------------+  typeof(R) RecurrenceMatrix{RecurrenceThreshold{Float64}} summary(R) \"(2001, 2001) RecurrenceMatrix with 383575 recurrences of type RecurrenceThreshold.\" The above simple plotting functionality is possible through the package  UnicodePlots . The following function creates the plot:"},{"id":967,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.recurrenceplot","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.recurrenceplot","content":" RecurrenceAnalysis.recurrenceplot  —  Function recurrenceplot([io,] R; minh = 25, maxh = 0.5, ascii, kwargs...) -> u Create a text-based scatterplot representation of a recurrence matrix  R  to be displayed in  io  (by default  stdout ) using  UnicodePlots . The matrix spans at minimum  minh  rows and at maximum  maxh*displaysize(io)[1]  (i.e. by default half the display). As we always try to plot in equal aspect ratio, if the width of the plot is even less, the minimum height is dictated by the width. The keyword  ascii::Bool  can ensure that all elements of the plot are ASCII characters ( true ) or Unicode ( false ). The rest of the  kwargs  are propagated into  UnicodePlots.scatterplot . Notice that the accuracy of this function drops drastically for matrices whose size is significantly bigger than the width and height of the display (assuming each index of the matrix is one character). source Here is the same plot but using Unicode Braille characters recurrenceplot(R; ascii = false)     (2001, 2001) RecurrenceMatrix with 383575 recurrences of type RecurrenceThreshold. \n    ┌────────────────────────────────────────────────────────────┐ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⠞⠃⢀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁⣠⡶⠋⢀⣤⠞⠁⠀⠀⠀⠀⠀⠀⠀⢀⣤⠞⠁⣀⡴⠟⠀⠀⠀⠀⢀⣤⠀⠀⣀⡴⠛│ \n    │⠀⠀⠀⢀⡀⠀⠀⣠⣴⠟⠁⣠⡶⠋⠁⠀⠀⠀⣠⡴⠋⠀⣠⡾⠋⢀⣠⠘⠉⣀⡴⠟⠁⠀⠀⠀⢀⠀⠀⠀⣠⡴⠟⠁⣠⡾⠋⠀⠀⠀⠀⣠⡶⠋⢀⣠⡾⠋⠀⠀│ \n    │⠀⣠⡶⠋⢁⣤⡾⠋⢀⣴⠾⠋⠀⡀⠀⠀⢠⡾⠋⢀⣴⠟⠋⣀⡴⠟⠁⢠⡾⠋⠀⠀⠀⠀⢠⡾⠋⢀⣴⡾⠋⢀⣴⠟⠁⢀⡀⠀⠀⣴⡾⠋⣀⣴⠟⠁⣠⡴⠋⠀│ \n    │⡿⠋⣠⣴⡿⠋⣠⣶⠟⠁⠀⠀⠀⠀⠀⠀⢈⣠⣾⠟⢁⣠⡾⠋⠀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠈⣠⣶⠟⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⢁⣤⡾⠟⢁⣠⡾⠋⠀⠀⠀│ \n    │⠀⠚⠛⠁⠀⠚⠛⠁⠀⠒⠀⣠⡄⠀⢀⣠⠞⠛⠁⠐⠛⠋⠀⠀⠀⢀⣠⠞⠋⢀⡄⠀⠀⣠⡼⠛⠁⠐⠛⠋⠀⠀⠒⠀⣠⠄⠀⢀⣤⠟⠋⠀⠐⠛⠉⠀⠀⠀⢀⣤│ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⠞⠉⢀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁⣠⡾⠋⢀⣴⠞⠉⠀⠀⠀⠀⠀⠀⠀⢀⣴⠞⠁⣀⡴⠛⠁⠀⠀⠀⠀⡀⠀⠀⣠⡴⠛⠁│ \n    │⠀⠀⠀⠀⠀⠀⣀⡴⠂⠀⣠⡾⠋⢀⣠⠞⠀⠀⠀⠀⢀⣠⡾⠋⢀⣤⠞⠁⣠⡴⠛⠁⣠⡶⠃⠀⠀⠀⠀⣠⡴⠋⢀⣠⡾⠋⢀⣤⠞⠀⠀⠀⠀⢀⣤⠾⠋⢀⣴⠞│ \n    │⡀⠀⠀⢀⣤⡾⠋⢀⣴⠟⠋⢀⡴⠟⠁⠀⠀⠀⣀⣴⠟⠁⣠⡴⠟⠁⠀⠸⠋⢀⣴⠞⠁⠀⠀⠀⢀⣴⠾⠋⣀⣴⠟⠁⣀⡴⠋⠁⠀⠒⠀⣠⣴⠟⠁⣠⡶⠋⠁⠀│ \n    │⠃⣠⣶⠟⠉⣠⣾⠟⠁⣠⡾⠋⠀⠀⠀⠀⣠⣾⠟⢁⣤⡾⠋⢀⡀⠀⠀⢠⡴⠋⠁⠀⠀⠀⢠⣶⠟⢁⣠⡾⠟⢁⣠⠾⠋⠀⠀⠀⠀⣤⡾⠋⢁⣴⡾⠋⠀⠀⠀⠀│ \n    │⣿⠟⣁⣴⡿⠋⣁⣴⠟⠋⠀⠀⠀⠀⠀⠀⠉⣠⣶⠿⠋⣠⡴⠛⠁⠀⠀⠈⠀⠀⠀⠀⠀⠀⠈⣡⣴⡿⠋⣠⣴⠟⠁⠀⠀⠀⠀⠀⠀⠉⣠⣾⠟⠁⣠⡄⠀⠀⠀⠀│ \n    │⠁⠾⠿⠋⠀⠾⠟⠁⠀⠀⠀⢀⡀⠀⠀⣠⡾⠟⠁⠰⠾⠋⠀⠀⠀⠀⣠⡄⠀⢀⠀⠀⠀⣀⡼⠿⠋⠠⠾⠟⠁⠀⠀⠀⣀⡀⠀⠀⣠⠿⠟⠁⠰⠞⠋⠀⠀⠀⠀⣠│ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡾⠋⢀⣴⠞⠉⠀⠀⠀⠀⠀⠀⠀⢀⣴⠞⠁⢠⡴⠋⠀⣠⡾⠋⠀⠀⠀⠀⠀⠀⠀⢀⣠⠞⠋⢀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁│ \n    │⠀⠀⠀⠀⠀⠀⢀⣴⠞⠁⣠⡴⠟⠁⣠⠶⠀⠀⠀⠀⠀⣠⡴⠋⠁⣠⡾⠋⢀⣴⠟⠁⣀⡤⠀⠀⠀⠀⠀⣀⣴⠟⠁⣠⡶⠋⠁⣠⠖⠀⢀⠀⠀⠀⣠⡾⠋⢁⣠⠞│ \n    │⠀⠀⠀⠀⠀⠶⠋⠁⠀⠾⠋⢀⣴⠞⠁⣠⡴⠀⠀⠰⠾⠋⠀⠰⠟⠁⣠⡼⠛⢁⣤⠞⠋⢀⡄⠀⠀⠀⠾⠋⠀⠰⠾⠋⣀⡴⠟⠁⣠⠾⠋⠀⠰⠟⠋⢀⡰⠟⠁⣠│ \n    │⠀⠀⠀⠀⠀⢀⠀⠀⠀⣠⡾⠋⢀⣤⠞⠋⠀⠀⠀⠀⣠⡶⠃⢀⣴⠞⠋⢠⡴⠛⠁⣠⡾⠋⠀⠀⠀⠀⣀⠀⠀⢀⣠⡾⠋⢀⣴⠞⠁⠀⠀⠀⢀⣠⠞⠋⢀⣴⠞⠁│ \n    │⠀⠀⢀⣠⡾⠋⢀⣴⠟⠋⢀⣴⠟⠁⠀⠀⠀⣀⣴⠟⠁⣠⣴⠟⠁⣠⠄⠈⢀⣴⠞⠉⠀⠀⠀⢀⣤⠞⠉⣀⣴⠟⠁⣠⡴⠛⠁⠀⠀⠀⣀⣴⠟⠁⣠⡶⠛⠁⠀⠀│ \n    │⣠⣴⠟⠁⣠⣾⠟⢁⣠⡾⠋⠀⠀⠀⠀⠀⠸⠟⢁⣴⡾⠋⢀⣴⠞⠁⠀⠰⠛⠁⠀⠀⠀⠀⠰⠟⢁⣠⡾⠟⢁⣤⡾⠋⠀⠀⠀⠀⠀⠾⠋⢁⣴⡾⠋⢀⣴⠆⠀⠀│ \n    │⠟⣁⣴⡿⠛⣁⣴⠿⠋⠀⠀⠀⠀⠀⠀⠀⢠⣶⡿⠋⣠⣶⠟⠁⠀⠀⠀⢀⠀⠀⠀⠀⠀⠀⢠⣴⡿⠋⣠⣴⠟⠉⠀⠀⠀⠀⠀⠀⠀⣠⣶⠟⠉⣠⡾⠋⠁⠀⠀⠀│ \n    │⠀⠉⠉⠀⠀⠉⠁⠀⠀⠁⢀⡴⠂⠀⣠⡶⠋⠁⠀⠈⠉⠀⠀⠀⠀⣠⡶⠋⢀⣤⠆⠀⣀⡴⠋⠉⠀⠈⠉⠁⠀⠀⠀⣀⡴⠂⠀⣠⡶⠋⠁⠀⠈⠉⠀⠀⠀⠀⣠⠶│ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⣠⠾⠋⢀⣤⠞⠋⢀⠀⠀⠀⠀⠀⠀⢀⣴⠞⠉⣀⡰⠋⠁⣠⡾⠋⢀⡀⠀⠀⠀⠀⠀⠀⣠⠾⠋⢀⣴⠟⠁⢀⠀⠀⠀⠀⠄⠀⢀⣴⠟⠁⣀│ \n    │⠀⠀⠀⠀⠀⢀⣴⠞⠁⣠⡴⠟⠁⣠⡶⠋⠀⠀⠀⠀⣠⡶⠛⠁⣠⡾⠋⢀⣴⠟⠁⣀⡴⠋⠀⠀⠀⠀⣀⣴⠟⠁⣠⡶⠋⢁⣠⠾⠋⢀⠀⠀⠀⣠⡾⠋⢀⣠⠾⠋│ \n    │⡤⠀⠀⣠⡾⠛⢁⣤⡾⠋⢀⣤⠞⠁⠀⠀⠈⢀⣴⡾⠋⢀⣴⠟⠉⠀⠀⠈⠁⣠⠾⠋⠀⠀⠀⢀⣠⡾⠋⢁⣴⡾⠋⢀⡤⠈⠁⠀⠀⠉⢀⣴⠿⠋⣀⣴⠟⠁⠀⠀│ \n    │⣁⣴⡿⠋⣠⣴⠟⠋⣠⡴⠛⠁⠀⠀⠀⠀⣼⠟⢉⣠⡾⠟⠁⣠⠄⠀⠀⢰⠟⠁⠀⠀⠀⠀⢰⡿⠋⣠⣶⠟⠁⣠⡶⠋⠀⠀⠀⠀⠀⣾⠟⢁⣠⡾⠋⠁⠀⠀⠀⠀│ \n    │⡿⢋⣴⣾⠟⢁⣴⡾⠋⠀⠀⠀⠀⠀⠀⠀⢠⣴⡿⠋⣀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⠟⣁⣴⡿⠋⠀⠀⠀⠀⠀⠀⠀⣠⣴⡿⠋⣀⣴⠆⠀⠀⠀⠀│ \n    │⣾⣿⢟⣡⣾⡿⠋⣀⡀⠀⠀⠀⠀⠀⠀⠀⠻⢋⣤⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢋⣥⣾⠟⠉⣀⠀⠀⠀⠀⠀⠀⠀⠀⠟⢋⣤⡾⠋⠀⠀⠀⠀⠀⠀│ \n    └────────────────────────────────────────────────────────────┘  As you can see, the Unicode based plotting doesn't display nicely everywhere. It does display perfectly in e.g. VSCode, which is where it is the default printing type."},{"id":968,"pagetitle":"Recurrence Plots","title":"Advanced Recurrence Plots","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Advanced-Recurrence-Plots","content":" Advanced Recurrence Plots A text-based plot is cool, fast and simple. But often one needs the full resolution offered by the data of a recurrence matrix. There are two more ways to plot a recurrence matrix using  RecurrenceAnalysis :"},{"id":969,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.coordinates","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.coordinates","content":" RecurrenceAnalysis.coordinates  —  Function coordinates(R) -> xs, ys Return the coordinates of the recurrence points of  R  (in indices). source"},{"id":970,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.grayscale","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.grayscale","content":" RecurrenceAnalysis.grayscale  —  Function grayscale(R [, bwcode]; width::Int, height::Int, exactsize=false) Transform the recurrence matrix  R  into a full matrix suitable for plotting as a grayscale image. By default it returns a matrix with the same size as  R , but switched axes, containing \"black\" values in the cells that represent recurrent points, and \"white\" values in the empty cells and interpolating in-between for cases with both recurrent and empty cells, see below. The numeric codes for black and white are given in a 2-element tuple as a second optional argument. Its default value is  (0.0, 1.0) , i.e. black is coded as  0.0  (no brightness) and white as  1.0  (full brightness). The type of the elements in the tuple defines the type of the returned matrix. This must be taken into account if, for instance, the image is coded as a matrix of integers corresponding to a grayscale; in such case the black and white codes must be given as numbers of the required integer type. The keyword arguments  width  and  height  can be given to define a custom size of the image. If only one dimension is given, the other is automatically calculated. If both dimensions are given, by default they are adjusted to keep an aspect proportional to the original matrix, such that the returned matrix fits into a matrix of the given dimensions. This automatic adjustment can be disabled by passing the keyword argument  exactsize=true . If the image has different dimensions than  R , the cells of  R  are distributed in a grid with the size of the image, and a gray level between white and black is calculated for each element of the grid, proportional to the number of recurrent points contained in it. The levels of gray are coded as numbers of the same type as the black and white codes. It is advised to use  width, height  arguments for large matrices otherwise plots using functions like e.g.  heatmap  could be misleading. source For example, here is the representation of the above  R  from the Roessler system using both plotting approaches: using CairoMakie\nfig = Figure(resolution = (1000,500))\n\nax = Axis(fig[1,1])\nxs, ys = coordinates(R)\nscatter!(ax, xs, ys; color = :black, markersize = 1)\nax.limits = ((1, size(R, 1)), (1, size(R, 2)));\nax.aspect = 1\n\nax2 = Axis(fig[1,2]; aspect = 1)\nRg = grayscale(R)\nheatmap!(ax2, Rg; colormap = :grays)\nfig and here is exactly the same process, but using a delay embedded trajectory instead using DelayEmbeddings\n\ny = X[:, 2]\nτ = estimate_delay(y, \"mi_min\")\nm = embed(y, 3, τ)\nE = RecurrenceMatrix(m, 5.0; metric = \"euclidean\")\n\nxs, ys = coordinates(E)\nfig, ax = scatter(xs, ys; markersize = 1)\nax.aspect = 1\nfig which justifies why recurrence plots are so fitting to be used in embedded timeseries. Careful when using Recurrence Plots It is easy when using  grayscale  to not change the width/height parameters. The width and height are important when in  grayscale  when the matrix size exceeds the display size! Most plotting libraries may resample arbitrarily or simply limit the displayed pixels, so one needs to be extra careful. Besides graphical problems there are also other potential pitfalls dealing with the conceptual understanding and use of recurrence plots. All of these are summarized in the following paper which we suggest users to take a look at: N. Marwan,  How to avoid potential pitfalls in recurrence plot based data analysis , Int. J. of Bifurcations and Chaos ( arXiv )."},{"id":971,"pagetitle":"Recurrence Plots","title":"Skeletonized Recurrence Plots","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Skeletonized-Recurrence-Plots","content":" Skeletonized Recurrence Plots The finite size of a recurrence plot can cause border effects in the recurrence quantification-measures  rqa . Also the sampling rate of the data and the chosen recurrence threshold selection method ( fixed ,  fixedrate ,  FAN ) plays a crucial role. They can cause the thickening of diagonal lines in the recurrence matrix. Both problems lead to biased line-based RQA-quantifiers and is discussed in: K.H. Kraemer & N. Marwan,  Border effect corrections for diagonal line based recurrence quantification analysis measures ,  Phys. Lett. A 2019 ."},{"id":972,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.skeletonize","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.skeletonize","content":" RecurrenceAnalysis.skeletonize  —  Function skeletonize(R) → R_skel Skeletonize the  RecurrenceMatrix R  by using the algorithm proposed by Kraemer & Marwan  [Kraemer2019] . This function returns  R_skel , a recurrence matrix, which only consists of diagonal lines of \"thickness\" one. source Consider, e.g. a skeletonized version of a simple sinusoidal: using RecurrenceAnalysis, DelayEmbeddings, CairoMakie\n\ndata = sin.(2*π .* (0:400)./ 60)\nY = embed(data, 3, 15)\n\nR = RecurrenceMatrix(Y, 0.25; fixedrate=true)\nR_skel = skeletonize(R)\n\nfig = Figure(resolution = (1000,600))\nax = Axis(fig[1,1]; title = \"RP of monochromatic signal\")\nheatmap!(ax, grayscale(R); colormap = :grays)\n\nax = Axis(fig[1,2]; title = \"skeletonized RP\")\nheatmap!(ax, grayscale(R_skel); colormap = :grays)\nfig This way spurious diagonal lines get removed from the recurrence matrix, which would otherwise effect the quantification based on these lines."},{"id":973,"pagetitle":"Recurrence Plots","title":"Distance matrix","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#Distance-matrix","content":" Distance matrix The distance function used in  RecurrenceMatrix  and co. can be specified either as any  Metric  instance from  Distances . In addition, the following function returns a matrix with the cross-distances across all points in one or two trajectories:"},{"id":974,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.distancematrix","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.distancematrix","content":" RecurrenceAnalysis.distancematrix  —  Function distancematrix(x [, y = x], metric = \"euclidean\") Create a matrix with the distances between each pair of points of the time series  x  and  y  using  metric . The time series  x  and  y  can be  AbstractDataset s or vectors or matrices with data points in rows. The data point dimensions (or number of columns) must be the same for  x  and  y . The returned value is a  n×m  matrix, with  n  being the length (or number of rows) of  x , and  m  the length of  y . The metric can be any of the  Metric s defined in the  Distances  package  and defaults to  Euclidean() . source"},{"id":975,"pagetitle":"Recurrence Plots","title":"StateSpaceSet  reference","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#StateSpaceSet-reference","content":" StateSpaceSet  reference"},{"id":976,"pagetitle":"Recurrence Plots","title":"StateSpaceSets.StateSpaceSet","ref":"/DynamicalSystemsDocs.jl/recurrenceanalysis/stable/rplots/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. Marwan2007 N. Marwan  et al. , \"Recurrence plots for the analysis of complex systems\",  Phys. Reports 438*(5-6), 237-329 (2007) Marwan2015 N. Marwan & C.L. Webber,  Recurrence Quantification Analysis. Theory and Best Practices Springer (2015) Kraemer2019 Kraemer, K.H., Marwan, N. (2019).  Border effect corrections for diagonal line based recurrence quantification analysis measures. Physics Letters A 383(34) ."}]