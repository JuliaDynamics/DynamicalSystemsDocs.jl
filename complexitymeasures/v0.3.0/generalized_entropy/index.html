<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/DynamicalSystemsDocs.jl/'</script><script charset="utf-8" src="../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Generalized entropy · Entropies.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script data-main="../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" href="../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/><link href="nothing/DynamicalSystemsDocs.jl/complexitymeasures/stable/generalized_entropy/" rel="canonical"/><link href="../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link nav-item" href="../../../dynamicalsystems/">DynamicalSystems.jl</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Core</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../../statespacesets/">StateSpaceSets.jl - numerical handling of sets in state space</a><a class="nav-link nav-item" href="../../../dynamicalsystemsbase/">DynamicalSystemsBase.jl - infrastructure for programming NLD algorithms</a></ul></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Nonlinear Dynamics</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../../predefineddynamicalsystems/">PredefinedDynamicalSystems.jl - predefined dynamical systems used in publications</a><a class="nav-link nav-item" href="../../../chaostools/">ChaosTools.jl - various tools for analysing nonlinear and chaotic behaviour</a><a class="nav-link nav-item" href="../../../attractors/">Attractors.jl - find attractors and basins; continuation; tipping</a></ul></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Nonlinear Timeseries Analysis</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../../delayembeddings/">DelayEmbeddings.jl - optimal (unified/separated) delay coordinate embeddings</a><a class="nav-link nav-item" href="../../../fractaldimensions/">FractalDimensions.jl - dozens of estimators for fractal dimensions</a><a class="nav-link active nav-item" href="../../">ComplexityMeasures.jl - rigorous framework for probabilities, entropies, and other complexity measures</a><a class="nav-link nav-item" href="../../../timeseriessurrogates/">TimeseriesSurrogates.jl - dozes of ways to generate timeseries surrogates and tests hypothesis</a><a class="nav-link nav-item" href="../../../recurrenceanalysis/">RecurrenceAnalysis.jl - recurrence quantification and recurrence network analysis</a></ul></div><div class="search nav-item"><input id="search-input" placeholder="Search everywhere..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Entropies.jl</span></div><form action="../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../histogram_estimation/">Histograms</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href="">Generalized entropy</a></li><li><span class="tocitem">Estimators</span><ul><li><a class="tocitem" href="../SymbolicPermutation/">Permutation (symbolic)</a></li><li><a class="tocitem" href="../SymbolicWeightedPermutation/">Weighted permutation (symbolic)</a></li><li><a class="tocitem" href="../VisitationFrequency/">Visitation frequency</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="">Generalized entropy</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Generalized entropy</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kahaaga/Entropies.jl/blob/master/docs/src/generalized_entropy.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Generalized-entropy"><a class="docs-heading-anchor" href="#Generalized-entropy">Generalized entropy</a><a id="Generalized-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-entropy" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" href="#Entropies.genentropy" id="Entropies.genentropy"><code>Entropies.genentropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">genentropy(α::Real, p::AbstractArray; base = Base.MathConstants.e)</code></pre><p>Compute the entropy, to the given <code>base</code>, of an array of probabilities <code>p</code>, assuming  that <code>p</code> is sum-normalized.</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi entropy is</p><div>\[H_\alpha(p) = \frac{1}{1-\alpha} \log \left(\sum_i p[i]^\alpha\right)\]</div><p>and generalizes other known entropies, like e.g. the information entropy (<span>$\alpha = 1$</span>, see <sup class="footnote-reference"><a href="#footnote-Shannon1948" id="citeref-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$\alpha=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$\alpha = 2$</span>, also known as collision entropy).</p><p><strong>Example</strong></p><pre><code class="language-julia">using Entropies
p = rand(5000)
p = p ./ sum(p) # normalizing to 1 ensures we have a probability distribution

# Estimate order-1 generalized entropy to base 2 of the distribution
Entropies.genentropy(1, ps, base = 2)</code></pre><p>See also: <a href="../histogram_estimation/#Entropies.non0hist-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T&lt;:Real"><code>non0hist</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/kahaaga/Entropies.jl/blob/df9175f10200749e9b50a0707fae4208abe554f4/src/generalized_entropy.jl#L3-L38" target="_blank">source</a></section><section><div><p><strong>Permutation entropy</strong></p><pre><code class="language-none">genentropy(x::Dataset, est::SymbolicPermutation, α::Real = 1; base = 2) → Real
genentropy(x::AbstractVector{&lt;:Real}, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real

genentropy!(s::Vector{Int}, x::Dataset, est::SymbolicPermutation, α::Real = 1; base = 2) → Real
genentropy!(s::Vector{Int}, x::AbstractVector{&lt;:Real}, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order-<code>α</code> entropy over a permutation symbolization of <code>x</code>, using  symbol size/order <code>m</code>. </p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p>A pre-allocated symbol array <code>s</code> can be provided to save some memory allocations if   probabilities are to be computed for multiple data sets. If provided, it is required that  <code>length(x) == length(s)</code> if <code>x</code> is a <code>Dataset</code>, or  <code>length(s) == length(x) - (m-1)τ</code>  if <code>x</code> is a univariate signal.</p><p><strong>Probability estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code>, using <a href="../probabilities/#Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}"><code>probabilities(::Dataset, ::SymbolicPermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the symbols.</p><p><strong>Entropy estimation</strong></p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a href="#footnote-Rényi1960" id="citeref-Rényi1960">[Rényi1960]</a></sup>, to the given <code>base</code>, is computed from that sum-normalized symbol  distribution, using <a href="#Entropies.genentropy"><code>genentropy</code></a>.</p><p><strong>Notes</strong></p><p><em>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</em></p><p>See also: <a href="../SymbolicPermutation/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>, <a href="#Entropies.genentropy"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/kahaaga/Entropies.jl/blob/df9175f10200749e9b50a0707fae4208abe554f4/src/symbolic/SymbolicPermutation.jl#L232-L276" target="_blank">source</a></section><section><div><p><strong>Weighted permutation entropy</strong></p><pre><code class="language-none">genentropy(x::Dataset, est::SymbolicWeightedPermutation, α::Real = 1; base = 2) → Real
genentropy(x::AbstractVector{&lt;:Real}, est::SymbolicWeightedPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order <code>α</code> entropy based on a weighted permutation  symbolization of <code>x</code>, using symbol size/order <code>m</code> for the permutations.</p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p><strong>Probability estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code> by a weighted procedure, using <a href="../probabilities/#Entropies.probabilities-Tuple{Dataset,SymbolicWeightedPermutation}"><code>probabilities(::Dataset, ::SymbolicWeightedPermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the weighted symbols.</p><p><strong>Entropy estimation</strong></p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a href="#footnote-Rényi1960" id="citeref-Rényi1960">[Rényi1960]</a></sup>, to the given <code>base</code>, is computed from that sum-normalized symbol  distribution, using <a href="#Entropies.genentropy"><code>genentropy</code></a>.</p><p><strong>Notes</strong></p><p><em>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</em></p><p>See also: <a href="../SymbolicWeightedPermutation/#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a>, <a href="#Entropies.genentropy"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/kahaaga/Entropies.jl/blob/df9175f10200749e9b50a0707fae4208abe554f4/src/symbolic/SymbolicWeightedPermutation.jl#L223-L259" target="_blank">source</a></section><section><div><p><strong>Visitation frequency, binning based entropy</strong></p><pre><code class="language-none">genentropy(x::Dataset, est::VisitationFrequency, α::Real = 1; base::Real = 2)</code></pre><p>Compute the order-<code>α</code> generalized (Rényi) entropy<sup class="footnote-reference"><a href="#footnote-Rényi1960" id="citeref-Rényi1960">[Rényi1960]</a></sup> of a multivariate dataset <code>x</code> using a visitation frequency approach.</p><p><strong>Description</strong></p><p>First, the state space defined by <code>x</code> is partitioned into rectangular boxes according to  the binning instructions given by <code>est.binning</code>. Then, a histogram of visitations to  each of those boxes is obtained, which is then sum-normalized to obtain a probability  distribution, using <a href="../VisitationFrequency/#Entropies.probabilities-Tuple{Dataset,VisitationFrequency}"><code>probabilities</code></a>. The generalized entropy to the given <code>base</code> is  then computed over that box visitation distribution using  <a href="#Entropies.genentropy"><code>genentropy(::Real, ::AbstractArray)</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia">using DelayEmbeddings, Entropies
D = Dataset(rand(1:3, 20000, 3))

# Estimator specification. Split each coordinate axis in five equal segments.
est = VisitationFrequency(RectangularBinning(5)) 

# Estimate order-1 (default) generalized entropy
Entropies.genentropy(D, est, base = 2)</code></pre><p>See also: <a href="../VisitationFrequency/#Entropies.VisitationFrequency"><code>VisitationFrequency</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/kahaaga/Entropies.jl/blob/df9175f10200749e9b50a0707fae4208abe554f4/src/binning_based/rectangular/VisitationFrequency.jl#L60-L91" target="_blank">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../SymbolicPermutation/">Permutation (symbolic) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 1 November 2020 01:57">Sunday 1 November 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>